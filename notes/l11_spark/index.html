<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<link href="../../img/favicon.ico" rel="shortcut icon"/>
<title>50.043 Spark - Database System and Big Data 50.043</title>
<link href="../../css/bootstrap.min.css" rel="stylesheet"/>
<link href="../../css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../css/base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</head>
<body>
<div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
<div class="container">
<a class="navbar-brand" href="../..">Database System and Big Data 50.043</a>
<!-- Expander button -->
<button class="navbar-toggler" data-target="#navbar-collapse" data-toggle="collapse" type="button">
<span class="navbar-toggler-icon"></span>
</button>
<!-- Expanded navigation -->
<div class="navbar-collapse collapse" id="navbar-collapse">
<!-- Main navigation -->
<ul class="nav navbar-nav">
<li class="navitem">
<a class="nav-link" href="../..">Home</a>
</li>
<li class="navitem">
<a class="nav-link" href="../../project/">Project</a>
</li>
<li class="navitem">
<a class="nav-link" href="../l1_course_handout/">Handout</a>
</li>
</ul>
<ul class="nav navbar-nav ml-auto">
<li class="nav-item">
<a class="nav-link" data-target="#mkdocs_search_modal" data-toggle="modal" href="#">
<i class="fa fa-search"></i> Search
                            </a>
</li>
</ul>
</div>
</div>
</div>
<div class="container">
<div class="row">
<div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
<div class="navbar-header">
<button class="navbar-toggler collapsed" data-target="#toc-collapse" data-toggle="collapse" title="Table of Contents" type="button">
<span class="fa fa-angle-down"></span>
</button>
</div>
<div class="navbar-collapse collapse card bg-secondary" id="toc-collapse">
<ul class="nav flex-column">
<li class="nav-item" data-level="1"><a class="nav-link" href="#50043-spark">50.043 Spark</a>
<ul class="nav flex-column">
<li class="nav-item" data-level="2"><a class="nav-link" href="#learning-outcomes">Learning Outcomes</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-vs-hadoop-mapreduce">Spark VS Hadoop MapReduce</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-rdd-api">Spark RDD API</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-performance-tuning">Spark performance tuning</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-failure-recovery">Spark Failure Recovery</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-dataframe">Spark DataFrame</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-sql">Spark SQL</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-machine-learning">Spark Machine Learning</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#spark-streaming">Spark Streaming</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#additional-references">Additional References</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div></div>
<div class="col-md-9" role="main">
<h1 id="50043-spark">50.043 Spark</h1>
<h2 id="learning-outcomes">Learning Outcomes</h2>
<p>By the end of this lesson, you are able to</p>
<ul>
<li>Differentiate Hadoop MapReduce and Spark</li>
<li>Apply Spark based on application requirements</li>
<li>Develop data engineering process using Spark RDD</li>
<li>Reason about Spark Execution Model</li>
<li>Develop machine learning application using Spark MLLib</li>
<li>Explain Spark Architecture</li>
<li>Develop Data processing application using Spark Dataframe</li>
<li>Develop Machine Learning application using Spark ML package</li>
<li>Explain Spark Streaming</li>
</ul>
<h2 id="spark-vs-hadoop-mapreduce">Spark VS Hadoop MapReduce</h2>
<p>Hadoop MapReduce was created to batch process data once or twice, e.g.  web search index, processing crawled data, etc.</p>
<p>When machine learning being applied to big data, the terabyte or zeta byte of data probably need to be processed iteratively.  For instance, if we need to run gradient descent thousands of times.</p>
<p>On top of that data visualization for big data also impose further challenge. It requires data to be reprocessed based on the user inputs such as sorting and filtering constraints. </p>
<p>Hadoop MapReduce is no longer suitable for these applications because of the following 
1. each of mapper (and educer) task needs to transfer intermediate data to the disk back and forth. 
2. its rigit computation model (i.e. one step of map followed by one step of reduce) makes most of the application look unnecessarily complex.
3. it does not utilize much of RAM. Many of the MapReduce applications are disk and network I/O bound rather than RAM and CPU bound.
4. it is hard to redistribute the workload</p>
<p>Apache Spark is a project which started off as an academic reseach idea and became an enterprise level success. It addressed the above-mentioned limitations of the Hadoop MapReduce by introducing the following features</p>
<ul>
<li>Resilient distributed datasets, which act as the primary data structure for distributed map reduce operation</li>
<li>It unions all the available RAM from the cluster (all data nodes) to form a large pool of virtual RAM. RDD and its derivative such as dataframe and dataset, are in memory parallel distributed data structure to be used in the virtual RAM pool.</li>
<li>It has a MapReduce like programming interface (closer to our toy MapReduce
  library compared to the Hadoop MapReduce)</li>
<li>It offers fault tolerance, RDD, dataframe and datasets can always be re-computed in case of node failure   </li>
<li>Machine Learning Libraries, Graph computation libraries.</li>
</ul>
<p>Spark supports many mainstream programming languagues such as Scala, Python, R, Java and SQL. In this module we consider the Python interface.</p>
<h2 id="spark-rdd-api">Spark RDD API</h2>
<p>The primary data structure of the Spark RDD API is the RDD. </p>
<pre><code class="language-python">data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
</code></pre>
<p>In the above code snippet, we initialize a list of integers and turn it into an RDD <code>distData</code>. </p>
<p>Alternatively, we can load the data from a file.</p>
<pre><code class="language-python">distData = sc.textFile("hdfs://127.0.0.1:9000/data.txt")
</code></pre>
<p>Given the RDD, we can now perform data manipulation. There are two kinds of RDD APIs, namely <em>Transformations</em> and <em>Actions</em>.  Transformation APIs are <em>lazy</em> and action APIs are <em>strict</em>.</p>
<h3 id="lazy-vs-strict-evaluation">Lazy vs Strict Evaluation</h3>
<p>In lazy evaluation, the argument of a function is not fully computed until it is needed. We can mimic this in Python using generator and iterator.</p>
<pre><code class="language-python">r = range(2,-1,-1) # a range 2,1,0
l = ( 4 // x for x in r) # a generator 2, 4, div_by_0
def takeOne(l):
    res = next(iter(l), None)
    if res is None:
        return error(" ... ")
    else:
        return res
takeOne(l) # yield 2
</code></pre>
<p>When <code>takeOne(l)</code> is called, only the first element of <code>l</code> is computed, the rest are discarded. </p>
<p>In strict evaluation, the argument of a function is alwaysfully computed.</p>
<pre><code class="language-python">def takeOneStrict(l):
    res = next(iter(list(l)), None)
    if res is None:
        return error(" ... ")
    else:
        return res
takeOneStrict(l) # yield an div by zero error.
</code></pre>
<p>When <code>takeOneStrict(l)</code> is called, even though only the first element of <code>l</code> is required, the rest are computed eagerly, thanks to the <code>list()</code> function call materializing the generator.</p>
<h3 id="rdd-transformations">RDD Transformations</h3>
<p>All Transformations takes the current RDD as (part of) the input and
return some a new RDD as output.</p>
<p>Let <code>l</code>, <code>l1</code> and <code>l2</code> be RDDs, <code>f</code> be a function</p>
<table>
<thead>
<tr>
<th>RDD APIs</th>
<th>Description</th>
<th>Toy MapReduce equivalent</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>l.map(f)</code></td>
<td></td>
<td><code>map(f,l)</code></td>
</tr>
<tr>
<td><code>l.flatMap(f)</code></td>
<td></td>
<td><code>flatMap(f,l)</code></td>
</tr>
<tr>
<td><code>l.filter(f)</code></td>
<td></td>
<td><code>filter(f,l)</code></td>
</tr>
<tr>
<td><code>l.reduceByKey(f)</code></td>
<td></td>
<td><code>reduceByKey(f,l)</code></td>
</tr>
<tr>
<td><code>l.mapPartition(f)</code></td>
<td>similar to <code>map</code>, <code>f</code> takes an iterator and produces an iterator</td>
<td>NA</td>
</tr>
<tr>
<td><code>l.distinct()</code></td>
<td>all distinct elems</td>
<td>N.A.</td>
</tr>
<tr>
<td><code>l.sample(b,ratio,seed)</code></td>
<td>sample dataset. <code>b</code>: a boolean value to indicate w/wo replacement. <code>ratio</code>: a value range [0,1]</td>
<td>N.A.</td>
</tr>
<tr>
<td><code>l.aggregateByKey(zv)(sop,cop)</code></td>
<td><code>zv</code>: accumulated value. <code>sop</code>: intra-partition aggregation function. <code>cop</code>: inter-partition aggregation function</td>
<td>similar to <code>reduceByKey(f,l,acc)</code>, except that we don't have 2 version of <code>f</code></td>
</tr>
<tr>
<td><code>l1.union(l2)</code></td>
<td>union <code>l1</code> <code>l2</code></td>
<td><code>l1 + l2</code></td>
</tr>
<tr>
<td><code>l1.intersection(l2)</code></td>
<td>the intersection of elements from <code>l1</code> and <code>l2</code></td>
<td>N.A.</td>
</tr>
<tr>
<td><code>l1.groupByKey()</code></td>
<td>group elemnts by keys</td>
<td><code>shuffle(l1)</code></td>
</tr>
<tr>
<td><code>l1.sortByKey()</code></td>
<td>sort by keys</td>
<td>N.A.</td>
</tr>
<tr>
<td><code>l1.join(l2)</code></td>
<td>join <code>l1</code> <code>l2</code> by keys</td>
<td>we've done it in lab</td>
</tr>
<tr>
<td><code>l1.cogroup(l2)</code></td>
<td>similar to <code>join</code>, it returns RDDs of <code>(key, ([v1,..], [v2,..]))</code>, <code>[v1,...]</code> are values from <code>l1</code>, <code>[v2,...]</code> are values from <code>l2</code></td>
<td>N.A.</td>
</tr>
</tbody>
</table>
<p>Note that the RDDS APIs follow the builtin Scala library's convention, <code>map</code>,
<code>filter</code> and etc are methods of the List class.</p>
<h3 id="rdd-actions">RDD Actions</h3>
<p>All Actions takes the current RDD as (part of) the input and
return some value that is not an RDD. It forces computation to happen.</p>
<table>
<thead>
<tr>
<th>RDDs</th>
<th>Desc</th>
<th>Toy MR</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>l.reduce(f)</code></td>
<td></td>
<td><code>reduce(f,l)</code></td>
</tr>
<tr>
<td><code>l.collect()</code></td>
<td>converts rdd to a local array</td>
<td></td>
</tr>
<tr>
<td><code>l.count()</code></td>
<td></td>
<td><code>len(l)</code></td>
</tr>
<tr>
<td><code>l.first()</code></td>
<td></td>
<td><code>l[0]</code></td>
</tr>
<tr>
<td><code>l.take(n)</code></td>
<td>returns an array</td>
<td><code>l[:n]</code></td>
</tr>
<tr>
<td><code>l.saveAsTextFile(path)</code></td>
<td>save rdd to text file</td>
<td>N.A.</td>
</tr>
<tr>
<td><code>l.countByKey()</code></td>
<td>return hash map of key and count</td>
<td>N.A.</td>
</tr>
<tr>
<td><code>l.foreach(f)</code></td>
<td>run a function for each element in the dataset with side-effects</td>
<td><code>for x in l: ...</code></td>
</tr>
</tbody>
</table>
<h3 id="special-transformation">Special Transformation</h3>
<p>Some transformation/opereations such as <code>reduceByKey</code>, <code>join</code>, <code>groupByKey</code> and <code>sortByKey</code> will trigger a shuffle event, in which
Spark redistribute the data across partititon, which means intermediate results from lazy operations will be materialized. </p>
<h3 id="wordcount-example-in-spark">Wordcount example in Spark</h3>
<p>The follow code snippet, we find the wordcount application implemented using PySpark.</p>
<pre><code class="language-python">import sys
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("Wordcount Application")
sc = SparkContext(conf=conf)

text_file = sc.textFile("hdfs://localhost:9000/input/")
counts = text_file.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile("hdfs://localhost:9000/output/")
sc.stop()
</code></pre>
<p>We can compare it with the version in toy MapReduce, and find many similarities </p>
<pre><code class="language-python">infile = open(sys.argv[1], 'r')
lines = []
for line in infile: lines.append(line.strip())

ws = flatMap(lambda line:line.split(" "),lines)
w1s = map(lambda w:(w,1), ws)
res = reduceByKey(lambda x,y:x+y,w1s,0)

with open(sys.argv[2], 'w') as out:
    for w,c in res:
        out.write(w + "\t" + str(c) + "\n")
</code></pre>
<p>We will see more examples of using PySpark in the lecture and the labs.</p>
<h3 id="spark-architecture">Spark Architecture</h3>
<p>Next we consider the how Spark manages the execution model.</p>
<p>In the following, we find the Spark Architecture (in simple standalone mode)</p>
<p><img alt="Spark Architecture" src="https://miro.medium.com/max/1117/1*Etqq2Ekh7BXso8Jcp_6e1g.png"/>{width=100%}</p>
<p>Like Hadoop, Spark follows a simple master and worker architecture. A machine is dedicated to manage the Spark cluster, which is known as the Spark Master node (c.f. namenode in a Hadoop Cluster). A set of machines are in charge of running the actual computation, namely, the Spark Worker nodes (c.f. data nodes in a Hadoop Cluster).</p>
<p><img alt="" src="https://docs.cloudera.com/runtime/7.2.1/developing-spark-applications/images/spark-tuning-f1.png"/></p>
<p>A <em>driver</em> is a program that runs on the Spark Master node, which manages the interaction between the application and the client. Upon a job submission from the client, a Spark driver schedules the jobs by analyzing the sub tasks dependency and allocate the sub tasks to the executors. A list of executors (runnning on some worker nodes) receive tasks from the driver and reports the result upon completion.</p>
<h3 id="spark-task-scheduling">Spark Task Scheduling</h3>
<p>In this section, we take a look at how Spark divides a given job into tasks and schedules the tasks to the workers. </p>
<p><img alt="" src="../images/spark-job-scheduling.jpeg"/></p>
<p>As illustrated by the above diagram Spark takes 4 stages to schedule the job. </p>
<ol>
<li>Given a job, Spark builds a directed acyclic graph (DAG) which represents the dependencies among the operations performed in the job.  </li>
<li>Given the DAG, Spark splits the graph into multiple stages of tasks.</li>
<li>Tasks are scheduled according to their stages, A later stage must not be started until the earlier stages are completed. </li>
<li>Tasks scheduled to the workers then executed. </li>
</ol>
<h4 id="a-simple-example">A simple example</h4>
<p>Let's consider a simple example </p>
<pre><code class="language-python"># spark job 1
r1 = sc.textFile("...")
r2 = r1.map(f) 
</code></pre>
<p>A spark driver takes the above program and construct the DAG, since it contain just a read followed by a map operations. It has the following DAG. </p>
<div class="mermaid">graph TD
    r1 --map--&gt; r2 
</div>
<p>It is clear that there is only one stage, (because there is only one operation, one set of inputs and one set of outputs). </p>
<p><img alt="" src="../images/spark_map.png"/></p>
<p>The Task Scheduler allocate the tasks in parallel, as follows.</p>
<p><img alt="" src="../images/spark_map2.png"/></p>
<h4 id="a-less-simple-example">A less simple example</h4>
<p>Let's consider another example </p>
<pre><code class="language-python"># spark job 2
r1 = sc.textFile("...")
r2 = r1.map(f)
r3 = sc.textFile("...")
r4 = r3.map(g)
r5 = r2.join(r4)
r6 = r5.groupByKey()
r7 = r6.map(h)
r8 = r7.reduce(i)
</code></pre>
<p>The DAG is as follows</p>
<div class="mermaid">graph TD;
r1 --map--&gt; r2 
r3 --map--&gt; r4
r2 --join--&gt; r5
r4 --join--&gt; r5
r5 --groupByKey--&gt; r6
r6 --map--&gt; r7
r7 --reduce--&gt; r8
</div>
<p>There are mulitple way of dividing the above DAG into stages. For instance, we could naively turns each operation (each arrow) into a stage (we end up with many stages and poor parallelization) or we group the simple paths (straight line paths) into a stage.</p>
<p>Spark takes a more intelligent approach by classifying different types of dependencies. </p>
<ul>
<li>
<p>Narrow dependencies - Input partition used by at most 1 output
partition</p>
</li>
<li>
<p>Wide dependencies - Input partition used by more than one output partitions</p>
</li>
</ul>
<p><img alt="Narrow dependencies vs Wide dependencies" src="../images/0%2AkAw8hogu1oZPy9QU.png"/></p>
<p>Thus for <code>Spark Job 2</code>,  we further annotate the DAG with dependency types.</p>
<div class="mermaid">graph TD;
r1 --map/narrow--&gt; r2 
r3 --map/narrow--&gt; r4
r2 --join/narrow--&gt; r5
r4 --join/wide--&gt; r5
r5 --groupByKey/wide--&gt; r6
r6 --map/narrow--&gt; r7
r7 --reduce--&gt; r8
</div>
<p>All the map operations are narrow. The join of <code>r2</code> and <code>r4</code> should be wide. However since the result must be ordered either by <code>r2</code>'s partition order or <code>r4</code>'s. Only one side of the operation is wide. In this case, we assume <code>r5</code>'s partition follows <code>r2</code>'s, hence <code>r2</code> to <code>r5</code> is narrow. <code>groupByKey</code> operations are wide. </p>
<p>Next we can decide how many stages we need by </p>
<ol>
<li>allowing narrow dependencies preceding the same wide dependency to the grouped under the same stage, because they do not incur any network I/O. <ul>
<li>We apply task parallelism here., </li>
</ul>
</li>
<li>wide dependency initiates a new stage, as we need to wait for all the data operands to be fully computed before the operation being executed. </li>
</ol>
<div class="mermaid">graph TD;
r1 --map/narrow/1--&gt; r2 
r3 --map/narrow/1--&gt; r4
r2 --join/narrow/2--&gt; r5
r4 --join/wide/2--&gt; r5
r5 --groupByKey/wide/3--&gt; r6
r6 --map/narrow/3--&gt; r7
r7 --reduce/3--&gt; r8
</div>
<p>In the above, we stage the first two map operations. The join operation is assigned to the 2nd stage. The <code>groupByKey</code> initiates the 3rd stage which includes the following map and reduce. </p>
<h2 id="spark-performance-tuning">Spark performance tuning</h2>
<p>Knowing how Spark schedules a job into stages of sub tasks. We can optimize the job by rewriting it into an equivalent one such that </p>
<ul>
<li>Pre-partition or re-partition of the data</li>
<li>Cache the common intermediate data</li>
</ul>
<h3 id="pre-partition-or-re-partition-of-the-data">Pre-partition or re-partition of the data</h3>
<p>Recall that a spark job is represented a DAG of stages</p>
<p><img alt="" src="../images/spark_stages3.png"/></p>
<p>If we know the join operation and pre- (or re-) arrange the data according to the key to partition mapping, we could reduce the shuffling. </p>
<p><img alt="" src="../images/spark_repartition.png"/></p>
<pre><code class="language-python">d1 = [(1, "A"), (2, "B"), (1, "C"), (2, "D"), (1, "E"), (3, "F")]
d2 = [(1, 0.1), (2, 0.2), (2, 3.1), (3, 0)]
r1 = sc.parallelize(d1) 
r2 = sc.parallelize(d2)
</code></pre>
<p>let assume that <code>r1</code> is by default partitioned randomly into two partitions
and so is <code>r2</code>.</p>
<pre><code class="language-python">r1.glom().collect() # collect rdd into list by retaining the partition
r2.glom().collect()
# r1 being partitioned 
[[(1, 'A'), (2, 'B'), (1, 'C')],   # p1
  [(2, 'D'), (1, 'E'), (3, 'F')]]  # p2
# r2 being partitioned
[[(1, 0.1), (2, 0.2)],  # p3
  [(2, 3.1), (3, 0)]])  # p4
</code></pre>
<p>next we would like join <code>r1</code> with <code>r2</code> by key, i.e. the first component of the pairs</p>
<pre><code class="language-python">r3 = r1.join(r2)
r3.glom().collect()
</code></pre>
<p>the result will be stored in three partitions</p>
<pre><code class="language-python">[[(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], # p1
 [(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))], # p2
 [(3, ('F', 0))]] # p3
</code></pre>
<p>Note that the actual order of the collected result might not be same as above, Spark tries to reuse partitions being created whenever possible. (For breivity we omit an empty partition <code>p4</code>.)</p>
<p>As observed, there are tuple transferred from <code>p1</code> to <code>p2</code>, e.g. <code>(2, 'B')</code> 
and from <code>p2</code> to <code>p1</code>, e.g. <code>(1, 'E')</code>. These transfers can be eliminited 
if we manually control the partitioning of the initial RDDs, </p>
<pre><code class="language-python">r4 = sc.parallelize(d1).partitionBy(3, lambda key: key)
r5 = sc.parallelize(d2).partitionBy(3, lambda key: key)
r4.glom().collect()
r5.glom().collect()
</code></pre>
<pre><code class="language-python"># r4 is partitioned 
[[(3, 'F')], #p1 
 [(1, 'A'), (1, 'C'), (1, 'E')], #p2 
 [(2, 'B'), (2, 'D')]] #p3
# r5 is partitioned 
[[(3, 0)],  #p4
[(1, 0.1)], #p5
[(2, 0.2), (2, 3.1)]] #p6
</code></pre>
<p>when we perform the join</p>
<pre><code class="language-python">r6 = r4.join(r5)
r6.glom().collect()
</code></pre>
<p>we have </p>
<pre><code class="language-python">[
[(3, ('F', 0))], #p1
[(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], #p2
[(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))] #p3
]
</code></pre>
<p>The number of tuples being transferred across partition is now minimized. </p>
<ul>
<li>Sample code can be found here.</li>
</ul>
<pre><code>https://colab.research.google.com/drive/1XO1hqcRCn9JKu0tkRb0ANQylCnBod3B-?usp=sharing
</code></pre>
<h3 id="cache-the-intermediate-data">Cache the intermediate data</h3>
<p>Recall that in Spark transformation are lazy until a shuffling is required. Laziness is a double-edged sword. </p>
<p><img alt="" src="../images/spark_cache.png"/></p>
<p>In the above, the orange partition (data) is being used by 3 different "sinks". If all the operations above the last levels are transformations (i.e. lazy.) Having 3 sink operations will require most of the intermediate partitions (all boxes except for the last) </p>
<p>For example consider the following</p>
<pre><code class="language-python">data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)]
r1 = sc.parallelize(data)
r2 = r1.map( lambda x:(x[0], x[1] * 0.01))
r3 = r2.groupByKey()
r4 = r3.map( lambda x:(x[0], std(x[1])))
r5 = r3.map( lambda x:(x[0], min(x[1])))
r4.collect()
r5.collect()
</code></pre>
<p>There are two downstream operation of <code>r3</code>, namely <code>r4</code> computing the standard deviation by key, and <code>r5</code> computing the minimum by key. </p>
<p>Due to the fact that <code>r3</code> is lazy. The <code>r4.collect()</code> action (sink) triggers the computation of <code>r1</code> to <code>r3</code> and <code>r4</code>. The <code>r5.collect()</code> triggers the recomputation of <code>r1</code> to <code>r3</code> and the computation of <code>r5</code>.</p>
<p>If we materialize <code>r3</code> and cache it, we would avoid the recomputation of <code>r1</code> to <code>r3</code>. </p>
<pre><code class="language-python">data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)]
r1 = sc.parallelize(data)
r2 = r1.map( lambda x:(x[0], x[1] * 0.01))
r3 = r2.groupByKey().cache()
r4 = r3.map( lambda x:(x[0], std(x[1])))
r5 = r3.map( lambda x:(x[0], min(x[1])))
r4.collect()
r5.collect()
</code></pre>
<ul>
<li>Sample code</li>
</ul>
<pre><code>https://colab.research.google.com/drive/1QqUSa5Kkjpw3Avw2DlWX9ZMrZcsnDz5x?usp=sharing
</code></pre>
<h3 id="other-optimization-tricks">Other optimization tricks</h3>
<p>Besides the above mentioned two approaches, a trick that we've used in RDBMS for optimization is also applicable to Spark. i.e. apply filter as early as possible so as to reduce the size of intermediate output. </p>
<p>Another Spark specific optimization trick is to rewriting <code>groupByKey().map(...)</code> by <code>reduceByKey(...)</code>. However in some cases, a rewrite solution might not exist.</p>
<h2 id="spark-failure-recovery">Spark Failure Recovery</h2>
<p>Recall that for any MapReduce implementation to produce deterministic results. The computation must be <em>pure</em>. </p>
<p>It turns out that purity property makes failure recovery much easier. 
For each Spark job, a lineage of the sub tasks is computed. In the event of failure, the Spark driver will refer to the lineage and recompute the affected sub-tasks. Thanks to the purity property, partially completed and incomplete computation can always be computed without the need of restoring the original state.</p>
<p>For instance consider the following job</p>
<p><img alt="" src="../images/spark_lineage1.png"/></p>
<p>Where all the square boxes denote the partitions of some RDDs, suppose partition <code>C1</code> is faulty.</p>
<p><img alt="" src="../images/spark_lineage2.png"/></p>
<p>Based on the diagram (lineage), we know that we can recompute <code>C1</code> elsewhere by using <code>B1</code> and <code>B2</code>.</p>
<h2 id="spark-dataframe">Spark DataFrame</h2>
<p>Besides Spark RDD, Spark offers DataFrame as a higher level API interface to the programmers and data engineers. The ussage is influenced and inspired by Python's Pandas.</p>
<p>In a nutshell, a dataframe can be seenas a schema plus a set of RDDs. </p>
<p><img alt="" src="../images/spark_dataframe_columnar.png"/></p>
<p>Since Dataframe was designed for machine learning applications, it adopts the column-based data structure instead of row based. </p>
<h3 id="question-why-columnar">Question: Why Columnar?</h3>
<p>Hint: How data are used in ML model?</p>
<h3 id="creating-spark-dataframe">Creating Spark Dataframe</h3>
<p>We can convert a Spark rdd into a dataframe.</p>
<pre><code class="language-python">data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)]
r1 = sc.parallelize(data)
df1 = r1.toDF("id", "score")
</code></pre>
<p>Alternatively, we can create DataFrames directly from a CSV file.</p>
<p>Given file <code>hdfs://127.0.0.1:9000/foo.csv</code></p>
<pre><code class="language-csv">foo,bar
1,true
2,false
3,true
4,false
</code></pre>
<pre><code class="language-python">df = sparkSession.read\
     .option("header", "true")\
     .option("inferSchema", "true")\
     .csv("hdfs://127.0.0.1:9000/foo.csv")
df.printSchema()
</code></pre>
<p>shows</p>
<pre><code class="language-text">root
 |-- foo: integer (nullable = true)
 |-- bar: boolean (nullable = true)
</code></pre>
<p>Note in the above, Spark loads a text file (CSV) from HDFS and infers the schema based on the first line and the values.</p>
<h3 id="dataframe-apis">DataFrame APIs</h3>
<p>Let's have tour of the Spark DataFrame APIs by going through some examples.</p>
<p>Here is the data we use in the examples</p>
<pre><code class="language-python">data = [("100001", "Ace", "50043", 90), \
        ("100002", "Brandon", "50043", 95), \
        ("100003", "Cheryl", "50043", 80)]
distData = sc.parallelize(data)
df = distData.toDF(["studentid", "name", \
                 "module", "score"])
df.show()
</code></pre>
<table>
<thead>
<tr>
<th>studentid</th>
<th>name</th>
<th>module</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>100001</td>
<td>Ace</td>
<td>50043</td>
<td>90</td>
</tr>
<tr>
<td>100002</td>
<td>Brandon</td>
<td>50043</td>
<td>95</td>
</tr>
<tr>
<td>100003</td>
<td>Cheryl</td>
<td>50043</td>
<td>80</td>
</tr>
</tbody>
</table>
<h4 id="column-projection">Column Projection</h4>
<p>To project (select the columns) we use the <code>.select()</code> method.</p>
<pre><code class="language-python">df.select(df["studentid"], df["score"]).show() # or

from pyspark.sql.functions import col
df.select(col("studentid"), col("score")).show() 
</code></pre>
<table>
<thead>
<tr>
<th>studentid</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>100001</td>
<td>90</td>
</tr>
<tr>
<td>100002</td>
<td>95</td>
</tr>
<tr>
<td>100003</td>
<td>80</td>
</tr>
</tbody>
</table>
<p>To compute a new column based on the existing one, we use an overloaded version of the <code>.select()</code> method whose first argument is the operation and the second argument is the name of the new column.</p>
<pre><code class="language-python">from pyspark.sql.functions import concat, lit
df.select(concat(df["studentid"]\
         ,lit("@mymail.sutd.edu.sg"))\
   .alias("email")).show()
</code></pre>
<table>
<thead>
<tr>
<th>email</th>
</tr>
</thead>
<tbody>
<tr>
<td>100001@mymail.sut...</td>
</tr>
<tr>
<td>100002@mymail.sut...</td>
</tr>
<tr>
<td>100003@mymail.sut...</td>
</tr>
</tbody>
</table>
<p>For the full set of builtin funtcions for column operations.</p>
<pre><code class="language-url">https://spark.apache.org/docs/3.0.1/api/python/pyspark.sql.html
</code></pre>
<p>There are times we want to create a new column and keeping the old columns.</p>
<pre><code class="language-python">df.withColumn("email",concat(col("studentid"),\
   lit("@mymail.sutd.edu.sg")))\
   .show()
</code></pre>
<table>
<thead>
<tr>
<th>studentid</th>
<th>name</th>
<th>module</th>
<th>score</th>
<th>email</th>
</tr>
</thead>
<tbody>
<tr>
<td>100001</td>
<td>Ace</td>
<td>50043</td>
<td>90</td>
<td>100001@mymail.sut...</td>
</tr>
<tr>
<td>100002</td>
<td>Brandon</td>
<td>50043</td>
<td>95</td>
<td>100002@mymail.sut...</td>
</tr>
<tr>
<td>100003</td>
<td>Cheryl</td>
<td>50043</td>
<td>80</td>
<td>100003@mymail.sut...</td>
</tr>
</tbody>
</table>
<h4 id="row-filtering">Row filtering</h4>
<p>For row filterings, we use <code>.filter()</code> method.</p>
<pre><code class="language-python">df.filter(col("studentid") == "100003").show()
</code></pre>
<table>
<thead>
<tr>
<th>studentid</th>
<th>name</th>
<th>module</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>100003</td>
<td>Cheryl</td>
<td>50043</td>
<td>80</td>
</tr>
</tbody>
</table>
<p>And similar for range filter</p>
<pre><code class="language-python">df.filter(col("score") &gt; 90).show()
</code></pre>
<table>
<thead>
<tr>
<th>studentid</th>
<th>name</th>
<th>module</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>100002</td>
<td>Brandon</td>
<td>50043</td>
<td>95</td>
</tr>
</tbody>
</table>
<p><code>lit()</code> is optional here, pyspark inserts it for us.</p>
<h4 id="group-by-and-aggregation">Group By and Aggregation</h4>
<p>For aggregation, we use <code>.groupBy()</code> </p>
<pre><code class="language-python">df.groupBy("module").avg().show()
</code></pre>
<table>
<thead>
<tr>
<th>module</th>
<th>avg(score)</th>
</tr>
</thead>
<tbody>
<tr>
<td>50043</td>
<td>88.33333333333333</td>
</tr>
</tbody>
</table>
<h4 id="join">Join</h4>
<p>For join, we use <code>.join()</code> </p>
<pre><code class="language-python">moddata = [("50043", "Database and Big Data Systems")]
distmodData = sc.parallelize(moddata)
moddf = distmodData.toDF(["module", "modname"])

df.join(moddf, df["module"] == moddf["module"], "inner")\
   .select(df["studentid"], df["name"], df["module"],\
   df["score"], moddf["modname"]).show()
</code></pre>
<table>
<thead>
<tr>
<th>studentid</th>
<th>name</th>
<th>module</th>
<th>score</th>
<th>modname</th>
</tr>
</thead>
<tbody>
<tr>
<td>100001</td>
<td>Ace</td>
<td>50043</td>
<td>90</td>
<td>Database and Big ...</td>
</tr>
<tr>
<td>100002</td>
<td>Brandon</td>
<td>50043</td>
<td>95</td>
<td>Database and Big ...</td>
</tr>
<tr>
<td>100003</td>
<td>Cheryl</td>
<td>50043</td>
<td>80</td>
<td>Database and Big ...</td>
</tr>
</tbody>
</table>
<h2 id="spark-sql">Spark SQL</h2>
<p>Besides Spark RDD, Spark allows program to use SQL query to perform data transformation and action. </p>
<p>For example, </p>
<pre><code class="language-python">df.createOrReplaceTempView("students")
spark.sql("SELECT * FROM students").show()
</code></pre>
<table>
<thead>
<tr>
<th>studentid</th>
<th>name</th>
<th>module</th>
<th>score</th>
</tr>
</thead>
<tbody>
<tr>
<td>100001</td>
<td>Ace</td>
<td>50043</td>
<td>90</td>
</tr>
<tr>
<td>100002</td>
<td>Brandon</td>
<td>50043</td>
<td>95</td>
</tr>
<tr>
<td>100003</td>
<td>Cheryl</td>
<td>50043</td>
<td>80</td>
</tr>
</tbody>
</table>
<p>With some notebook support, we can even use SQL to perform
data visualization.</p>
<h2 id="spark-machine-learning">Spark Machine Learning</h2>
<p>Spark comes with two differen Machine Learning libraries.</p>
<ul>
<li><code>MLLib</code> package - for RDD</li>
<li><code>ML</code> package - for dataframe (and dataset)</li>
</ul>
<h3 id="mllib-package">MLLib package</h3>
<p>MLLib package offers a lower level access of data type such as vectors and label points. </p>
<h4 id="vectors">Vectors</h4>
<p>In Spark, vectors are local data collection (non-distributed). 
There are dense vectors and sparse vectors.</p>
<ul>
<li>For dense vector - all values need to be specified when it is created</li>
</ul>
<pre><code class="language-python">from pyspark.mllib.linalg import * 
dv = Vectors.dense(1.0, 0.0, 3.0)
</code></pre>
<ul>
<li>For sparse vector - we don't need to specify all the value, instead we specify the size of the vector as well as the non-zero values.</li>
</ul>
<pre><code class="language-python">sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0]) # or
sv2 = Vectors.sparse(3, [(0, 1.0), (2, 3.0)])
</code></pre>
<h4 id="labeled-points">Labeled points</h4>
<p>With features extracted as vectors. We need to find a way to label them.</p>
<p>Spark MLLib comes with its own labeled point data type</p>
<pre><code class="language-python">from pyspark.mllib.regression import *
# Create a labeled point with a positive label
# and a dense feature vector.
pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))
# Create a labeled point with a negative label
# and a sparse feature vector.
neg = LabeledPoint(0.0, Vectors.sparse(3, \
      [(0, 1.0), (2, 3.0)]))
</code></pre>
<h4 id="training-an-inference">Training an inference</h4>
<p>With labeled points defined and extracted, assuming</p>
<pre><code class="language-python">pos = ... # RDD of labeled points
neg = ... # RDD of labeled points
</code></pre>
<p>we train the model</p>
<pre><code class="language-python">from pyspark.mllib.classification import SVMWithSGD
training = pos + neg
numIteration = 20
model = SVMWithSGD.train(training, numIterations)
</code></pre>
<p>which is a support vector machine with SGD algorithm.</p>
<p>To perform inference, we need to feed a new sample as a vector to the model.</p>
<pre><code class="language-python">newInstance = Vectors.dense(1.0, 2.0, 3.0)
model.predict(newInstance)
</code></pre>
<h3 id="ml-package">ML Package</h3>
<p>As the ML package is targetting the higher level data structures (Dataframe and Dataset), machine learning models in ML package are built using the pipeline. </p>
<h4 id="the-training-pipeline">The Training Pipeline</h4>
<p>One of the training pipelines is known as the estimator</p>
<p><img alt="Estimator Pipeline" src="https://spark.apache.org/docs/latest/img/ml-Pipeline.png"/></p>
<p>In the diagram above, it illustrate the pipeline of training a classifier using logistic regression. </p>
<pre><code class="language-python">from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer
data = spark.createDataFrame([
    (0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0),
    (4, "spark is scaling", 1.0),
    (5, "random stuff", 0.0)
], ["id", "text", "label"])

train, test = data.randomSplit([0.7, 0.3], seed=12345)
# Configure an estimator pipeline, 
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

# Fit the pipeline to train
model = pipeline.fit(train)
</code></pre>
<h4 id="the-transformer-pipeline">The transformer pipeline</h4>
<p>The inference pipeline on the other hand is known as the transformer </p>
<p><img alt="Transformer Pipeline" src="https://spark.apache.org/docs/latest/img/ml-PipelineModel.png"/></p>
<pre><code class="language-python"># Configure an Inference pipeline
# Note now model include tokenizterm hashingTF, and lr
pipeline_model = PipelineModel(stages=[model]) 
prediction = pipeline_model.transform(test)
result = prediction.select("id", "text", "probability", "prediction")
result.show()
</code></pre>
<h4 id="sample-code">Sample code</h4>
<pre><code>https://colab.research.google.com/drive/1ZI-BG2XaB3AOyzPrXeqO7xqNGYycCJ-U?usp=sharing
</code></pre>
<h2 id="spark-streaming">Spark Streaming</h2>
<p>Spark offers Streaming API which handles real time (infinite) input data. </p>
<p>The real-timeness is approxmiated by chopping the data stream into small batches. 
These small batches are fed to the spark application. </p>
<p><img alt="" src="https://spark.apache.org/docs/latest/img/streaming-flow.png"/></p>
<p>For example the following is a simplified version of a data streaming application that computes the page views by URL over time.</p>
<pre><code class="language-python">from pyspark import SparkContext
from pyspark.streaming import StreamingContext
sc = SparkContext("local[2]", "PageView")
ssc = StreamingContext(sc, 1)

lines = ssc.socketTextStream("localhost", 9999)
pageViews = lines.map(lambda l:parse(l))
ones = pageViews.map(lambda x: (x.url, 1))
counts = ones.runningReduce(lambda x,y: x+y)
</code></pre>
<h2 id="additional-references">Additional References</h2>
<ul>
<li>Spark Dataframe</li>
<li>https://spark.apache.org/docs/latest/sql-getting-started.html</li>
<li>Spark MLLib and ML package</li>
<li>https://spark.apache.org/docs/latest/ml-guide.html</li>
<li>Spark Streaming </li>
<li>https://spark.apache.org/docs/latest/streaming-programming-guide.html</li>
</ul></div>
</div>
</div>
<footer class="col-md-12">
<hr/>
<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
</footer>
<script src="../../js/jquery-3.6.0.min.js"></script>
<script src="../../js/bootstrap.min.js"></script>
<script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
<script src="../../js/base.js"></script>
<script src="../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="../../search/main.js"></script>
<div aria-hidden="true" aria-labelledby="searchModalLabel" class="modal" id="mkdocs_search_modal" role="dialog" tabindex="-1">
<div class="modal-dialog modal-lg">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="searchModalLabel">Search</h4>
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
</div>
<div class="modal-body">
<p>From here you can search these documents. Enter your search terms below.</p>
<form>
<div class="form-group">
<input class="form-control" id="mkdocs-search-query" placeholder="Search..." title="Type search term here" type="search"/>
</div>
</form>
<div data-no-results-text="No results found" id="mkdocs-search-results"></div>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div><div aria-hidden="true" aria-labelledby="keyboardModalLabel" class="modal" id="mkdocs_keyboard_modal" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
</div>
<div class="modal-body">
<table class="table">
<thead>
<tr>
<th style="width: 20%;">Keys</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td class="help shortcut"><kbd>?</kbd></td>
<td>Open this help</td>
</tr>
<tr>
<td class="next shortcut"><kbd>n</kbd></td>
<td>Next page</td>
</tr>
<tr>
<td class="prev shortcut"><kbd>p</kbd></td>
<td>Previous page</td>
</tr>
<tr>
<td class="search shortcut"><kbd>s</kbd></td>
<td>Search</td>
</tr>
</tbody>
</table>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div>
<script type="module">import mermaid from "https://unpkg.com/mermaid@10.4.0/dist/mermaid.esm.min.mjs";
mermaid.initialize({});</script></body>
</html>
