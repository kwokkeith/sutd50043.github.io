<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<link href="../../img/favicon.ico" rel="shortcut icon"/>
<title>L10 hdfs - Database System and Big Data 50.043</title>
<link href="../../css/bootstrap.min.css" rel="stylesheet"/>
<link href="../../css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../css/base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</head>
<body>
<div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
<div class="container">
<a class="navbar-brand" href="../..">Database System and Big Data 50.043</a>
<!-- Expander button -->
<button class="navbar-toggler" data-target="#navbar-collapse" data-toggle="collapse" type="button">
<span class="navbar-toggler-icon"></span>
</button>
<!-- Expanded navigation -->
<div class="navbar-collapse collapse" id="navbar-collapse">
<!-- Main navigation -->
<ul class="nav navbar-nav">
<li class="navitem">
<a class="nav-link" href="../..">Home</a>
</li>
<li class="navitem">
<a class="nav-link" href="../../project/">Project</a>
</li>
<li class="navitem">
<a class="nav-link" href="../l1_course_handout/">Handout</a>
</li>
</ul>
<ul class="nav navbar-nav ml-auto">
<li class="nav-item">
<a class="nav-link" data-target="#mkdocs_search_modal" data-toggle="modal" href="#">
<i class="fa fa-search"></i> Search
                            </a>
</li>
</ul>
</div>
</div>
</div>
<div class="container">
<div class="row">
<div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
<div class="navbar-header">
<button class="navbar-toggler collapsed" data-target="#toc-collapse" data-toggle="collapse" title="Table of Contents" type="button">
<span class="fa fa-angle-down"></span>
</button>
</div>
<div class="navbar-collapse collapse card bg-secondary" id="toc-collapse">
<ul class="nav flex-column">
<li class="nav-item" data-level="1"><a class="nav-link" href="#50043-hadoop-distributed-file-system">50.043 Hadoop Distributed File System</a>
<ul class="nav flex-column">
<li class="nav-item" data-level="2"><a class="nav-link" href="#learning-outcomes">Learning Outcomes</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#hadoop">Hadoop</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#hadoop-distributed-file-system">Hadoop Distributed File System</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#hdfs-file-system-model">HDFS File system model</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#hdfs-architecture">HDFS Architecture</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div></div>
<div class="col-md-9" role="main">
<p>--
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
--</p>
<h1 id="50043-hadoop-distributed-file-system">50.043 Hadoop Distributed File System</h1>
<h2 id="learning-outcomes">Learning Outcomes</h2>
<p>By the end of this unit, you should be able to</p>
<ol>
<li>Explain the file system model of HDFS</li>
<li>Explain the architecture of HDFS</li>
<li>Explain the replication placement strategy</li>
<li>Explain the operation of HDFS client</li>
<li>Explain the use of erasure coding</li>
</ol>
<h2 id="hadoop">Hadoop</h2>
<p>Hadoop is one of the widely-used framework for big data. It was started in 2005 by a group of Yahoo! scientists. The initial objective was to provide a scalable backend for the nutch crawler. </p>
<p>Over time Hadoop has evolved into a fully fledged distributed data storage and processing framework. It has a few major components</p>
<ul>
<li>MapReduce - the data processing layer</li>
<li>Hadoop Distributed File System - the data storage layer</li>
<li>YARN - the resource management layer</li>
</ul>
<p>In this unit, our focus is on HDFS.</p>
<h2 id="hadoop-distributed-file-system">Hadoop Distributed File System</h2>
<p>Hadoop Distrubuted File System was developed based on the white paper of a closed source project the google file system. </p>
<ul>
<li>The Google file system, <em>Sanjay Ghemawat, Howard Bradley Gobioff  and Shun-Tak Leung</em>, ACM SIGOPS 2005<ul>
<li>https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf</li>
</ul>
</li>
</ul>
<h3 id="google-file-system">Google File System</h3>
<p>Google File System was created in the period of time where databases were expensive, but hard disks were cheap. They need to store a lot of non relational data (crawled data), i.e. data are in huge volume. These data needed to be processed by batches in huge sizes in a relative short amount of time, i.e. updating the search engine indices. </p>
<p>The only alternative during that time was network file system (NFS), which deemed to be limited by its original use cases. </p>
<ul>
<li>a file must reside on one and only one machine.</li>
<li>there is no reliability guarantee, data loss could be hard to recovered</li>
<li>it is not scalable, a computation over a huge batch of data cannot be distributed to multiple processors.</li>
<li>there is no builtin replication mechanism, which leads to a high fail rate.</li>
<li>file accessed over the network would incur huge amount of network I/O</li>
</ul>
<p>The google team then identified a list of features that required by the search engine indexing use case, and ordered them according to the priority</p>
<ol>
<li>support many clients</li>
<li>support many disks</li>
<li>support file in size of petabytes</li>
<li>offer high fault tolerance</li>
<li>allow file read/file write like normal file system</li>
</ol>
<p>It turned out no system can achieve all the above requirements. The team decided to drop the last one. </p>
<p>By design, Google file system is a distributed file system that supports only sequential read and append operation. </p>
<p>HDFS inherited the design of Google file system.</p>
<h2 id="hdfs-file-system-model">HDFS File system model</h2>
<p>Like many conventional file systems, HDFS follows a hierarchical name space, i.e. data are organized by tree like structure, where each branch is a folder (sub folder) and the leaf nodes are the file.</p>
<pre><code>|- A
|  |- B
|     |- mydoc.txt
|  |- B'
|  |- B''
|
|- A'
|- A''

</code></pre>
<p>For instance in the file system above, there are three folders under the root directory, namely <code>A</code>, <code>A'</code> and <code>A''</code>.  Under <code>A</code>, there are three sub folders, <code>B</code>, <code>B'</code> and <code>B''</code>. Under <code>B</code>, we find a file <code>mydoc.txt</code></p>
<p><img alt="" src="../images/mydoc_fs.png"/></p>
<p>In the above diagram, we find the illustration of the data storage of the file <code>mydoc.txt</code>.  The first data block <code>Block 0</code> stores the first level of folders. Folder <code>A</code> is a reference pointing at the data block <code>Block 1</code>, which stores the list of subfolders within. Sub-folder <code>B</code> is a reference pointing at the data block <code>Block 2</code>. <code>mydoc.txt</code> is the first entry in <code>Block 2</code>, which is an array of pointers pointing at the actual file data. The data blocks <code>Block 0</code>, <code>Block 1</code> and <code>Block 2</code> are known as the meta data. File data block (in grey color) are the real data. </p>
<p>There are several merits of such a file system model. </p>
<ol>
<li>It is a simple abstraction.</li>
<li>It supports very large files, (could be larger than a single physical disk)</li>
<li>Data can be distributed to multiple disks in multiple hosts.</li>
</ol>
<h3 id="choice-of-block-size">Choice of Block Size</h3>
<p>Building upon the above model, the next design decision to be made is the size of the block. In normal file system, (i.e .OSes), the size of a block is around 4KB. For relational database system, the block size can range from 4 to 32KB. For HDFS the default block size is 128MB, which can be changed via system configuration.</p>
<p>The advantage of a larger block size is to fetch more data in a single file IO operation, in the expense of larger unfilled disk space (due to data fragmentation).
For HDFS, since files are written via append, data fragmentation is minimized. </p>
<h2 id="hdfs-architecture">HDFS Architecture</h2>
<p>The HDFS architecture follows a master-worker pattern. </p>
<p><img alt="" src="../images/hdfsarchitecture.png"/></p>
<p>The master node, which is also known as the <strong>name node</strong>, keeps track of a set of data node. </p>
<p>The meta data of the files are stored in the name node. A secondary name node is provisioned for the backup purposes. </p>
<p>The real data are stored and replicated among the data nodes. By default HDFS recommends there should be at least 3 copies of the same data block, i.e. <strong>replication factor</strong> = 3. Replication factors 5 or 7 are also recommended. </p>
<p>A client, i.e. an application that tries to access a file in the HDFS, has to make the requests to the name node and waits for the responses. </p>
<h3 id="replication-and-replica-placement-policy">Replication and Replica placement policy</h3>
<p>Replication is to increase data survivability in case of hardware failure. But how and where shall we store / distribute the replicas of a data block? To maximize the survivability, it might be beneficial to take into account the actual location of the data nodes. </p>
<p>In data centers, servers are physically mounted and connected in racks. Multiple servers can be placed in a rack which shares a power source and network switch (with redudancy). In case of power or network failure, the entire rack is affected. 
Hence, it would be wise not to store replicas of a data block in servers located in the same rack. On the other hand intra rack data transfer will be more cost effective. </p>
<p>In a HDFS cluster, when the rack information is present, the replicas of a data block are distributed as follows </p>
<ul>
<li>Max 1 replica per datanode</li>
<li>Max 2 replicas per rack</li>
<li>Number of racks for replication should be less than the replication factor</li>
</ul>
<p>For example, we consider the following setup with 3 racks, each rack contains 3 datanodes. 3 data blocks need to be stored and replicated with RF = 3.</p>
<p><img alt="" src="../images/replica-placement.png"/></p>
<p>The first copy of data block <code>A</code> is stored in data node <code>1</code>.  The 2nd copy must not be stored in any other data node in rack <code>1</code>. In this case the system randomly picks one rack out of <code>2</code> and <code>3</code>. It places the 2nd copy in data node <code>5</code> of rack <code>2</code> in this case. The 3rd replica can be placed in any rack. In this case put it in data node <code>6</code>.</p>
<h3 id="hdfs-client-operation">HDFS client operation</h3>
<h4 id="read">Read</h4>
<p>When a client attempts to read a file in the HDFS.</p>
<ol>
<li>The client requests for the data block locations of the file from the name node.</li>
<li>The name node returns the list of data block locations to the client.</li>
<li>Based on the data block locations, the client requests for the data blocks from the data node.</li>
<li>The data nodes replied the client with the data being requested. </li>
</ol>
<div class="mermaid">graph LR;
  client --1. file path--&gt; NN["name node"]
  NN["name node"] --2. block locations --&gt; client
  client --3. block location --&gt; DN1
  client --3. block location --&gt; DN2
  client --3. block location --&gt; DN3
  DN1["data node 1"] --4. data --&gt; client
  DN2["data node 2"] --4. data --&gt; client
  DN3["data node 3"] --4. data --&gt; client
</div>
<h4 id="write">Write</h4>
<p>When a client attempts to write/append to a file in the HDFS.</p>
<ol>
<li>The client creates the meta data entry in the name node.</li>
<li>The name node allocates and returns the first block locations to the client.</li>
<li>Based on the first block locations the client initializes the data writing pipeline.</li>
<li>The data will be sent in the daisy chain manner from the client to the data nodes.</li>
<li>When the data is successfully transmitted, an acknowledgement will be sent to data source; When some data block (replica) write operation fails, the information is recorded by the name node and the operation will be re-executed on some data node.</li>
<li>When the last data block is written, the client initiates a close request to the name node. </li>
<li>The name node checks with the data nodes to ensure the minimum replica requirement is met. </li>
<li>The data nodes reports the minimum replica status back to the name node.</li>
<li>The name node sends the acknowledgement to the client notifying that the file write operation was successful.</li>
</ol>
<div class="mermaid">graph LR 
  client --1. create--&gt; NN["name node"]
  NN["name node"] --2. first block locations--&gt; client
  client --3. organize pipeline; 4. send data--&gt; DN1["data node 1"]
  DN1 --5. acknowledge--&gt; client
  DN1["data node 1"] --3. organize pipeline; 4. send data--&gt; DN2["data node 2"]
  DN2 --5. acknowledge--&gt; DN1
  DN2["data node 2"] --3. organize pipeline; 4. send data--&gt; DN3["data node 3"]
  DN3 --5. acknowledge--&gt;DN2
  client --6. close --&gt; NN["name node"]
  NN --7. check minimum replica --&gt; DN1
  NN --7. check minimum replica --&gt; DN2
  NN --7. check minimum replica --&gt; DN3
  DN1 --8. report --&gt; NN
  DN2 --8. report --&gt; NN
  DN3 --8. report --&gt; NN
</div>
<p>In case of the minimum replica requirement is not satisfied, the name node will arrange re-distribution the replica. </p>
<h3 id="alternative-to-data-replication">Alternative To Data Replication</h3>
<p>Replicating data and redistributing them is a simple and effective way to improve fault tolerance. However, one draw-back of data replication is the data overhead. A system with replication factor <code>n</code> will lead to <code>(n-1) * 100%</code> storage overhead and <code>(1/n)</code> storage efficiency. </p>
<p>For example, when <code>n = 3</code>, we have <code>200%</code> storage overhead and <code>0.3333</code> storage efficiency.</p>
<p>One of the popular alternatives to data replication is <em>erasure coding</em>. The main idea of erasure coding is not to replicate data, instead it generates a parity for each segment of actual data, with which the actual data could be restored in case of data loss. The idea behind was inspired by the Exclusive OR (XOR) operation for bits. </p>
<h4 id="properties-of-xor">Properties of XOR</h4>
<p>Recall that XOR operation <span class="arithmatex">\(\bigoplus\)</span> on bits</p>
<table>
<thead>
<tr>
<th>IN</th>
<th>IN</th>
<th>XOR</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>having some nice properties</p>
<div class="arithmatex">\[ X \bigoplus Y = Y \bigoplus X \]</div>
<div class="arithmatex">\[ (X \bigoplus Y) \bigoplus Z = X \bigoplus (Y \bigoplus Z) \]</div>
<div class="arithmatex">\[ X \bigoplus Y = Z \Rightarrow X \bigoplus Z = Y \]</div>
<p>We can use the result of XOR to recover if one of the inputs is lost.</p>
<p>We are trying to transfer this idea to increase the recoverability of the HDFS data. Let one of the input to be the actual data, the other input is a special key, the output is the data with the parity.</p>
<p>One issue remained is that we are dealing with more than bit of actual data, and we want to keep the special key and the data parity relatively small.</p>
<h4 id="reed-solomon-algorithm">Reed-solomon Algorithm</h4>
<p>Addressing the issue leads us to the Reed-solomon algorithm.</p>
<p>The algorithm requires a <em>Generator Matrix</em> <span class="arithmatex">\(G^T\)</span> (the special key), which has the following requirement. </p>
<ul>
<li>It is <span class="arithmatex">\(n \times k\)</span> matrix</li>
<li>all <span class="arithmatex">\(k \times k\)</span> sub matrices in <span class="arithmatex">\(G^T\)</span> are non-singular (the inverse matrices exist).</li>
</ul>
<p>The actual data (from HDFS) are arranged into a <span class="arithmatex">\(k \times L\)</span> matrix. What <span class="arithmatex">\(L\)</span> is not important for demonstration purpose, we assume may let <span class="arithmatex">\(L = 1\)</span> for simplicity.</p>
<p><img alt="Erasure Coding" src="../images/hdfs-erasure-f1.png"/></p>
<p>In the above diagram we multiply the generator matrix with the actual data, which produces a code word, consists of the data and the parity. </p>
<p>Let's instantiate the diagram with concrete numbers. </p>
<div class="arithmatex">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\ 
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 1 
\end{bmatrix} 
\times
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0
\end{bmatrix} 
= 
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 2
\end{bmatrix} 
\]</div>
<p>Thanks to the properties of the left operand <span class="arithmatex">\(G^T\)</span>, the first four rows of the code word is identical to the data. </p>
<p>In the HDFS we will store the <span class="arithmatex">\(G^T\)</span> in some safe place, i.e. name node with backup and the codeword is distributed among the data nodes </p>
<p>Now let's say we lose the 2nd and 4th rows in the codeword and try to recover the data. </p>
<div class="arithmatex">\[
\begin{bmatrix}
1 \\ 1 \\ 1 \\ 2
\end{bmatrix} 
\]</div>
<p>We remove the correspondent rows from the <span class="arithmatex">\(G^T\)</span>, </p>
<div class="arithmatex">\[
G^{T^{-1}}_{\neg{(1,3)}} = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\ 
0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 1 
\end{bmatrix} 
\]</div>
<p>Based on the matrix multiplication the following equation holds</p>
<div class="arithmatex">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\ 
0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 1 
\end{bmatrix} 
\times
\begin{bmatrix}
? \\ ? \\ ? \\ ?
\end{bmatrix} 
= 
\begin{bmatrix}
1 \\ 1 \\ 1 \\ 2
\end{bmatrix} 
\]</div>
<p>where the question marks are the actual to be reovered. </p>
<p>Thanks to the fact that all <span class="arithmatex">\(k \times k\)</span> sub matrices in <span class="arithmatex">\(G^T\)</span> are non-singular, the inverse of <span class="arithmatex">\(G^{T^{-1}}_{\neg{(1,3)}}\)</span> exists </p>
<div class="arithmatex">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
-1 &amp; -1 &amp; 0 &amp; 1 
\end{bmatrix}
\times
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\ 
0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 1 
\end{bmatrix} 
\times
\begin{bmatrix}
? \\ ? \\ ? \\ ?
\end{bmatrix} 
= 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
-1 &amp; -1 &amp; 0 &amp; 1 
\end{bmatrix}
\times
\begin{bmatrix}
1 \\ 1 \\ 1 \\ 2
\end{bmatrix} 
\]</div>
<p>We cancel <span class="arithmatex">\(G^{T^{-1}}_{\neg{(1,3)}} \times G^T_{\neg{(1,3)}}\)</span> from the LHS
$$
\begin{bmatrix}
1 \ 0 \ 1 \ 0
\end{bmatrix} 
= 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \
0 &amp; -1 &amp; 1 &amp; 0 \
0 &amp; 1 &amp; 0 &amp; 0 \
-1 &amp; -1 &amp; 0 &amp; 1 
\end{bmatrix}
\times
\begin{bmatrix}
1 \ 1 \ 1 \ 2
\end{bmatrix} 
$$</p>
<p>We recover the lost data.</p>
<h4 id="erasure-coding-storage-overhead-and-data-efficiency">Erasure Coding storage overhead and data efficiency</h4>
<p>Erase coding is more <em>economical</em> compared to data replication. <span class="arithmatex">\(G^T\)</span> is fixed for all data, hence its overhead is neglectable. 
We write <span class="arithmatex">\(RS(k,m)\)</span> to denote that <span class="arithmatex">\(G^T\)</span> is a matrix of <span class="arithmatex">\((k+m) \times k\)</span> and <span class="arithmatex">\(k\)</span> is the number of rows of the actual data matrix and <span class="arithmatex">\(m\)</span> is the number of parity rows. We have <span class="arithmatex">\(m/k\)</span> storage overhead and <span class="arithmatex">\(k / (k + m)\)</span> storage efficiency.</p>
<p>One crucial observation is that we can always recover the a codeword with <span class="arithmatex">\((k +m) \times 1\)</span> when the number of lost cells is less than or equal to <span class="arithmatex">\(m\)</span>.</p>
<h4 id="question">Question</h4>
<p>What happen when <span class="arithmatex">\(L &gt; 1\)</span>? Hint: you may think about how matrix multiplication works.</p></div>
</div>
</div>
<footer class="col-md-12">
<hr/>
<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
</footer>
<script src="../../js/jquery-3.6.0.min.js"></script>
<script src="../../js/bootstrap.min.js"></script>
<script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
<script src="../../js/base.js"></script>
<script src="../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="../../search/main.js"></script>
<div aria-hidden="true" aria-labelledby="searchModalLabel" class="modal" id="mkdocs_search_modal" role="dialog" tabindex="-1">
<div class="modal-dialog modal-lg">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="searchModalLabel">Search</h4>
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
</div>
<div class="modal-body">
<p>From here you can search these documents. Enter your search terms below.</p>
<form>
<div class="form-group">
<input class="form-control" id="mkdocs-search-query" placeholder="Search..." title="Type search term here" type="search"/>
</div>
</form>
<div data-no-results-text="No results found" id="mkdocs-search-results"></div>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div><div aria-hidden="true" aria-labelledby="keyboardModalLabel" class="modal" id="mkdocs_keyboard_modal" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
</div>
<div class="modal-body">
<table class="table">
<thead>
<tr>
<th style="width: 20%;">Keys</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td class="help shortcut"><kbd>?</kbd></td>
<td>Open this help</td>
</tr>
<tr>
<td class="next shortcut"><kbd>n</kbd></td>
<td>Next page</td>
</tr>
<tr>
<td class="prev shortcut"><kbd>p</kbd></td>
<td>Previous page</td>
</tr>
<tr>
<td class="search shortcut"><kbd>s</kbd></td>
<td>Search</td>
</tr>
</tbody>
</table>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div>
<script type="module">import mermaid from "https://unpkg.com/mermaid@10.4.0/dist/mermaid.esm.min.mjs";
mermaid.initialize({});</script></body>
</html>
