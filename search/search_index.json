{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to 50.043 For course handout click here . For project info click here .","title":"Home"},{"location":"#welcome-to-50043","text":"For course handout click here . For project info click here .","title":"Welcome to 50.043"},{"location":"project/","text":"ISTD 50043 Group Project Welcome! Labs Your project consists of four labs. You are strongly advised to start early. Lab 1 : due 3/3 11.59pm . Lab 2 : due 17/3 11.59pm . Lab 3 : due 7/4 11.59pm . Lab 4 : due 21/4 11.59pm . Each lab is given 15 points : 12 for passing all the tests, and 3 for the report. Important In addition to the academic integrity policy (as discussed in the lecture), take note that: * You cannot make your Github repository for the labs public. * You cannot copy and paste code from other people or any other sources. * You cannot share your solutions. Acknowledgement The labs are modified from the MIT 6830 course. We thank the MIT staff for the materials.","title":"Project"},{"location":"project/#istd-50043-group-project","text":"Welcome!","title":"ISTD 50043 Group Project"},{"location":"project/#labs","text":"Your project consists of four labs. You are strongly advised to start early. Lab 1 : due 3/3 11.59pm . Lab 2 : due 17/3 11.59pm . Lab 3 : due 7/4 11.59pm . Lab 4 : due 21/4 11.59pm . Each lab is given 15 points : 12 for passing all the tests, and 3 for the report.","title":"Labs"},{"location":"project/#important","text":"In addition to the academic integrity policy (as discussed in the lecture), take note that: * You cannot make your Github repository for the labs public. * You cannot copy and paste code from other people or any other sources. * You cannot share your solutions.","title":"Important"},{"location":"project/#acknowledgement","text":"The labs are modified from the MIT 6830 course. We thank the MIT staff for the materials.","title":"Acknowledgement"},{"location":"notes/","text":"notes","title":"notes"},{"location":"notes/#notes","text":"","title":"notes"},{"location":"notes/l10_hdfs/","text":"-- header-includes: - \\usepackage{tikz} - \\usepackage{pgfplots} -- 50.043 Hadoop Distributed File System Learning Outcomes By the end of this unit, you should be able to Explain the file system model of HDFS Explain the architecture of HDFS Explain the replication placement strategy Explain the operation of HDFS client Explain the use of erasure coding Hadoop Hadoop is one of the widely-used frameworks for big data. It was started in 2005 by a group of Yahoo! scientists. The initial objective was to provide a scalable backend for the nutch crawler. Over time Hadoop has evolved into a fully fledged distributed data storage and processing framework. It has a few major components MapReduce - the data processing layer Hadoop Distributed File System - the data storage layer YARN - the resource management layer In this unit, our focus is on HDFS. Hadoop Distributed File System Hadoop Distrubuted File System was developed based on the white paper of a closed source project the google file system. The Google file system, Sanjay Ghemawat, Howard Bradley Gobioff and Shun-Tak Leung , ACM SIGOPS 2005 https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf Google File System Google File System was created in the period of time where databases were expensive, but hard disks were cheap. They need to store a lot of non relational data (crawled data), i.e. data are in huge volume. These data needed to be processed by batches in huge sizes in a relative short amount of time, i.e. updating the search engine indices. The only alternative during that time was network file system (NFS), which deemed to be limited by its original use cases. a file must reside on one and only one machine. there is no reliability guarantee, data loss could be hard to recover it is not scalable, a computation over a huge batch of data cannot be distributed to multiple processors. there is no builtin replication mechanism, which leads to a high fail rate. file accessed over the network would incur huge amount of network I/O The google team then identified a list of features that required by the search engine indexing use case, and ordered them according to the priority support many clients support many disks support file in size of petabytes offer high fault tolerance allow file read/file write like normal file system It turned out no system can achieve all the above requirements. The team decided to drop the last one. By design, Google file system is a distributed file system that supports only sequential read and append operation. HDFS inherited the design of Google file system. HDFS File system model Like many conventional file systems, HDFS follows a hierarchical name space, i.e. data are organized by tree like structure, where each branch is a folder (sub folder) and the leaf nodes are the file. |- A | |- B | |- mydoc.txt | |- B' | |- B'' | |- A' |- A'' For instance in the file system above, there are three folders under the root directory, namely A , A' and A'' . Under A , there are three sub folders, B , B' and B'' . Under B , we find a file mydoc.txt In the above diagram, we find the illustration of the data storage of the file mydoc.txt . The first data block Block 0 stores the first level of folders. Folder A is a reference pointing at the data block Block 1 , which stores the list of subfolders within. Sub-folder B is a reference pointing at the data block Block 2 . mydoc.txt is the first entry in Block 2 , which is an array of pointers pointing at the actual file data. The data blocks Block 0 , Block 1 and Block 2 are known as the meta data. File data block (in grey color) are the real data. There are several merits of such a file system model. It is a simple abstraction. It supports very large files, (could be larger than a single physical disk) Data can be distributed to multiple disks in multiple hosts. Choice of Block Size Building upon the above model, the next design decision to be made is the size of the block. In normal file system, (i.e .OSes), the size of a block is around 4KB. For relational database system, the block size can range from 4 to 32KB. For HDFS the default block size is 128MB, which can be changed via system configuration. The advantage of a larger block size is to fetch more data in a single file IO operation, in the expense of larger unfilled disk space (due to data fragmentation). For HDFS, since files are written via append, data fragmentation is minimized. HDFS Architecture The HDFS architecture follows a master-worker pattern. The master node, which is also known as the name node , keeps track of a set of data nodes. The meta data of the files is stored in the name node. A secondary name node is provisioned for the backup purposes. The real data are stored and replicated among the data nodes. By default HDFS recommends there should be at least 3 copies of the same data block, i.e. replication factor = 3. Replication factors 5 or 7 are also recommended. A client, i.e. an application that tries to access a file in the HDFS, has to make the requests to the name node and waits for the responses. Replication and Replica placement policy Replication is to increase data survivability in case of hardware failure. But how and where shall we store / distribute the replicas of a data block? To maximize the survivability, it might be beneficial to take into account the actual location of the data nodes. In data centers, servers are physically mounted and connected in racks. Multiple servers can be placed in a rack which shares a power source and network switch (with redudancy). In case of power or network failure, the entire rack is affected. Hence, it would be wise not to store replicas of a data block in servers located in the same rack. On the other hand intra rack data transfer will be more cost effective. In a HDFS cluster, when the rack information is present, the replicas of a data block are distributed as follows Max 1 replica per datanode Max 2 replicas per rack Number of racks for replication should be less than the replication factor For example, we consider the following setup with 3 racks, each rack contains 3 datanodes. 3 data blocks need to be stored and replicated with RF = 3. The first copy of data block A is stored in data node 1 . The 2nd copy must not be stored in any other data node in rack 1 . In this case the system randomly picks one rack out of 2 and 3 . It places the 2nd copy in data node 5 of rack 2 in this case. The 3rd replica can be placed in any rack. In this case put it in data node 6 . HDFS client operation Read When a client attempts to read a file in the HDFS. The client requests for the data block locations of the file from the name node. The name node returns the list of data block locations to the client. Based on the data block locations, the client requests for the data blocks from the data node. The data nodes reply the client with the data being requested. graph LR; client --1. file path--> NN[\"name node\"] NN[\"name node\"] --2. block locations --> client client --3. block location --> DN1 client --3. block location --> DN2 client --3. block location --> DN3 DN1[\"data node 1\"] --4. data --> client DN2[\"data node 2\"] --4. data --> client DN3[\"data node 3\"] --4. data --> client Write When a client attempts to write/append to a file in the HDFS. The client creates the meta data entry in the name node. The name node allocates and returns the first block locations to the client. Based on the first block locations the client initializes the data writing pipeline. The data will be sent in the daisy chain manner from the client to the data nodes. When the data is successfully transmitted, an acknowledgement will be sent to data source; When some data block (replica) write operation fails, the information is recorded by the name node and the operation will be re-executed on some data node. When the last data block is written, the client initiates a close request to the name node. The name node checks with the data nodes to ensure the minimum replica requirement is met. The data nodes reports the minimum replica status back to the name node. The name node sends the acknowledgement to the client notifying that the file write operation was successful. graph LR client --1. create--> NN[\"name node\"] NN[\"name node\"] --2. first block locations--> client client --3. organize pipeline; 4. send data--> DN1[\"data node 1\"] DN1 --5. acknowledge--> client DN1[\"data node 1\"] --3. organize pipeline; 4. send data--> DN2[\"data node 2\"] DN2 --5. acknowledge--> DN1 DN2[\"data node 2\"] --3. organize pipeline; 4. send data--> DN3[\"data node 3\"] DN3 --5. acknowledge -->DN2 client --6. close --> NN[\"name node\"] NN --7. check minimum replica --> DN1 NN --7. check minimum replica --> DN2 NN --7. check minimum replica --> DN3 DN1 --8. report --> NN DN2 --8. report --> NN DN3 --8. report --> NN In case of the minimum replica requirement is not satisfied, the name node will arrange re-distribution the replica. Alternative To Data Replication Replicating data and redistributing them is a simple and effective way to improve fault tolerance. However, one draw-back of data replication is the data overhead. A system with replication factor n will lead to (n-1) * 100% storage overhead and (1/n) storage efficiency. For example, when n = 3 , we have 200% storage overhead and 0.3333 storage efficiency. One of the popular alternatives to data replication is erasure coding . The main idea of erasure coding is not to replicate data, instead it generates a parity for each segment of actual data, with which the actual data could be restored in case of data loss. The idea behind was inspired by the Exclusive OR (XOR) operation for bits. Properties of XOR Recall that XOR operation \\(\\bigoplus\\) on bits IN IN XOR 0 0 0 1 0 1 0 1 1 1 1 0 having some nice properties \\[ X \\bigoplus Y = Y \\bigoplus X \\] \\[ (X \\bigoplus Y) \\bigoplus Z = X \\bigoplus (Y \\bigoplus Z) \\] \\[ X \\bigoplus Y = Z \\Rightarrow X \\bigoplus Z = Y \\] We can use the result of XOR to recover if one of the inputs is lost. We are trying to transfer this idea to increase the recoverability of the HDFS data. Let one of the input to be the actual data, the other input is a special key, the output is the data with the parity. One issue remained is that we are dealing with more than bit of actual data, and we want to keep the special key and the data parity relatively small. Reed-solomon Algorithm Addressing the issue leads us to the Reed-solomon algorithm. The algorithm requires a Generator Matrix \\(G^T\\) (the special key), which has the following requirement. It is \\(n \\times k\\) matrix all \\(k \\times k\\) sub matrices in \\(G^T\\) are non-singular (the inverse matrices exist). The actual data (from HDFS) are arranged into a \\(k \\times L\\) matrix. What \\(L\\) is not important for demonstration purpose, we assume may let \\(L = 1\\) for simplicity. In the above diagram we multiply the generator matrix with the actual data, which produces a code word, consists of the data and the parity. Let's instantiate the diagram with concrete numbers. \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] Thanks to the properties of the left operand \\(G^T\\) , the first four rows of the code word is identical to the data. In the HDFS we will store the \\(G^T\\) in some safe place, i.e. name node with backup and the codeword is distributed among the data nodes Now let's say we lose the 2nd and 4th rows in the codeword and try to recover the data. \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] We remove the correspondent rows from the \\(G^T\\) , \\[ G^{T^{-1}}_{\\neg{(1,3)}} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\] Based on the matrix multiplication the following equation holds \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} ? \\\\ ? \\\\ ? \\\\ ? \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] where the question marks are the actual to be reovered. Thanks to the fact that all \\(k \\times k\\) sub matrices in \\(G^T\\) are non-singular, the inverse of \\(G^{T^{-1}}_{\\neg{(1,3)}}\\) exists \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ -1 & -1 & 0 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} ? \\\\ ? \\\\ ? \\\\ ? \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ -1 & -1 & 0 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] We cancel \\(G^{T^{-1}}_{\\neg{(1,3)}} \\times G^T_{\\neg{(1,3)}}\\) from the LHS $$ \\begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & -1 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ -1 & -1 & 0 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\ 1 \\ 1 \\ 2 \\end{bmatrix} $$ We recover the lost data. Erasure Coding storage overhead and data efficiency Erase coding is more economical compared to data replication. \\(G^T\\) is fixed for all data, hence its overhead is neglectable. We write \\(RS(k,m)\\) to denote that \\(G^T\\) is a matrix of \\((k+m) \\times k\\) and \\(k\\) is the number of rows of the actual data matrix and \\(m\\) is the number of parity rows. We have \\(m/k\\) storage overhead and \\(k / (k + m)\\) storage efficiency. One crucial observation is that we can always recover the codeword with \\((k +m) \\times 1\\) when the number of lost cells is less than or equal to \\(m\\) . Question What happen when \\(L > 1\\) ? Hint: you may think about how matrix multiplication works.","title":"L10 hdfs"},{"location":"notes/l10_hdfs/#50043-hadoop-distributed-file-system","text":"","title":"50.043 Hadoop Distributed File System"},{"location":"notes/l10_hdfs/#learning-outcomes","text":"By the end of this unit, you should be able to Explain the file system model of HDFS Explain the architecture of HDFS Explain the replication placement strategy Explain the operation of HDFS client Explain the use of erasure coding","title":"Learning Outcomes"},{"location":"notes/l10_hdfs/#hadoop","text":"Hadoop is one of the widely-used frameworks for big data. It was started in 2005 by a group of Yahoo! scientists. The initial objective was to provide a scalable backend for the nutch crawler. Over time Hadoop has evolved into a fully fledged distributed data storage and processing framework. It has a few major components MapReduce - the data processing layer Hadoop Distributed File System - the data storage layer YARN - the resource management layer In this unit, our focus is on HDFS.","title":"Hadoop"},{"location":"notes/l10_hdfs/#hadoop-distributed-file-system","text":"Hadoop Distrubuted File System was developed based on the white paper of a closed source project the google file system. The Google file system, Sanjay Ghemawat, Howard Bradley Gobioff and Shun-Tak Leung , ACM SIGOPS 2005 https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf","title":"Hadoop Distributed File System"},{"location":"notes/l10_hdfs/#google-file-system","text":"Google File System was created in the period of time where databases were expensive, but hard disks were cheap. They need to store a lot of non relational data (crawled data), i.e. data are in huge volume. These data needed to be processed by batches in huge sizes in a relative short amount of time, i.e. updating the search engine indices. The only alternative during that time was network file system (NFS), which deemed to be limited by its original use cases. a file must reside on one and only one machine. there is no reliability guarantee, data loss could be hard to recover it is not scalable, a computation over a huge batch of data cannot be distributed to multiple processors. there is no builtin replication mechanism, which leads to a high fail rate. file accessed over the network would incur huge amount of network I/O The google team then identified a list of features that required by the search engine indexing use case, and ordered them according to the priority support many clients support many disks support file in size of petabytes offer high fault tolerance allow file read/file write like normal file system It turned out no system can achieve all the above requirements. The team decided to drop the last one. By design, Google file system is a distributed file system that supports only sequential read and append operation. HDFS inherited the design of Google file system.","title":"Google File System"},{"location":"notes/l10_hdfs/#hdfs-file-system-model","text":"Like many conventional file systems, HDFS follows a hierarchical name space, i.e. data are organized by tree like structure, where each branch is a folder (sub folder) and the leaf nodes are the file. |- A | |- B | |- mydoc.txt | |- B' | |- B'' | |- A' |- A'' For instance in the file system above, there are three folders under the root directory, namely A , A' and A'' . Under A , there are three sub folders, B , B' and B'' . Under B , we find a file mydoc.txt In the above diagram, we find the illustration of the data storage of the file mydoc.txt . The first data block Block 0 stores the first level of folders. Folder A is a reference pointing at the data block Block 1 , which stores the list of subfolders within. Sub-folder B is a reference pointing at the data block Block 2 . mydoc.txt is the first entry in Block 2 , which is an array of pointers pointing at the actual file data. The data blocks Block 0 , Block 1 and Block 2 are known as the meta data. File data block (in grey color) are the real data. There are several merits of such a file system model. It is a simple abstraction. It supports very large files, (could be larger than a single physical disk) Data can be distributed to multiple disks in multiple hosts.","title":"HDFS File system model"},{"location":"notes/l10_hdfs/#choice-of-block-size","text":"Building upon the above model, the next design decision to be made is the size of the block. In normal file system, (i.e .OSes), the size of a block is around 4KB. For relational database system, the block size can range from 4 to 32KB. For HDFS the default block size is 128MB, which can be changed via system configuration. The advantage of a larger block size is to fetch more data in a single file IO operation, in the expense of larger unfilled disk space (due to data fragmentation). For HDFS, since files are written via append, data fragmentation is minimized.","title":"Choice of Block Size"},{"location":"notes/l10_hdfs/#hdfs-architecture","text":"The HDFS architecture follows a master-worker pattern. The master node, which is also known as the name node , keeps track of a set of data nodes. The meta data of the files is stored in the name node. A secondary name node is provisioned for the backup purposes. The real data are stored and replicated among the data nodes. By default HDFS recommends there should be at least 3 copies of the same data block, i.e. replication factor = 3. Replication factors 5 or 7 are also recommended. A client, i.e. an application that tries to access a file in the HDFS, has to make the requests to the name node and waits for the responses.","title":"HDFS Architecture"},{"location":"notes/l10_hdfs/#replication-and-replica-placement-policy","text":"Replication is to increase data survivability in case of hardware failure. But how and where shall we store / distribute the replicas of a data block? To maximize the survivability, it might be beneficial to take into account the actual location of the data nodes. In data centers, servers are physically mounted and connected in racks. Multiple servers can be placed in a rack which shares a power source and network switch (with redudancy). In case of power or network failure, the entire rack is affected. Hence, it would be wise not to store replicas of a data block in servers located in the same rack. On the other hand intra rack data transfer will be more cost effective. In a HDFS cluster, when the rack information is present, the replicas of a data block are distributed as follows Max 1 replica per datanode Max 2 replicas per rack Number of racks for replication should be less than the replication factor For example, we consider the following setup with 3 racks, each rack contains 3 datanodes. 3 data blocks need to be stored and replicated with RF = 3. The first copy of data block A is stored in data node 1 . The 2nd copy must not be stored in any other data node in rack 1 . In this case the system randomly picks one rack out of 2 and 3 . It places the 2nd copy in data node 5 of rack 2 in this case. The 3rd replica can be placed in any rack. In this case put it in data node 6 .","title":"Replication and Replica placement policy"},{"location":"notes/l10_hdfs/#hdfs-client-operation","text":"","title":"HDFS client operation"},{"location":"notes/l10_hdfs/#read","text":"When a client attempts to read a file in the HDFS. The client requests for the data block locations of the file from the name node. The name node returns the list of data block locations to the client. Based on the data block locations, the client requests for the data blocks from the data node. The data nodes reply the client with the data being requested. graph LR; client --1. file path--> NN[\"name node\"] NN[\"name node\"] --2. block locations --> client client --3. block location --> DN1 client --3. block location --> DN2 client --3. block location --> DN3 DN1[\"data node 1\"] --4. data --> client DN2[\"data node 2\"] --4. data --> client DN3[\"data node 3\"] --4. data --> client","title":"Read"},{"location":"notes/l10_hdfs/#write","text":"When a client attempts to write/append to a file in the HDFS. The client creates the meta data entry in the name node. The name node allocates and returns the first block locations to the client. Based on the first block locations the client initializes the data writing pipeline. The data will be sent in the daisy chain manner from the client to the data nodes. When the data is successfully transmitted, an acknowledgement will be sent to data source; When some data block (replica) write operation fails, the information is recorded by the name node and the operation will be re-executed on some data node. When the last data block is written, the client initiates a close request to the name node. The name node checks with the data nodes to ensure the minimum replica requirement is met. The data nodes reports the minimum replica status back to the name node. The name node sends the acknowledgement to the client notifying that the file write operation was successful. graph LR client --1. create--> NN[\"name node\"] NN[\"name node\"] --2. first block locations--> client client --3. organize pipeline; 4. send data--> DN1[\"data node 1\"] DN1 --5. acknowledge--> client DN1[\"data node 1\"] --3. organize pipeline; 4. send data--> DN2[\"data node 2\"] DN2 --5. acknowledge--> DN1 DN2[\"data node 2\"] --3. organize pipeline; 4. send data--> DN3[\"data node 3\"] DN3 --5. acknowledge -->DN2 client --6. close --> NN[\"name node\"] NN --7. check minimum replica --> DN1 NN --7. check minimum replica --> DN2 NN --7. check minimum replica --> DN3 DN1 --8. report --> NN DN2 --8. report --> NN DN3 --8. report --> NN In case of the minimum replica requirement is not satisfied, the name node will arrange re-distribution the replica.","title":"Write"},{"location":"notes/l10_hdfs/#alternative-to-data-replication","text":"Replicating data and redistributing them is a simple and effective way to improve fault tolerance. However, one draw-back of data replication is the data overhead. A system with replication factor n will lead to (n-1) * 100% storage overhead and (1/n) storage efficiency. For example, when n = 3 , we have 200% storage overhead and 0.3333 storage efficiency. One of the popular alternatives to data replication is erasure coding . The main idea of erasure coding is not to replicate data, instead it generates a parity for each segment of actual data, with which the actual data could be restored in case of data loss. The idea behind was inspired by the Exclusive OR (XOR) operation for bits.","title":"Alternative To Data Replication"},{"location":"notes/l10_hdfs/#properties-of-xor","text":"Recall that XOR operation \\(\\bigoplus\\) on bits IN IN XOR 0 0 0 1 0 1 0 1 1 1 1 0 having some nice properties \\[ X \\bigoplus Y = Y \\bigoplus X \\] \\[ (X \\bigoplus Y) \\bigoplus Z = X \\bigoplus (Y \\bigoplus Z) \\] \\[ X \\bigoplus Y = Z \\Rightarrow X \\bigoplus Z = Y \\] We can use the result of XOR to recover if one of the inputs is lost. We are trying to transfer this idea to increase the recoverability of the HDFS data. Let one of the input to be the actual data, the other input is a special key, the output is the data with the parity. One issue remained is that we are dealing with more than bit of actual data, and we want to keep the special key and the data parity relatively small.","title":"Properties of XOR"},{"location":"notes/l10_hdfs/#reed-solomon-algorithm","text":"Addressing the issue leads us to the Reed-solomon algorithm. The algorithm requires a Generator Matrix \\(G^T\\) (the special key), which has the following requirement. It is \\(n \\times k\\) matrix all \\(k \\times k\\) sub matrices in \\(G^T\\) are non-singular (the inverse matrices exist). The actual data (from HDFS) are arranged into a \\(k \\times L\\) matrix. What \\(L\\) is not important for demonstration purpose, we assume may let \\(L = 1\\) for simplicity. In the above diagram we multiply the generator matrix with the actual data, which produces a code word, consists of the data and the parity. Let's instantiate the diagram with concrete numbers. \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] Thanks to the properties of the left operand \\(G^T\\) , the first four rows of the code word is identical to the data. In the HDFS we will store the \\(G^T\\) in some safe place, i.e. name node with backup and the codeword is distributed among the data nodes Now let's say we lose the 2nd and 4th rows in the codeword and try to recover the data. \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] We remove the correspondent rows from the \\(G^T\\) , \\[ G^{T^{-1}}_{\\neg{(1,3)}} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\] Based on the matrix multiplication the following equation holds \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} ? \\\\ ? \\\\ ? \\\\ ? \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] where the question marks are the actual to be reovered. Thanks to the fact that all \\(k \\times k\\) sub matrices in \\(G^T\\) are non-singular, the inverse of \\(G^{T^{-1}}_{\\neg{(1,3)}}\\) exists \\[ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ -1 & -1 & 0 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} ? \\\\ ? \\\\ ? \\\\ ? \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ -1 & -1 & 0 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix} \\] We cancel \\(G^{T^{-1}}_{\\neg{(1,3)}} \\times G^T_{\\neg{(1,3)}}\\) from the LHS $$ \\begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & -1 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ -1 & -1 & 0 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\ 1 \\ 1 \\ 2 \\end{bmatrix} $$ We recover the lost data.","title":"Reed-solomon Algorithm"},{"location":"notes/l10_hdfs/#erasure-coding-storage-overhead-and-data-efficiency","text":"Erase coding is more economical compared to data replication. \\(G^T\\) is fixed for all data, hence its overhead is neglectable. We write \\(RS(k,m)\\) to denote that \\(G^T\\) is a matrix of \\((k+m) \\times k\\) and \\(k\\) is the number of rows of the actual data matrix and \\(m\\) is the number of parity rows. We have \\(m/k\\) storage overhead and \\(k / (k + m)\\) storage efficiency. One crucial observation is that we can always recover the codeword with \\((k +m) \\times 1\\) when the number of lost cells is less than or equal to \\(m\\) .","title":"Erasure Coding storage overhead and data efficiency"},{"location":"notes/l10_hdfs/#question","text":"What happen when \\(L > 1\\) ? Hint: you may think about how matrix multiplication works.","title":"Question"},{"location":"notes/l10_mapreduce/","text":"50.043 Map Reduce Learning Outcomes By the end of this lesson, you are able to Differentiate Parallelism and Concurrency Differentiate Data Parallelism and Task Parallelism Explain map and reduce Explain Hadoop MapReduce Recall HDFS addresses the big data issues by distributing the data into multiple data nodes. storing meta data on the name node. allowing only sequential read and append. Based on the design decision made during the design of Google File system, data query via join and B-tree based indexing was not the top-most wanted use case. On the other hand, most of the data processing is to scan through the needed data files and perform transformation or aggregation (page rank algorithm). To shorten the processing time, we could leverage on the processors found in the data nodes. This leads us to parallel computing. Parallel Computing A parallel program is one that uses a multiplicity of computational hardware (e.g., several processor cores or several server nodes) to perform a computation more quickly . The aim is to arrive at the answer earlier, by delegating different parts of the computation to different processors that execute at the same time. We are often confused parallelism with concurrency. By contrast, concurrency is a program-structuring technique in which there are multiple threads of control. Conceptually, the threads of control execute \"at the same time\"; that is, the user sees their effects interleaved . Whether they actually execute at the same time or not is an implementation detail; a concurrent program can execute on a single processor through interleaved execution or on multiple physical processors. We summarize the difference between parallelism and and concurrency in the following table. Parallelism Concurrency Area of Focus Efficiency Structural and Modularity Number of Goals One One or more Program Semantic Deterministic Non-deterministic Single processor Exec Sequential Interleaving We focus on the key differences. Though concurrency also achieve certain level of speed up, its focus is in handling multiple tasks with a fixed set of resources through scheduling. Parallelism focuses on getting one thing done and fast. The result of parallel computing must be deterministic whilst concurrency does not necessarily entail determinism. The following are some examples of parallelism A sodoku solver uses multiple CPU cores A parallelized database query that retrieves and aggregates records from a cluster of replica of database. A K-means analyses running over a Hadoop cluster and concurrency A web application that handles multiple clients HTTP requests and interacting with the databases. A User Interface of the a mobile phone handles user's touch screen input and exchanging data via the 4G network Means of Parallelism Parallelism can be achieved via special hardware or/and via software Hardware Parallelism Single Processor Parallelism can be actualized via a single processor when the given instructions use less than provided bits, e.g. we can parallelize the execution of some 32 bit operations given that the processor operates on 64 bit addresses. Alterantively, the parallelism can be achieved by instruction pipelining, assume different sub components of the processor handles different types of process operators. For instance, 1: add r1 r2 r3 // r1 = r2 + r3 2: mul r2 2 // r2 *= 2 3: add r2 r4 r5 // r2 = r4 + r5 assuming add and mul instructions are executed by different components of the same processor, after the instruction 1 is executed, instructions 2 and 3 can be executed in parallel. GPU GPU can be leveraged to perform matrix operations in parallel by making use of the multiple cores builtin. The acceleration of computation often requires firwmware level and driver level support, e.g. Nvidia CUDA and AMD ROCM. Multiple processors with shared memory Modern personal computers and mobile devices are equipped with multi-core processors. These processors share the same pool of physical memory. Parallelism can be achieved via data parallelism, task parallelism, multi-threading etc. Multiple processors with distributed memory As the number of processors increases, the processing power per dollar for multiple processors with shared memory is diminishing. Large scale systems are often built over multiple processors (multiple servers) with distributed memory. Each processor (set of processors) has its own dedicated memory. They are connected and communicating through high speed networks. Software Parallelism In the earlier year of study, we came acrossing the concept of multithreading. In this module, we focus on the other two commonly used (higher level) software parallelism concepts: task parallelism and data paralleism. Task Parallelism In Task parallelism, if some sub-computations (sub tasks) are mutually independent, (which can be identified via human checking or software analysis), they can be executed in parallel. For instance, def cook_beef_bolognese(beef,spaghetti): sauce = cook_sauce(beef) # task A pasta = cook_pasta(spaghetti) # task B return mix(sauce,pasta) If we identify that sub task of cooking sauce is independent of the sub task of cooking pasta, we can execute the two sub tasks in parallel when the hardware resource is available. Note that in Task parallelism, the sub tasks are not necessary having the same set of instructions. Data Parallelism In data parallelism, we compute the result by running a common routine repeatedly over a set of input data. If there is no dependency among the iteration, we could parallize these computations too. For example, def fib(n): ... def main(): inputs = [ 10, 100, 200, ...] results = [] for i in inputs: results.append(fib(i)) return results in the above code snippet, we repeatedly compute the fibonacci numbers given the index positions as the input. Note that these computations are independent. We could rewrite the above using the python `map`` function. def fib(n): ... def main(): inputs = [ 10, 100, 200, ...] # task C results = map(fib, inputs) return results def map(f,l): if len(l) == 0: return [] else: hd = l[0] tl = l[1:] fst = f(hd) rest = map(f,tl) return [fst] + rest The above variant is the computing the same results as the original version, except that we are using the map function instead of the for loop. The map function applies a higher order function f to a list of items l . In case l is an empty list, it returns an empty list. otherwise, it applies f to the first item in l and recursively calls itself to handle the rest of the elements. (Note: since Python 3, the map is implemented in iterator style. However, we still use the above version for the ease of reasoning.) One motivation to use map instead of for-loop is to explicitly show that each call to f(hd) for each item in l is using the same instruction each call to f(hd) for each item in l is indpendent (c.f. task parallelism) Exploiting these results, the compiler could parallelize these calls, e.g. by scheduling fib(10) in CPU core 1, fib(100) in CPU core 2, and etc. Unfortunately, Python does not have data parallelism builtin for map . In some other languages such as Scala, the map function has builtin support of data parallelism. We will see some of the demo during the lecture when time permits. Data Parallelism and determinism Defintion: Determinisism We say a programm \\(P\\) is deterministic iff for all input \\(i_1\\) and \\(i_2\\) such that \\(i_1 \\equiv i_2\\) then \\(P(i_1) \\equiv P(i_2)\\) . In the presence of parallelism and concurrency, it is tricky to ensure a program is determinsitic. Fortunately, for map data parallelism, all we need is a side condition to ensure deterministic semantics. Pure function Recall from some earlier module, we learned that a function f is pure if it does not modify nor is dependent on the external state when it is executed. map(f,l) is guaranteed to be deterministic (regardless whether it is executed sequentially or in parallel.) Data Parallelism with Reduce Orthogonal to map , function reduce(f, l, initial) aggregates all the items in l with a binary operation f , using initial as the initial aggregated value. def reduce(f,l,acc): if len(l) == 0: return acc else: hd = l[0] tl = l[1:] return reduce(f, tl, f(hd,acc)) def reduce(f,l): return reduce(f,l[1:], l[0]) For example, def main(): inputs = [10, 100, 200, 400] result = reduce(lambda x,y:x+y,inputs,0) return result computes the sum of all numbers found in inputs , i.e. it is effectively computing (0 + 10 + 100 + 200 + 400) If we are given 2 CPU cores, we could evaluate (0 + 10 + 100) in Core 1, (200 + 400) in Core 2, then (110 + 600) in Core 1. Reduce and determinism Given that a binary function f is pure , commutative and associative it is guaranteed that reduce(f,l,a) can be parallelized. The results will be the same as it is executed sequentially. Note: A binary function f is commutative iff f(x,y) = f(y,x) . A binary function f is associative iff f(x,f(y,z)) = f(f(x,y),z) . MapReduce Framework Though this looks prosing on paper, In practice, it won't scale well with each map or reduce task being teeny tiny. It is better to partition data into chunks so that each map or reduce task is reasonably large enough. This leads to the MapReduce Framework found in Google FS, Hadoop and many other big data platforms. The Toy MapReduce Framework For the ease of understanding, we consider a scaled down and simplified implementation of the MapReduce Framework in Python. Though there is no parallelism builtin, we know that when the same library can be easily ported to other languguages or environments with parallel map and reduce support. Besides map and reduce we need a few more combinators (functions) def flatMap(f,l): ll = map(f,l) return reduce(lambda x,y:x+y, ll, []) flatMap is similar to map, except that each inner list is flattened. e.g. flatMap(lambda x: [x+1], [1,2,3]) yields [2,3,4] . def lift_if(p,x): if p(x): return [x] else: return [] def filter(p,l): return flatMap(lambda x:lift_if(p,x), l) filter returns a new list whose elements are from l and satisfying the test p . e.g. filter(lambda x:x%2==0, [1,2,3,4,5,6]) yields [2,4,6] . def merge(kvls1, kvls2): if len(kvls1) == 0: return kvls2 elif len(kvls2) == 0: return kvls1 else: ((k1,vl1), tl1) = (kvls1[0], kvls1[1:]) ((k2,vl2), tl2) = (kvls2[0], kvls2[1:]) if k1 == k2: return [(k1,vl1+vl2)]+merge(tl1,tl2) elif k1 < k2: return [(k1,vl1)]+merge(tl1,kvls2) else: return [(k2,vl2)]+merge(kvls1, tl2) def shuffle(kvs): kvls = map(lambda kv: [(kv[0], [kv[1]])], kvs) return reduce(merge, kvls, []) We assume that there exists a total order among keys. Given a list of key-value pairs, shuffle shuffles and merge values sharing the same key. e.g. shuffle([(\"k1\",1),(\"k2\",1), (\"k1\",2), (\"k2\",3)]) yields [('k1', [1, 2]), ('k2', [1, 3])] def reduceByKey(f, kvs, acc): s = shuffle(kvs) return map(lambda p: (p[0], reduce(f,p[1],acc)), s) def reduceByKey(f, kvs): s = shuffle(kvs) return map(lambda p: (p[0], reduce(f,p[1])), s) reduceByKey shuffles the list of key-value pairs, grouping them by keys, then applies the binary aggregation function f to values in each group. e.g. reduceByKey(lambda x,y:x+y, [(\"k1\",1),(\"k2\",1), (\"k1\",2), (\"k2\",3)],0) yields [('k1', 3), ('k2', 4)] Note that all these combinators are implemented using map and reduce . If map and reduce are parallelized, so are these combinators. Apart from reduceByKey , we would also like to include a variant def reduceByKey2(agg, kvs): return map(agg, shuffle(kvs)) Both variants call shuffle to partition the input. The difference is that given a partition (key,values) obtained from the shuffled results, the function agg in reducedByKey2 is applied to aggregate values , and returns the key of the partition and the aggregated result. Note that agg is given by the user/programmer, which might not be implemented using reduce , in contrast, the f in reduceByKey is applied to the reduce(f,values) . Example : Word Count In this example, we would to write a program which opens a text file, reads all the words in the file and counts the number of occurences of words. Using for loop infile = open(sys.argv[1], 'r') dict = {} for line in infile: words = line.strip().split() for word in words: if (dict.has_key(word)): dict[word] +=1 else: dict[word] = 1 for word,count in dict.items(): print(\"%s,%d\\n\" % (word,count)) Using Toy MapReduce infile = open(sys.argv[1], 'r') lines = [] for line in infile: lines.append(line.strip()) def f(text): wordand1s = [] for word in text.split(): wordand1s.append((word,1)) return wordand1s def g(p): word,icounts = p return (word, sum(icounts)) w1s = flatMap(f,lines) res = reduceByKey2(g, w1s) for word,count in res: print(\"%s,%d\" % (word,count)) we abstract away the use of for loop and the dictionary by using flatMap and reduceByKey2 . Using MapReduce in Hadoop We consider using the Python API of Hadoop (a.k.a. pydoop). Hadoop MapReduce provides the standard parallelized/distributed implementtation of the flatMap and reduceByKey2 , we don't need to worry implement them. def mapper(key, text, writer): for word in text.split(): writer.emit(word, \"1\") def reducer(word, icounts, writer): writer.emit(word, sum(map(int, icounts))) the function mapper is taking the role of f in the toy mapreduce version, and function reducer is taking the role of g . We can think of writer.emit is similar to the regular return and yield in Python's iterator depending on the context. Note that mapper also takes a key as input. Hadoop generalize to all data that potentially has a key for each entry. In case like the input ot the mapper is a plain text file, the key is the byte offset w.r.t to the start of the text file. Since we are using Pydoop, the integration with with the Hadoop MapReduce is just to run it as a script. $ pydoop script wordcount.py /input /output Note that we do not need to call flatMap or reduceByKey2 explicitly. In the following chart illustrate of the steps taken place during wordcount example. Hadoop MapReduce Architecture The following diagram show a basic structure of all the server and clients component of the Hadoop MapReduce. In Hadoop v1, a JobTracker is spawned for coordination purposes; TaskTrackers are spawed for executing the actual jobs. We can think of the JobTrackers are run in the name node, and the Tasktrackers are processes in the data nodes. We leave the advanced architecture of Hadoop Version 2+ in the upcoming lesson, i.e. YARN. Hadoop MapReduce Job Management Hadoop MapReduce follows the locallity principal, i.e. computation must be moved to the data, i.e. compute at the data nodes. This is possible thanks to the determinism property discussed earlier. These common tasks, mappers and reducers are compiled and packaged as (JAR) and uploaded to the data node. The input data are (pre-) partitioned into splits, (by data nodes). Data are fed to the mappers by split. The output of the mappers will be re-shuffled and re-partitioned and sent to the reducers. In some cases combiners are used. Combiners are like local mini-reducers, they reduce the immediate output coming straight from the mapper task so that the network traffic can be further reduced.","title":"50.043 Map Reduce"},{"location":"notes/l10_mapreduce/#50043-map-reduce","text":"","title":"50.043 Map Reduce"},{"location":"notes/l10_mapreduce/#learning-outcomes","text":"By the end of this lesson, you are able to Differentiate Parallelism and Concurrency Differentiate Data Parallelism and Task Parallelism Explain map and reduce Explain Hadoop MapReduce","title":"Learning Outcomes"},{"location":"notes/l10_mapreduce/#recall","text":"HDFS addresses the big data issues by distributing the data into multiple data nodes. storing meta data on the name node. allowing only sequential read and append. Based on the design decision made during the design of Google File system, data query via join and B-tree based indexing was not the top-most wanted use case. On the other hand, most of the data processing is to scan through the needed data files and perform transformation or aggregation (page rank algorithm). To shorten the processing time, we could leverage on the processors found in the data nodes. This leads us to parallel computing.","title":"Recall"},{"location":"notes/l10_mapreduce/#parallel-computing","text":"A parallel program is one that uses a multiplicity of computational hardware (e.g., several processor cores or several server nodes) to perform a computation more quickly . The aim is to arrive at the answer earlier, by delegating different parts of the computation to different processors that execute at the same time. We are often confused parallelism with concurrency. By contrast, concurrency is a program-structuring technique in which there are multiple threads of control. Conceptually, the threads of control execute \"at the same time\"; that is, the user sees their effects interleaved . Whether they actually execute at the same time or not is an implementation detail; a concurrent program can execute on a single processor through interleaved execution or on multiple physical processors. We summarize the difference between parallelism and and concurrency in the following table. Parallelism Concurrency Area of Focus Efficiency Structural and Modularity Number of Goals One One or more Program Semantic Deterministic Non-deterministic Single processor Exec Sequential Interleaving We focus on the key differences. Though concurrency also achieve certain level of speed up, its focus is in handling multiple tasks with a fixed set of resources through scheduling. Parallelism focuses on getting one thing done and fast. The result of parallel computing must be deterministic whilst concurrency does not necessarily entail determinism. The following are some examples of parallelism A sodoku solver uses multiple CPU cores A parallelized database query that retrieves and aggregates records from a cluster of replica of database. A K-means analyses running over a Hadoop cluster and concurrency A web application that handles multiple clients HTTP requests and interacting with the databases. A User Interface of the a mobile phone handles user's touch screen input and exchanging data via the 4G network","title":"Parallel Computing"},{"location":"notes/l10_mapreduce/#means-of-parallelism","text":"Parallelism can be achieved via special hardware or/and via software","title":"Means of Parallelism"},{"location":"notes/l10_mapreduce/#hardware-parallelism","text":"","title":"Hardware Parallelism"},{"location":"notes/l10_mapreduce/#single-processor","text":"Parallelism can be actualized via a single processor when the given instructions use less than provided bits, e.g. we can parallelize the execution of some 32 bit operations given that the processor operates on 64 bit addresses. Alterantively, the parallelism can be achieved by instruction pipelining, assume different sub components of the processor handles different types of process operators. For instance, 1: add r1 r2 r3 // r1 = r2 + r3 2: mul r2 2 // r2 *= 2 3: add r2 r4 r5 // r2 = r4 + r5 assuming add and mul instructions are executed by different components of the same processor, after the instruction 1 is executed, instructions 2 and 3 can be executed in parallel.","title":"Single Processor"},{"location":"notes/l10_mapreduce/#gpu","text":"GPU can be leveraged to perform matrix operations in parallel by making use of the multiple cores builtin. The acceleration of computation often requires firwmware level and driver level support, e.g. Nvidia CUDA and AMD ROCM.","title":"GPU"},{"location":"notes/l10_mapreduce/#multiple-processors-with-shared-memory","text":"Modern personal computers and mobile devices are equipped with multi-core processors. These processors share the same pool of physical memory. Parallelism can be achieved via data parallelism, task parallelism, multi-threading etc.","title":"Multiple processors with shared memory"},{"location":"notes/l10_mapreduce/#multiple-processors-with-distributed-memory","text":"As the number of processors increases, the processing power per dollar for multiple processors with shared memory is diminishing. Large scale systems are often built over multiple processors (multiple servers) with distributed memory. Each processor (set of processors) has its own dedicated memory. They are connected and communicating through high speed networks.","title":"Multiple processors with distributed memory"},{"location":"notes/l10_mapreduce/#software-parallelism","text":"In the earlier year of study, we came acrossing the concept of multithreading. In this module, we focus on the other two commonly used (higher level) software parallelism concepts: task parallelism and data paralleism.","title":"Software Parallelism"},{"location":"notes/l10_mapreduce/#task-parallelism","text":"In Task parallelism, if some sub-computations (sub tasks) are mutually independent, (which can be identified via human checking or software analysis), they can be executed in parallel. For instance, def cook_beef_bolognese(beef,spaghetti): sauce = cook_sauce(beef) # task A pasta = cook_pasta(spaghetti) # task B return mix(sauce,pasta) If we identify that sub task of cooking sauce is independent of the sub task of cooking pasta, we can execute the two sub tasks in parallel when the hardware resource is available. Note that in Task parallelism, the sub tasks are not necessary having the same set of instructions.","title":"Task Parallelism"},{"location":"notes/l10_mapreduce/#data-parallelism","text":"In data parallelism, we compute the result by running a common routine repeatedly over a set of input data. If there is no dependency among the iteration, we could parallize these computations too. For example, def fib(n): ... def main(): inputs = [ 10, 100, 200, ...] results = [] for i in inputs: results.append(fib(i)) return results in the above code snippet, we repeatedly compute the fibonacci numbers given the index positions as the input. Note that these computations are independent. We could rewrite the above using the python `map`` function. def fib(n): ... def main(): inputs = [ 10, 100, 200, ...] # task C results = map(fib, inputs) return results def map(f,l): if len(l) == 0: return [] else: hd = l[0] tl = l[1:] fst = f(hd) rest = map(f,tl) return [fst] + rest The above variant is the computing the same results as the original version, except that we are using the map function instead of the for loop. The map function applies a higher order function f to a list of items l . In case l is an empty list, it returns an empty list. otherwise, it applies f to the first item in l and recursively calls itself to handle the rest of the elements. (Note: since Python 3, the map is implemented in iterator style. However, we still use the above version for the ease of reasoning.) One motivation to use map instead of for-loop is to explicitly show that each call to f(hd) for each item in l is using the same instruction each call to f(hd) for each item in l is indpendent (c.f. task parallelism) Exploiting these results, the compiler could parallelize these calls, e.g. by scheduling fib(10) in CPU core 1, fib(100) in CPU core 2, and etc. Unfortunately, Python does not have data parallelism builtin for map . In some other languages such as Scala, the map function has builtin support of data parallelism. We will see some of the demo during the lecture when time permits.","title":"Data Parallelism"},{"location":"notes/l10_mapreduce/#data-parallelism-and-determinism","text":"","title":"Data Parallelism and determinism"},{"location":"notes/l10_mapreduce/#defintion-determinisism","text":"We say a programm \\(P\\) is deterministic iff for all input \\(i_1\\) and \\(i_2\\) such that \\(i_1 \\equiv i_2\\) then \\(P(i_1) \\equiv P(i_2)\\) . In the presence of parallelism and concurrency, it is tricky to ensure a program is determinsitic. Fortunately, for map data parallelism, all we need is a side condition to ensure deterministic semantics.","title":"Defintion: Determinisism"},{"location":"notes/l10_mapreduce/#pure-function","text":"Recall from some earlier module, we learned that a function f is pure if it does not modify nor is dependent on the external state when it is executed. map(f,l) is guaranteed to be deterministic (regardless whether it is executed sequentially or in parallel.)","title":"Pure function"},{"location":"notes/l10_mapreduce/#data-parallelism-with-reduce","text":"Orthogonal to map , function reduce(f, l, initial) aggregates all the items in l with a binary operation f , using initial as the initial aggregated value. def reduce(f,l,acc): if len(l) == 0: return acc else: hd = l[0] tl = l[1:] return reduce(f, tl, f(hd,acc)) def reduce(f,l): return reduce(f,l[1:], l[0]) For example, def main(): inputs = [10, 100, 200, 400] result = reduce(lambda x,y:x+y,inputs,0) return result computes the sum of all numbers found in inputs , i.e. it is effectively computing (0 + 10 + 100 + 200 + 400) If we are given 2 CPU cores, we could evaluate (0 + 10 + 100) in Core 1, (200 + 400) in Core 2, then (110 + 600) in Core 1.","title":"Data Parallelism with Reduce"},{"location":"notes/l10_mapreduce/#reduce-and-determinism","text":"Given that a binary function f is pure , commutative and associative it is guaranteed that reduce(f,l,a) can be parallelized. The results will be the same as it is executed sequentially. Note: A binary function f is commutative iff f(x,y) = f(y,x) . A binary function f is associative iff f(x,f(y,z)) = f(f(x,y),z) .","title":"Reduce and determinism"},{"location":"notes/l10_mapreduce/#mapreduce-framework","text":"Though this looks prosing on paper, In practice, it won't scale well with each map or reduce task being teeny tiny. It is better to partition data into chunks so that each map or reduce task is reasonably large enough. This leads to the MapReduce Framework found in Google FS, Hadoop and many other big data platforms.","title":"MapReduce Framework"},{"location":"notes/l10_mapreduce/#the-toy-mapreduce-framework","text":"For the ease of understanding, we consider a scaled down and simplified implementation of the MapReduce Framework in Python. Though there is no parallelism builtin, we know that when the same library can be easily ported to other languguages or environments with parallel map and reduce support. Besides map and reduce we need a few more combinators (functions) def flatMap(f,l): ll = map(f,l) return reduce(lambda x,y:x+y, ll, []) flatMap is similar to map, except that each inner list is flattened. e.g. flatMap(lambda x: [x+1], [1,2,3]) yields [2,3,4] . def lift_if(p,x): if p(x): return [x] else: return [] def filter(p,l): return flatMap(lambda x:lift_if(p,x), l) filter returns a new list whose elements are from l and satisfying the test p . e.g. filter(lambda x:x%2==0, [1,2,3,4,5,6]) yields [2,4,6] . def merge(kvls1, kvls2): if len(kvls1) == 0: return kvls2 elif len(kvls2) == 0: return kvls1 else: ((k1,vl1), tl1) = (kvls1[0], kvls1[1:]) ((k2,vl2), tl2) = (kvls2[0], kvls2[1:]) if k1 == k2: return [(k1,vl1+vl2)]+merge(tl1,tl2) elif k1 < k2: return [(k1,vl1)]+merge(tl1,kvls2) else: return [(k2,vl2)]+merge(kvls1, tl2) def shuffle(kvs): kvls = map(lambda kv: [(kv[0], [kv[1]])], kvs) return reduce(merge, kvls, []) We assume that there exists a total order among keys. Given a list of key-value pairs, shuffle shuffles and merge values sharing the same key. e.g. shuffle([(\"k1\",1),(\"k2\",1), (\"k1\",2), (\"k2\",3)]) yields [('k1', [1, 2]), ('k2', [1, 3])] def reduceByKey(f, kvs, acc): s = shuffle(kvs) return map(lambda p: (p[0], reduce(f,p[1],acc)), s) def reduceByKey(f, kvs): s = shuffle(kvs) return map(lambda p: (p[0], reduce(f,p[1])), s) reduceByKey shuffles the list of key-value pairs, grouping them by keys, then applies the binary aggregation function f to values in each group. e.g. reduceByKey(lambda x,y:x+y, [(\"k1\",1),(\"k2\",1), (\"k1\",2), (\"k2\",3)],0) yields [('k1', 3), ('k2', 4)] Note that all these combinators are implemented using map and reduce . If map and reduce are parallelized, so are these combinators. Apart from reduceByKey , we would also like to include a variant def reduceByKey2(agg, kvs): return map(agg, shuffle(kvs)) Both variants call shuffle to partition the input. The difference is that given a partition (key,values) obtained from the shuffled results, the function agg in reducedByKey2 is applied to aggregate values , and returns the key of the partition and the aggregated result. Note that agg is given by the user/programmer, which might not be implemented using reduce , in contrast, the f in reduceByKey is applied to the reduce(f,values) .","title":"The Toy MapReduce Framework"},{"location":"notes/l10_mapreduce/#example-word-count","text":"In this example, we would to write a program which opens a text file, reads all the words in the file and counts the number of occurences of words.","title":"Example : Word Count"},{"location":"notes/l10_mapreduce/#using-for-loop","text":"infile = open(sys.argv[1], 'r') dict = {} for line in infile: words = line.strip().split() for word in words: if (dict.has_key(word)): dict[word] +=1 else: dict[word] = 1 for word,count in dict.items(): print(\"%s,%d\\n\" % (word,count))","title":"Using for loop"},{"location":"notes/l10_mapreduce/#using-toy-mapreduce","text":"infile = open(sys.argv[1], 'r') lines = [] for line in infile: lines.append(line.strip()) def f(text): wordand1s = [] for word in text.split(): wordand1s.append((word,1)) return wordand1s def g(p): word,icounts = p return (word, sum(icounts)) w1s = flatMap(f,lines) res = reduceByKey2(g, w1s) for word,count in res: print(\"%s,%d\" % (word,count)) we abstract away the use of for loop and the dictionary by using flatMap and reduceByKey2 .","title":"Using Toy MapReduce"},{"location":"notes/l10_mapreduce/#using-mapreduce-in-hadoop","text":"We consider using the Python API of Hadoop (a.k.a. pydoop). Hadoop MapReduce provides the standard parallelized/distributed implementtation of the flatMap and reduceByKey2 , we don't need to worry implement them. def mapper(key, text, writer): for word in text.split(): writer.emit(word, \"1\") def reducer(word, icounts, writer): writer.emit(word, sum(map(int, icounts))) the function mapper is taking the role of f in the toy mapreduce version, and function reducer is taking the role of g . We can think of writer.emit is similar to the regular return and yield in Python's iterator depending on the context. Note that mapper also takes a key as input. Hadoop generalize to all data that potentially has a key for each entry. In case like the input ot the mapper is a plain text file, the key is the byte offset w.r.t to the start of the text file. Since we are using Pydoop, the integration with with the Hadoop MapReduce is just to run it as a script. $ pydoop script wordcount.py /input /output Note that we do not need to call flatMap or reduceByKey2 explicitly. In the following chart illustrate of the steps taken place during wordcount example.","title":"Using MapReduce in Hadoop"},{"location":"notes/l10_mapreduce/#hadoop-mapreduce-architecture","text":"The following diagram show a basic structure of all the server and clients component of the Hadoop MapReduce. In Hadoop v1, a JobTracker is spawned for coordination purposes; TaskTrackers are spawed for executing the actual jobs. We can think of the JobTrackers are run in the name node, and the Tasktrackers are processes in the data nodes. We leave the advanced architecture of Hadoop Version 2+ in the upcoming lesson, i.e. YARN.","title":"Hadoop MapReduce Architecture"},{"location":"notes/l10_mapreduce/#hadoop-mapreduce-job-management","text":"Hadoop MapReduce follows the locallity principal, i.e. computation must be moved to the data, i.e. compute at the data nodes. This is possible thanks to the determinism property discussed earlier. These common tasks, mappers and reducers are compiled and packaged as (JAR) and uploaded to the data node. The input data are (pre-) partitioned into splits, (by data nodes). Data are fed to the mappers by split. The output of the mappers will be re-shuffled and re-partitioned and sent to the reducers. In some cases combiners are used. Combiners are like local mini-reducers, they reduce the immediate output coming straight from the mapper task so that the network traffic can be further reduced.","title":"Hadoop MapReduce Job Management"},{"location":"notes/l11_spark/","text":"50.043 Spark Learning Outcomes By the end of this lesson, you are able to Differentiate Hadoop MapReduce and Spark Apply Spark based on application requirements Develop data engineering process using Spark RDD Reason about Spark Execution Model Develop machine learning application using Spark MLLib Explain Spark Architecture Develop Data processing application using Spark Dataframe Develop Machine Learning application using Spark ML package Explain Spark Streaming Spark VS Hadoop MapReduce Hadoop MapReduce was created to batch process data once or twice, e.g. web search index, processing crawled data, etc. When machine learning being applied to big data, the terabyte or zeta byte of data probably need to be processed iteratively. For instance, if we need to run gradient descent thousands of times. On top of that data visualization for big data also impose further challenge. It requires data to be reprocessed based on the user inputs such as sorting and filtering constraints. Hadoop MapReduce is no longer suitable for these applications because of the following 1. each of mapper (and educer) task needs to transfer intermediate data to the disk back and forth. 2. its rigit computation model (i.e. one step of map followed by one step of reduce) makes most of the application look unnecessarily complex. 3. it does not utilize much of RAM. Many of the MapReduce applications are disk and network I/O bound rather than RAM and CPU bound. 4. it is hard to redistribute the workload Apache Spark is a project which started off as an academic reseach idea and became an enterprise level success. It addressed the above-mentioned limitations of the Hadoop MapReduce by introducing the following features Resilient distributed datasets, which act as the primary data structure for distributed map reduce operation It unions all the available RAM from the cluster (all data nodes) to form a large pool of virtual RAM. RDD and its derivative such as dataframe and dataset, are in memory parallel distributed data structure to be used in the virtual RAM pool. It has a MapReduce like programming interface (closer to our toy MapReduce library compared to the Hadoop MapReduce) It offers fault tolerance, RDD, dataframe and datasets can always be re-computed in case of node failure Machine Learning Libraries, Graph computation libraries. Spark supports many mainstream programming languagues such as Scala, Python, R, Java and SQL. In this module we consider the Python interface. Spark RDD API The primary data structure of the Spark RDD API is the RDD. data = [1, 2, 3, 4, 5] distData = sc.parallelize(data) In the above code snippet, we initialize a list of integers and turn it into an RDD distData . Alternatively, we can load the data from a file. distData = sc.textFile(\"hdfs://127.0.0.1:9000/data.txt\") Given the RDD, we can now perform data manipulation. There are two kinds of RDD APIs, namely Transformations and Actions . Transformation APIs are lazy and action APIs are strict . Lazy vs Strict Evaluation In lazy evaluation, the argument of a function is not fully computed until it is needed. We can mimic this in Python using generator and iterator. r = range(2,-1,-1) # a range 2,1,0 l = ( 4 // x for x in r) # a generator 2, 4, div_by_0 def takeOne(l): res = next(iter(l), None) if res is None: return error(\" ... \") else: return res takeOne(l) # yield 2 When takeOne(l) is called, only the first element of l is computed, the rest are discarded. In strict evaluation, the argument of a function is alwaysfully computed. def takeOneStrict(l): res = next(iter(list(l)), None) if res is None: return error(\" ... \") else: return res takeOneStrict(l) # yield an div by zero error. When takeOneStrict(l) is called, even though only the first element of l is required, the rest are computed eagerly, thanks to the list() function call materializing the generator. RDD Transformations All Transformations takes the current RDD as (part of) the input and return some a new RDD as output. Let l , l1 and l2 be RDDs, f be a function RDD APIs Description Toy MapReduce equivalent l.map(f) map(f,l) l.flatMap(f) flatMap(f,l) l.filter(f) filter(f,l) l.reduceByKey(f) reduceByKey(f,l) l.mapPartition(f) similar to map , f takes an iterator and produces an iterator NA l.distinct() all distinct elems N.A. l.sample(b,ratio,seed) sample dataset. b : a boolean value to indicate w/wo replacement. ratio : a value range [0,1] N.A. l.aggregateByKey(zv)(sop,cop) zv : accumulated value. sop : intra-partition aggregation function. cop : inter-partition aggregation function similar to reduceByKey(f,l,acc) , except that we don't have 2 version of f l1.union(l2) union l1 l2 l1 + l2 l1.intersection(l2) the intersection of elements from l1 and l2 N.A. l1.groupByKey() group elemnts by keys shuffle(l1) l1.sortByKey() sort by keys N.A. l1.join(l2) join l1 l2 by keys we've done it in lab l1.cogroup(l2) similar to join , it returns RDDs of (key, ([v1,..], [v2,..])) , [v1,...] are values from l1 , [v2,...] are values from l2 N.A. Note that the RDDS APIs follow the builtin Scala library's convention, map , filter and etc are methods of the List class. RDD Actions All Actions takes the current RDD as (part of) the input and return some value that is not an RDD. It forces computation to happen. RDDs Desc Toy MR l.reduce(f) reduce(f,l) l.collect() converts rdd to a local array l.count() len(l) l.first() l[0] l.take(n) returns an array l[:n] l.saveAsTextFile(path) save rdd to text file N.A. l.countByKey() return hash map of key and count N.A. l.foreach(f) run a function for each element in the dataset with side-effects for x in l: ... Special Transformation Some transformation/opereations such as reduceByKey , join , groupByKey and sortByKey will trigger a shuffle event, in which Spark redistribute the data across partititon, which means intermediate results from lazy operations will be materialized. Wordcount example in Spark The follow code snippet, we find the wordcount application implemented using PySpark. import sys from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName(\"Wordcount Application\") sc = SparkContext(conf=conf) text_file = sc.textFile(\"hdfs://localhost:9000/input/\") counts = text_file.flatMap(lambda line: line.split(\" \")) \\ .map(lambda word: (word, 1)) \\ .reduceByKey(lambda a, b: a + b) counts.saveAsTextFile(\"hdfs://localhost:9000/output/\") sc.stop() We can compare it with the version in toy MapReduce, and find many similarities infile = open(sys.argv[1], 'r') lines = [] for line in infile: lines.append(line.strip()) ws = flatMap(lambda line:line.split(\" \"),lines) w1s = map(lambda w:(w,1), ws) res = reduceByKey(lambda x,y:x+y,w1s,0) with open(sys.argv[2], 'w') as out: for w,c in res: out.write(w + \"\\t\" + str(c) + \"\\n\") We will see more examples of using PySpark in the lecture and the labs. Spark Architecture Next we consider the how Spark manages the execution model. In the following, we find the Spark Architecture (in simple standalone mode) {width=100%} Like Hadoop, Spark follows a simple master and worker architecture. A machine is dedicated to manage the Spark cluster, which is known as the Spark Master node (c.f. namenode in a Hadoop Cluster). A set of machines are in charge of running the actual computation, namely, the Spark Worker nodes (c.f. data nodes in a Hadoop Cluster). A driver is a program that runs on the Spark Master node, which manages the interaction between the application and the client. Upon a job submission from the client, a Spark driver schedules the jobs by analyzing the sub tasks dependency and allocate the sub tasks to the executors. A list of executors (runnning on some worker nodes) receive tasks from the driver and reports the result upon completion. Spark Task Scheduling In this section, we take a look at how Spark divides a given job into tasks and schedules the tasks to the workers. As illustrated by the above diagram Spark takes 4 stages to schedule the job. Given a job, Spark builds a directed acyclic graph (DAG) which represents the dependencies among the operations performed in the job. Given the DAG, Spark splits the graph into multiple stages of tasks. Tasks are scheduled according to their stages, A later stage must not be started until the earlier stages are completed. Tasks scheduled to the workers then executed. A simple example Let's consider a simple example # spark job 1 r1 = sc.textFile(\"...\") r2 = r1.map(f) A spark driver takes the above program and construct the DAG, since it contain just a read followed by a map operations. It has the following DAG. graph TD r1 --map--> r2 It is clear that there is only one stage, (because there is only one operation, one set of inputs and one set of outputs). The Task Scheduler allocate the tasks in parallel, as follows. A less simple example Let's consider another example # spark job 2 r1 = sc.textFile(\"...\") r2 = r1.map(f) r3 = sc.textFile(\"...\") r4 = r3.map(g) r5 = r2.join(r4) r6 = r5.groupByKey() r7 = r6.map(h) r8 = r7.reduce(i) The DAG is as follows graph TD; r1 --map--> r2 r3 --map--> r4 r2 --join--> r5 r4 --join--> r5 r5 --groupByKey--> r6 r6 --map--> r7 r7 --reduce--> r8 There are mulitple way of dividing the above DAG into stages. For instance, we could naively turns each operation (each arrow) into a stage (we end up with many stages and poor parallelization) or we group the simple paths (straight line paths) into a stage. Spark takes a more intelligent approach by classifying different types of dependencies. Narrow dependencies - Input partition used by at most 1 output partition Wide dependencies - Input partition used by more than one output partitions Thus for Spark Job 2 , we further annotate the DAG with dependency types. graph TD; r1 --map/narrow--> r2 r3 --map/narrow--> r4 r2 --join/narrow--> r5 r4 --join/wide--> r5 r5 --groupByKey/wide--> r6 r6 --map/narrow--> r7 r7 --reduce--> r8 All the map operations are narrow. The join of r2 and r4 should be wide. However since the result must be ordered either by r2 's partition order or r4 's. Only one side of the operation is wide. In this case, we assume r5 's partition follows r2 's, hence r2 to r5 is narrow. groupByKey operations are wide. Next we can decide how many stages we need by allowing narrow dependencies preceding the same wide dependency to the grouped under the same stage, because they do not incur any network I/O. We apply task parallelism here., wide dependency initiates a new stage, as we need to wait for all the data operands to be fully computed before the operation being executed. graph TD; r1 --map/narrow/1--> r2 r3 --map/narrow/1--> r4 r2 --join/narrow/2--> r5 r4 --join/wide/2--> r5 r5 --groupByKey/wide/3--> r6 r6 --map/narrow/3--> r7 r7 --reduce/3--> r8 In the above, we stage the first two map operations. The join operation is assigned to the 2nd stage. The groupByKey initiates the 3rd stage which includes the following map and reduce. Spark performance tuning Knowing how Spark schedules a job into stages of sub tasks. We can optimize the job by rewriting it into an equivalent one such that Pre-partition or re-partition of the data Cache the common intermediate data Pre-partition or re-partition of the data Recall that a spark job is represented a DAG of stages If we know the join operation and pre- (or re-) arrange the data according to the key to partition mapping, we could reduce the shuffling. d1 = [(1, \"A\"), (2, \"B\"), (1, \"C\"), (2, \"D\"), (1, \"E\"), (3, \"F\")] d2 = [(1, 0.1), (2, 0.2), (2, 3.1), (3, 0)] r1 = sc.parallelize(d1) r2 = sc.parallelize(d2) let assume that r1 is by default partitioned randomly into two partitions and so is r2 . r1.glom().collect() # collect rdd into list by retaining the partition r2.glom().collect() # r1 being partitioned [[(1, 'A'), (2, 'B'), (1, 'C')], # p1 [(2, 'D'), (1, 'E'), (3, 'F')]] # p2 # r2 being partitioned [[(1, 0.1), (2, 0.2)], # p3 [(2, 3.1), (3, 0)]]) # p4 next we would like join r1 with r2 by key, i.e. the first component of the pairs r3 = r1.join(r2) r3.glom().collect() the result will be stored in three partitions [[(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], # p1 [(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))], # p2 [(3, ('F', 0))]] # p3 Note that the actual order of the collected result might not be same as above, Spark tries to reuse partitions being created whenever possible. (For breivity we omit an empty partition p4 .) As observed, there are tuple transferred from p1 to p2 , e.g. (2, 'B') and from p2 to p1 , e.g. (1, 'E') . These transfers can be eliminited if we manually control the partitioning of the initial RDDs, r4 = sc.parallelize(d1).partitionBy(3, lambda key: key) r5 = sc.parallelize(d2).partitionBy(3, lambda key: key) r4.glom().collect() r5.glom().collect() # r4 is partitioned [[(3, 'F')], #p1 [(1, 'A'), (1, 'C'), (1, 'E')], #p2 [(2, 'B'), (2, 'D')]] #p3 # r5 is partitioned [[(3, 0)], #p4 [(1, 0.1)], #p5 [(2, 0.2), (2, 3.1)]] #p6 when we perform the join r6 = r4.join(r5) r6.glom().collect() we have [ [(3, ('F', 0))], #p1 [(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], #p2 [(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))] #p3 ] The number of tuples being transferred across partition is now minimized. Sample code can be found here. https://colab.research.google.com/drive/1XO1hqcRCn9JKu0tkRb0ANQylCnBod3B-?usp=sharing Cache the intermediate data Recall that in Spark transformation are lazy until a shuffling is required. Laziness is a double-edged sword. In the above, the orange partition (data) is being used by 3 different \"sinks\". If all the operations above the last levels are transformations (i.e. lazy.) Having 3 sink operations will require most of the intermediate partitions (all boxes except for the last) For example consider the following data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)] r1 = sc.parallelize(data) r2 = r1.map( lambda x:(x[0], x[1] * 0.01)) r3 = r2.groupByKey() r4 = r3.map( lambda x:(x[0], std(x[1]))) r5 = r3.map( lambda x:(x[0], min(x[1]))) r4.collect() r5.collect() There are two downstream operation of r3 , namely r4 computing the standard deviation by key, and r5 computing the minimum by key. Due to the fact that r3 is lazy. The r4.collect() action (sink) triggers the computation of r1 to r3 and r4 . The r5.collect() triggers the recomputation of r1 to r3 and the computation of r5 . If we materialize r3 and cache it, we would avoid the recomputation of r1 to r3 . data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)] r1 = sc.parallelize(data) r2 = r1.map( lambda x:(x[0], x[1] * 0.01)) r3 = r2.groupByKey().cache() r4 = r3.map( lambda x:(x[0], std(x[1]))) r5 = r3.map( lambda x:(x[0], min(x[1]))) r4.collect() r5.collect() Sample code https://colab.research.google.com/drive/1QqUSa5Kkjpw3Avw2DlWX9ZMrZcsnDz5x?usp=sharing Other optimization tricks Besides the above mentioned two approaches, a trick that we've used in RDBMS for optimization is also applicable to Spark. i.e. apply filter as early as possible so as to reduce the size of intermediate output. Another Spark specific optimization trick is to rewriting groupByKey().map(...) by reduceByKey(...) . However in some cases, a rewrite solution might not exist. Spark Failure Recovery Recall that for any MapReduce implementation to produce deterministic results. The computation must be pure . It turns out that purity property makes failure recovery much easier. For each Spark job, a lineage of the sub tasks is computed. In the event of failure, the Spark driver will refer to the lineage and recompute the affected sub-tasks. Thanks to the purity property, partially completed and incomplete computation can always be computed without the need of restoring the original state. For instance consider the following job Where all the square boxes denote the partitions of some RDDs, suppose partition C1 is faulty. Based on the diagram (lineage), we know that we can recompute C1 elsewhere by using B1 and B2 . Spark DataFrame Besides Spark RDD, Spark offers DataFrame as a higher level API interface to the programmers and data engineers. The ussage is influenced and inspired by Python's Pandas. In a nutshell, a dataframe can be seenas a schema plus a set of RDDs. Since Dataframe was designed for machine learning applications, it adopts the column-based data structure instead of row based. Question: Why Columnar? Hint: How data are used in ML model? Creating Spark Dataframe We can convert a Spark rdd into a dataframe. data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)] r1 = sc.parallelize(data) df1 = r1.toDF(\"id\", \"score\") Alternatively, we can create DataFrames directly from a CSV file. Given file hdfs://127.0.0.1:9000/foo.csv foo,bar 1,true 2,false 3,true 4,false df = sparkSession.read\\ .option(\"header\", \"true\")\\ .option(\"inferSchema\", \"true\")\\ .csv(\"hdfs://127.0.0.1:9000/foo.csv\") df.printSchema() shows root |-- foo: integer (nullable = true) |-- bar: boolean (nullable = true) Note in the above, Spark loads a text file (CSV) from HDFS and infers the schema based on the first line and the values. DataFrame APIs Let's have tour of the Spark DataFrame APIs by going through some examples. Here is the data we use in the examples data = [(\"100001\", \"Ace\", \"50043\", 90), \\ (\"100002\", \"Brandon\", \"50043\", 95), \\ (\"100003\", \"Cheryl\", \"50043\", 80)] distData = sc.parallelize(data) df = distData.toDF([\"studentid\", \"name\", \\ \"module\", \"score\"]) df.show() studentid name module score 100001 Ace 50043 90 100002 Brandon 50043 95 100003 Cheryl 50043 80 Column Projection To project (select the columns) we use the .select() method. df.select(df[\"studentid\"], df[\"score\"]).show() # or from pyspark.sql.functions import col df.select(col(\"studentid\"), col(\"score\")).show() studentid score 100001 90 100002 95 100003 80 To compute a new column based on the existing one, we use an overloaded version of the .select() method whose first argument is the operation and the second argument is the name of the new column. from pyspark.sql.functions import concat, lit df.select(concat(df[\"studentid\"]\\ ,lit(\"@mymail.sutd.edu.sg\"))\\ .alias(\"email\")).show() email 100001@mymail.sut... 100002@mymail.sut... 100003@mymail.sut... For the full set of builtin funtcions for column operations. https://spark.apache.org/docs/3.0.1/api/python/pyspark.sql.html There are times we want to create a new column and keeping the old columns. df.withColumn(\"email\",concat(col(\"studentid\"),\\ lit(\"@mymail.sutd.edu.sg\")))\\ .show() studentid name module score email 100001 Ace 50043 90 100001@mymail.sut... 100002 Brandon 50043 95 100002@mymail.sut... 100003 Cheryl 50043 80 100003@mymail.sut... Row filtering For row filterings, we use .filter() method. df.filter(col(\"studentid\") == \"100003\").show() studentid name module score 100003 Cheryl 50043 80 And similar for range filter df.filter(col(\"score\") > 90).show() studentid name module score 100002 Brandon 50043 95 lit() is optional here, pyspark inserts it for us. Group By and Aggregation For aggregation, we use .groupBy() df.groupBy(\"module\").avg().show() module avg(score) 50043 88.33333333333333 Join For join, we use .join() moddata = [(\"50043\", \"Database and Big Data Systems\")] distmodData = sc.parallelize(moddata) moddf = distmodData.toDF([\"module\", \"modname\"]) df.join(moddf, df[\"module\"] == moddf[\"module\"], \"inner\")\\ .select(df[\"studentid\"], df[\"name\"], df[\"module\"],\\ df[\"score\"], moddf[\"modname\"]).show() studentid name module score modname 100001 Ace 50043 90 Database and Big ... 100002 Brandon 50043 95 Database and Big ... 100003 Cheryl 50043 80 Database and Big ... Spark SQL Besides Spark RDD, Spark allows program to use SQL query to perform data transformation and action. For example, df.createOrReplaceTempView(\"students\") spark.sql(\"SELECT * FROM students\").show() studentid name module score 100001 Ace 50043 90 100002 Brandon 50043 95 100003 Cheryl 50043 80 With some notebook support, we can even use SQL to perform data visualization. Spark Machine Learning Spark comes with two differen Machine Learning libraries. MLLib package - for RDD ML package - for dataframe (and dataset) MLLib package MLLib package offers a lower level access of data type such as vectors and label points. Vectors In Spark, vectors are local data collection (non-distributed). There are dense vectors and sparse vectors. For dense vector - all values need to be specified when it is created from pyspark.mllib.linalg import * dv = Vectors.dense(1.0, 0.0, 3.0) For sparse vector - we don't need to specify all the value, instead we specify the size of the vector as well as the non-zero values. sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0]) # or sv2 = Vectors.sparse(3, [(0, 1.0), (2, 3.0)]) Labeled points With features extracted as vectors. We need to find a way to label them. Spark MLLib comes with its own labeled point data type from pyspark.mllib.regression import * # Create a labeled point with a positive label # and a dense feature vector. pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0)) # Create a labeled point with a negative label # and a sparse feature vector. neg = LabeledPoint(0.0, Vectors.sparse(3, \\ [(0, 1.0), (2, 3.0)])) Training an inference With labeled points defined and extracted, assuming pos = ... # RDD of labeled points neg = ... # RDD of labeled points we train the model from pyspark.mllib.classification import SVMWithSGD training = pos + neg numIteration = 20 model = SVMWithSGD.train(training, numIterations) which is a support vector machine with SGD algorithm. To perform inference, we need to feed a new sample as a vector to the model. newInstance = Vectors.dense(1.0, 2.0, 3.0) model.predict(newInstance) ML Package As the ML package is targetting the higher level data structures (Dataframe and Dataset), machine learning models in ML package are built using the pipeline. The Training Pipeline One of the training pipelines is known as the estimator In the diagram above, it illustrate the pipeline of training a classifier using logistic regression. from pyspark.ml import Pipeline, PipelineModel from pyspark.ml.classification import LogisticRegression from pyspark.ml.feature import HashingTF, Tokenizer data = spark.createDataFrame([ (0, \"a b c d e spark\", 1.0), (1, \"b d\", 0.0), (2, \"spark f g h\", 1.0), (3, \"hadoop mapreduce\", 0.0), (4, \"spark is scaling\", 1.0), (5, \"random stuff\", 0.0) ], [\"id\", \"text\", \"label\"]) train, test = data.randomSplit([0.7, 0.3], seed=12345) # Configure an estimator pipeline, tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") lr = LogisticRegression(maxIter=10, regParam=0.001) pipeline = Pipeline(stages=[tokenizer, hashingTF, lr]) # Fit the pipeline to train model = pipeline.fit(train) The transformer pipeline The inference pipeline on the other hand is known as the transformer # Configure an Inference pipeline # Note now model include tokenizterm hashingTF, and lr pipeline_model = PipelineModel(stages=[model]) prediction = pipeline_model.transform(test) result = prediction.select(\"id\", \"text\", \"probability\", \"prediction\") result.show() Sample code https://colab.research.google.com/drive/1ZI-BG2XaB3AOyzPrXeqO7xqNGYycCJ-U?usp=sharing Spark Streaming Spark offers Streaming API which handles real time (infinite) input data. The real-timeness is approxmiated by chopping the data stream into small batches. These small batches are fed to the spark application. For example the following is a simplified version of a data streaming application that computes the page views by URL over time. from pyspark import SparkContext from pyspark.streaming import StreamingContext sc = SparkContext(\"local[2]\", \"PageView\") ssc = StreamingContext(sc, 1) lines = ssc.socketTextStream(\"localhost\", 9999) pageViews = lines.map(lambda l:parse(l)) ones = pageViews.map(lambda x: (x.url, 1)) counts = ones.runningReduce(lambda x,y: x+y) Additional References Spark Dataframe https://spark.apache.org/docs/latest/sql-getting-started.html Spark MLLib and ML package https://spark.apache.org/docs/latest/ml-guide.html Spark Streaming https://spark.apache.org/docs/latest/streaming-programming-guide.html","title":"50.043 Spark"},{"location":"notes/l11_spark/#50043-spark","text":"","title":"50.043 Spark"},{"location":"notes/l11_spark/#learning-outcomes","text":"By the end of this lesson, you are able to Differentiate Hadoop MapReduce and Spark Apply Spark based on application requirements Develop data engineering process using Spark RDD Reason about Spark Execution Model Develop machine learning application using Spark MLLib Explain Spark Architecture Develop Data processing application using Spark Dataframe Develop Machine Learning application using Spark ML package Explain Spark Streaming","title":"Learning Outcomes"},{"location":"notes/l11_spark/#spark-vs-hadoop-mapreduce","text":"Hadoop MapReduce was created to batch process data once or twice, e.g. web search index, processing crawled data, etc. When machine learning being applied to big data, the terabyte or zeta byte of data probably need to be processed iteratively. For instance, if we need to run gradient descent thousands of times. On top of that data visualization for big data also impose further challenge. It requires data to be reprocessed based on the user inputs such as sorting and filtering constraints. Hadoop MapReduce is no longer suitable for these applications because of the following 1. each of mapper (and educer) task needs to transfer intermediate data to the disk back and forth. 2. its rigit computation model (i.e. one step of map followed by one step of reduce) makes most of the application look unnecessarily complex. 3. it does not utilize much of RAM. Many of the MapReduce applications are disk and network I/O bound rather than RAM and CPU bound. 4. it is hard to redistribute the workload Apache Spark is a project which started off as an academic reseach idea and became an enterprise level success. It addressed the above-mentioned limitations of the Hadoop MapReduce by introducing the following features Resilient distributed datasets, which act as the primary data structure for distributed map reduce operation It unions all the available RAM from the cluster (all data nodes) to form a large pool of virtual RAM. RDD and its derivative such as dataframe and dataset, are in memory parallel distributed data structure to be used in the virtual RAM pool. It has a MapReduce like programming interface (closer to our toy MapReduce library compared to the Hadoop MapReduce) It offers fault tolerance, RDD, dataframe and datasets can always be re-computed in case of node failure Machine Learning Libraries, Graph computation libraries. Spark supports many mainstream programming languagues such as Scala, Python, R, Java and SQL. In this module we consider the Python interface.","title":"Spark VS Hadoop MapReduce"},{"location":"notes/l11_spark/#spark-rdd-api","text":"The primary data structure of the Spark RDD API is the RDD. data = [1, 2, 3, 4, 5] distData = sc.parallelize(data) In the above code snippet, we initialize a list of integers and turn it into an RDD distData . Alternatively, we can load the data from a file. distData = sc.textFile(\"hdfs://127.0.0.1:9000/data.txt\") Given the RDD, we can now perform data manipulation. There are two kinds of RDD APIs, namely Transformations and Actions . Transformation APIs are lazy and action APIs are strict .","title":"Spark RDD API"},{"location":"notes/l11_spark/#lazy-vs-strict-evaluation","text":"In lazy evaluation, the argument of a function is not fully computed until it is needed. We can mimic this in Python using generator and iterator. r = range(2,-1,-1) # a range 2,1,0 l = ( 4 // x for x in r) # a generator 2, 4, div_by_0 def takeOne(l): res = next(iter(l), None) if res is None: return error(\" ... \") else: return res takeOne(l) # yield 2 When takeOne(l) is called, only the first element of l is computed, the rest are discarded. In strict evaluation, the argument of a function is alwaysfully computed. def takeOneStrict(l): res = next(iter(list(l)), None) if res is None: return error(\" ... \") else: return res takeOneStrict(l) # yield an div by zero error. When takeOneStrict(l) is called, even though only the first element of l is required, the rest are computed eagerly, thanks to the list() function call materializing the generator.","title":"Lazy vs Strict Evaluation"},{"location":"notes/l11_spark/#rdd-transformations","text":"All Transformations takes the current RDD as (part of) the input and return some a new RDD as output. Let l , l1 and l2 be RDDs, f be a function RDD APIs Description Toy MapReduce equivalent l.map(f) map(f,l) l.flatMap(f) flatMap(f,l) l.filter(f) filter(f,l) l.reduceByKey(f) reduceByKey(f,l) l.mapPartition(f) similar to map , f takes an iterator and produces an iterator NA l.distinct() all distinct elems N.A. l.sample(b,ratio,seed) sample dataset. b : a boolean value to indicate w/wo replacement. ratio : a value range [0,1] N.A. l.aggregateByKey(zv)(sop,cop) zv : accumulated value. sop : intra-partition aggregation function. cop : inter-partition aggregation function similar to reduceByKey(f,l,acc) , except that we don't have 2 version of f l1.union(l2) union l1 l2 l1 + l2 l1.intersection(l2) the intersection of elements from l1 and l2 N.A. l1.groupByKey() group elemnts by keys shuffle(l1) l1.sortByKey() sort by keys N.A. l1.join(l2) join l1 l2 by keys we've done it in lab l1.cogroup(l2) similar to join , it returns RDDs of (key, ([v1,..], [v2,..])) , [v1,...] are values from l1 , [v2,...] are values from l2 N.A. Note that the RDDS APIs follow the builtin Scala library's convention, map , filter and etc are methods of the List class.","title":"RDD Transformations"},{"location":"notes/l11_spark/#rdd-actions","text":"All Actions takes the current RDD as (part of) the input and return some value that is not an RDD. It forces computation to happen. RDDs Desc Toy MR l.reduce(f) reduce(f,l) l.collect() converts rdd to a local array l.count() len(l) l.first() l[0] l.take(n) returns an array l[:n] l.saveAsTextFile(path) save rdd to text file N.A. l.countByKey() return hash map of key and count N.A. l.foreach(f) run a function for each element in the dataset with side-effects for x in l: ...","title":"RDD Actions"},{"location":"notes/l11_spark/#special-transformation","text":"Some transformation/opereations such as reduceByKey , join , groupByKey and sortByKey will trigger a shuffle event, in which Spark redistribute the data across partititon, which means intermediate results from lazy operations will be materialized.","title":"Special Transformation"},{"location":"notes/l11_spark/#wordcount-example-in-spark","text":"The follow code snippet, we find the wordcount application implemented using PySpark. import sys from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName(\"Wordcount Application\") sc = SparkContext(conf=conf) text_file = sc.textFile(\"hdfs://localhost:9000/input/\") counts = text_file.flatMap(lambda line: line.split(\" \")) \\ .map(lambda word: (word, 1)) \\ .reduceByKey(lambda a, b: a + b) counts.saveAsTextFile(\"hdfs://localhost:9000/output/\") sc.stop() We can compare it with the version in toy MapReduce, and find many similarities infile = open(sys.argv[1], 'r') lines = [] for line in infile: lines.append(line.strip()) ws = flatMap(lambda line:line.split(\" \"),lines) w1s = map(lambda w:(w,1), ws) res = reduceByKey(lambda x,y:x+y,w1s,0) with open(sys.argv[2], 'w') as out: for w,c in res: out.write(w + \"\\t\" + str(c) + \"\\n\") We will see more examples of using PySpark in the lecture and the labs.","title":"Wordcount example in Spark"},{"location":"notes/l11_spark/#spark-architecture","text":"Next we consider the how Spark manages the execution model. In the following, we find the Spark Architecture (in simple standalone mode) {width=100%} Like Hadoop, Spark follows a simple master and worker architecture. A machine is dedicated to manage the Spark cluster, which is known as the Spark Master node (c.f. namenode in a Hadoop Cluster). A set of machines are in charge of running the actual computation, namely, the Spark Worker nodes (c.f. data nodes in a Hadoop Cluster). A driver is a program that runs on the Spark Master node, which manages the interaction between the application and the client. Upon a job submission from the client, a Spark driver schedules the jobs by analyzing the sub tasks dependency and allocate the sub tasks to the executors. A list of executors (runnning on some worker nodes) receive tasks from the driver and reports the result upon completion.","title":"Spark Architecture"},{"location":"notes/l11_spark/#spark-task-scheduling","text":"In this section, we take a look at how Spark divides a given job into tasks and schedules the tasks to the workers. As illustrated by the above diagram Spark takes 4 stages to schedule the job. Given a job, Spark builds a directed acyclic graph (DAG) which represents the dependencies among the operations performed in the job. Given the DAG, Spark splits the graph into multiple stages of tasks. Tasks are scheduled according to their stages, A later stage must not be started until the earlier stages are completed. Tasks scheduled to the workers then executed.","title":"Spark Task Scheduling"},{"location":"notes/l11_spark/#a-simple-example","text":"Let's consider a simple example # spark job 1 r1 = sc.textFile(\"...\") r2 = r1.map(f) A spark driver takes the above program and construct the DAG, since it contain just a read followed by a map operations. It has the following DAG. graph TD r1 --map--> r2 It is clear that there is only one stage, (because there is only one operation, one set of inputs and one set of outputs). The Task Scheduler allocate the tasks in parallel, as follows.","title":"A simple example"},{"location":"notes/l11_spark/#a-less-simple-example","text":"Let's consider another example # spark job 2 r1 = sc.textFile(\"...\") r2 = r1.map(f) r3 = sc.textFile(\"...\") r4 = r3.map(g) r5 = r2.join(r4) r6 = r5.groupByKey() r7 = r6.map(h) r8 = r7.reduce(i) The DAG is as follows graph TD; r1 --map--> r2 r3 --map--> r4 r2 --join--> r5 r4 --join--> r5 r5 --groupByKey--> r6 r6 --map--> r7 r7 --reduce--> r8 There are mulitple way of dividing the above DAG into stages. For instance, we could naively turns each operation (each arrow) into a stage (we end up with many stages and poor parallelization) or we group the simple paths (straight line paths) into a stage. Spark takes a more intelligent approach by classifying different types of dependencies. Narrow dependencies - Input partition used by at most 1 output partition Wide dependencies - Input partition used by more than one output partitions Thus for Spark Job 2 , we further annotate the DAG with dependency types. graph TD; r1 --map/narrow--> r2 r3 --map/narrow--> r4 r2 --join/narrow--> r5 r4 --join/wide--> r5 r5 --groupByKey/wide--> r6 r6 --map/narrow--> r7 r7 --reduce--> r8 All the map operations are narrow. The join of r2 and r4 should be wide. However since the result must be ordered either by r2 's partition order or r4 's. Only one side of the operation is wide. In this case, we assume r5 's partition follows r2 's, hence r2 to r5 is narrow. groupByKey operations are wide. Next we can decide how many stages we need by allowing narrow dependencies preceding the same wide dependency to the grouped under the same stage, because they do not incur any network I/O. We apply task parallelism here., wide dependency initiates a new stage, as we need to wait for all the data operands to be fully computed before the operation being executed. graph TD; r1 --map/narrow/1--> r2 r3 --map/narrow/1--> r4 r2 --join/narrow/2--> r5 r4 --join/wide/2--> r5 r5 --groupByKey/wide/3--> r6 r6 --map/narrow/3--> r7 r7 --reduce/3--> r8 In the above, we stage the first two map operations. The join operation is assigned to the 2nd stage. The groupByKey initiates the 3rd stage which includes the following map and reduce.","title":"A less simple example"},{"location":"notes/l11_spark/#spark-performance-tuning","text":"Knowing how Spark schedules a job into stages of sub tasks. We can optimize the job by rewriting it into an equivalent one such that Pre-partition or re-partition of the data Cache the common intermediate data","title":"Spark performance tuning"},{"location":"notes/l11_spark/#pre-partition-or-re-partition-of-the-data","text":"Recall that a spark job is represented a DAG of stages If we know the join operation and pre- (or re-) arrange the data according to the key to partition mapping, we could reduce the shuffling. d1 = [(1, \"A\"), (2, \"B\"), (1, \"C\"), (2, \"D\"), (1, \"E\"), (3, \"F\")] d2 = [(1, 0.1), (2, 0.2), (2, 3.1), (3, 0)] r1 = sc.parallelize(d1) r2 = sc.parallelize(d2) let assume that r1 is by default partitioned randomly into two partitions and so is r2 . r1.glom().collect() # collect rdd into list by retaining the partition r2.glom().collect() # r1 being partitioned [[(1, 'A'), (2, 'B'), (1, 'C')], # p1 [(2, 'D'), (1, 'E'), (3, 'F')]] # p2 # r2 being partitioned [[(1, 0.1), (2, 0.2)], # p3 [(2, 3.1), (3, 0)]]) # p4 next we would like join r1 with r2 by key, i.e. the first component of the pairs r3 = r1.join(r2) r3.glom().collect() the result will be stored in three partitions [[(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], # p1 [(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))], # p2 [(3, ('F', 0))]] # p3 Note that the actual order of the collected result might not be same as above, Spark tries to reuse partitions being created whenever possible. (For breivity we omit an empty partition p4 .) As observed, there are tuple transferred from p1 to p2 , e.g. (2, 'B') and from p2 to p1 , e.g. (1, 'E') . These transfers can be eliminited if we manually control the partitioning of the initial RDDs, r4 = sc.parallelize(d1).partitionBy(3, lambda key: key) r5 = sc.parallelize(d2).partitionBy(3, lambda key: key) r4.glom().collect() r5.glom().collect() # r4 is partitioned [[(3, 'F')], #p1 [(1, 'A'), (1, 'C'), (1, 'E')], #p2 [(2, 'B'), (2, 'D')]] #p3 # r5 is partitioned [[(3, 0)], #p4 [(1, 0.1)], #p5 [(2, 0.2), (2, 3.1)]] #p6 when we perform the join r6 = r4.join(r5) r6.glom().collect() we have [ [(3, ('F', 0))], #p1 [(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], #p2 [(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))] #p3 ] The number of tuples being transferred across partition is now minimized. Sample code can be found here. https://colab.research.google.com/drive/1XO1hqcRCn9JKu0tkRb0ANQylCnBod3B-?usp=sharing","title":"Pre-partition or re-partition of the data"},{"location":"notes/l11_spark/#cache-the-intermediate-data","text":"Recall that in Spark transformation are lazy until a shuffling is required. Laziness is a double-edged sword. In the above, the orange partition (data) is being used by 3 different \"sinks\". If all the operations above the last levels are transformations (i.e. lazy.) Having 3 sink operations will require most of the intermediate partitions (all boxes except for the last) For example consider the following data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)] r1 = sc.parallelize(data) r2 = r1.map( lambda x:(x[0], x[1] * 0.01)) r3 = r2.groupByKey() r4 = r3.map( lambda x:(x[0], std(x[1]))) r5 = r3.map( lambda x:(x[0], min(x[1]))) r4.collect() r5.collect() There are two downstream operation of r3 , namely r4 computing the standard deviation by key, and r5 computing the minimum by key. Due to the fact that r3 is lazy. The r4.collect() action (sink) triggers the computation of r1 to r3 and r4 . The r5.collect() triggers the recomputation of r1 to r3 and the computation of r5 . If we materialize r3 and cache it, we would avoid the recomputation of r1 to r3 . data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)] r1 = sc.parallelize(data) r2 = r1.map( lambda x:(x[0], x[1] * 0.01)) r3 = r2.groupByKey().cache() r4 = r3.map( lambda x:(x[0], std(x[1]))) r5 = r3.map( lambda x:(x[0], min(x[1]))) r4.collect() r5.collect() Sample code https://colab.research.google.com/drive/1QqUSa5Kkjpw3Avw2DlWX9ZMrZcsnDz5x?usp=sharing","title":"Cache the intermediate data"},{"location":"notes/l11_spark/#other-optimization-tricks","text":"Besides the above mentioned two approaches, a trick that we've used in RDBMS for optimization is also applicable to Spark. i.e. apply filter as early as possible so as to reduce the size of intermediate output. Another Spark specific optimization trick is to rewriting groupByKey().map(...) by reduceByKey(...) . However in some cases, a rewrite solution might not exist.","title":"Other optimization tricks"},{"location":"notes/l11_spark/#spark-failure-recovery","text":"Recall that for any MapReduce implementation to produce deterministic results. The computation must be pure . It turns out that purity property makes failure recovery much easier. For each Spark job, a lineage of the sub tasks is computed. In the event of failure, the Spark driver will refer to the lineage and recompute the affected sub-tasks. Thanks to the purity property, partially completed and incomplete computation can always be computed without the need of restoring the original state. For instance consider the following job Where all the square boxes denote the partitions of some RDDs, suppose partition C1 is faulty. Based on the diagram (lineage), we know that we can recompute C1 elsewhere by using B1 and B2 .","title":"Spark Failure Recovery"},{"location":"notes/l11_spark/#spark-dataframe","text":"Besides Spark RDD, Spark offers DataFrame as a higher level API interface to the programmers and data engineers. The ussage is influenced and inspired by Python's Pandas. In a nutshell, a dataframe can be seenas a schema plus a set of RDDs. Since Dataframe was designed for machine learning applications, it adopts the column-based data structure instead of row based.","title":"Spark DataFrame"},{"location":"notes/l11_spark/#question-why-columnar","text":"Hint: How data are used in ML model?","title":"Question: Why Columnar?"},{"location":"notes/l11_spark/#creating-spark-dataframe","text":"We can convert a Spark rdd into a dataframe. data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)] r1 = sc.parallelize(data) df1 = r1.toDF(\"id\", \"score\") Alternatively, we can create DataFrames directly from a CSV file. Given file hdfs://127.0.0.1:9000/foo.csv foo,bar 1,true 2,false 3,true 4,false df = sparkSession.read\\ .option(\"header\", \"true\")\\ .option(\"inferSchema\", \"true\")\\ .csv(\"hdfs://127.0.0.1:9000/foo.csv\") df.printSchema() shows root |-- foo: integer (nullable = true) |-- bar: boolean (nullable = true) Note in the above, Spark loads a text file (CSV) from HDFS and infers the schema based on the first line and the values.","title":"Creating Spark Dataframe"},{"location":"notes/l11_spark/#dataframe-apis","text":"Let's have tour of the Spark DataFrame APIs by going through some examples. Here is the data we use in the examples data = [(\"100001\", \"Ace\", \"50043\", 90), \\ (\"100002\", \"Brandon\", \"50043\", 95), \\ (\"100003\", \"Cheryl\", \"50043\", 80)] distData = sc.parallelize(data) df = distData.toDF([\"studentid\", \"name\", \\ \"module\", \"score\"]) df.show() studentid name module score 100001 Ace 50043 90 100002 Brandon 50043 95 100003 Cheryl 50043 80","title":"DataFrame APIs"},{"location":"notes/l11_spark/#column-projection","text":"To project (select the columns) we use the .select() method. df.select(df[\"studentid\"], df[\"score\"]).show() # or from pyspark.sql.functions import col df.select(col(\"studentid\"), col(\"score\")).show() studentid score 100001 90 100002 95 100003 80 To compute a new column based on the existing one, we use an overloaded version of the .select() method whose first argument is the operation and the second argument is the name of the new column. from pyspark.sql.functions import concat, lit df.select(concat(df[\"studentid\"]\\ ,lit(\"@mymail.sutd.edu.sg\"))\\ .alias(\"email\")).show() email 100001@mymail.sut... 100002@mymail.sut... 100003@mymail.sut... For the full set of builtin funtcions for column operations. https://spark.apache.org/docs/3.0.1/api/python/pyspark.sql.html There are times we want to create a new column and keeping the old columns. df.withColumn(\"email\",concat(col(\"studentid\"),\\ lit(\"@mymail.sutd.edu.sg\")))\\ .show() studentid name module score email 100001 Ace 50043 90 100001@mymail.sut... 100002 Brandon 50043 95 100002@mymail.sut... 100003 Cheryl 50043 80 100003@mymail.sut...","title":"Column Projection"},{"location":"notes/l11_spark/#row-filtering","text":"For row filterings, we use .filter() method. df.filter(col(\"studentid\") == \"100003\").show() studentid name module score 100003 Cheryl 50043 80 And similar for range filter df.filter(col(\"score\") > 90).show() studentid name module score 100002 Brandon 50043 95 lit() is optional here, pyspark inserts it for us.","title":"Row filtering"},{"location":"notes/l11_spark/#group-by-and-aggregation","text":"For aggregation, we use .groupBy() df.groupBy(\"module\").avg().show() module avg(score) 50043 88.33333333333333","title":"Group By and Aggregation"},{"location":"notes/l11_spark/#join","text":"For join, we use .join() moddata = [(\"50043\", \"Database and Big Data Systems\")] distmodData = sc.parallelize(moddata) moddf = distmodData.toDF([\"module\", \"modname\"]) df.join(moddf, df[\"module\"] == moddf[\"module\"], \"inner\")\\ .select(df[\"studentid\"], df[\"name\"], df[\"module\"],\\ df[\"score\"], moddf[\"modname\"]).show() studentid name module score modname 100001 Ace 50043 90 Database and Big ... 100002 Brandon 50043 95 Database and Big ... 100003 Cheryl 50043 80 Database and Big ...","title":"Join"},{"location":"notes/l11_spark/#spark-sql","text":"Besides Spark RDD, Spark allows program to use SQL query to perform data transformation and action. For example, df.createOrReplaceTempView(\"students\") spark.sql(\"SELECT * FROM students\").show() studentid name module score 100001 Ace 50043 90 100002 Brandon 50043 95 100003 Cheryl 50043 80 With some notebook support, we can even use SQL to perform data visualization.","title":"Spark SQL"},{"location":"notes/l11_spark/#spark-machine-learning","text":"Spark comes with two differen Machine Learning libraries. MLLib package - for RDD ML package - for dataframe (and dataset)","title":"Spark Machine Learning"},{"location":"notes/l11_spark/#mllib-package","text":"MLLib package offers a lower level access of data type such as vectors and label points.","title":"MLLib package"},{"location":"notes/l11_spark/#vectors","text":"In Spark, vectors are local data collection (non-distributed). There are dense vectors and sparse vectors. For dense vector - all values need to be specified when it is created from pyspark.mllib.linalg import * dv = Vectors.dense(1.0, 0.0, 3.0) For sparse vector - we don't need to specify all the value, instead we specify the size of the vector as well as the non-zero values. sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0]) # or sv2 = Vectors.sparse(3, [(0, 1.0), (2, 3.0)])","title":"Vectors"},{"location":"notes/l11_spark/#labeled-points","text":"With features extracted as vectors. We need to find a way to label them. Spark MLLib comes with its own labeled point data type from pyspark.mllib.regression import * # Create a labeled point with a positive label # and a dense feature vector. pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0)) # Create a labeled point with a negative label # and a sparse feature vector. neg = LabeledPoint(0.0, Vectors.sparse(3, \\ [(0, 1.0), (2, 3.0)]))","title":"Labeled points"},{"location":"notes/l11_spark/#training-an-inference","text":"With labeled points defined and extracted, assuming pos = ... # RDD of labeled points neg = ... # RDD of labeled points we train the model from pyspark.mllib.classification import SVMWithSGD training = pos + neg numIteration = 20 model = SVMWithSGD.train(training, numIterations) which is a support vector machine with SGD algorithm. To perform inference, we need to feed a new sample as a vector to the model. newInstance = Vectors.dense(1.0, 2.0, 3.0) model.predict(newInstance)","title":"Training an inference"},{"location":"notes/l11_spark/#ml-package","text":"As the ML package is targetting the higher level data structures (Dataframe and Dataset), machine learning models in ML package are built using the pipeline.","title":"ML Package"},{"location":"notes/l11_spark/#the-training-pipeline","text":"One of the training pipelines is known as the estimator In the diagram above, it illustrate the pipeline of training a classifier using logistic regression. from pyspark.ml import Pipeline, PipelineModel from pyspark.ml.classification import LogisticRegression from pyspark.ml.feature import HashingTF, Tokenizer data = spark.createDataFrame([ (0, \"a b c d e spark\", 1.0), (1, \"b d\", 0.0), (2, \"spark f g h\", 1.0), (3, \"hadoop mapreduce\", 0.0), (4, \"spark is scaling\", 1.0), (5, \"random stuff\", 0.0) ], [\"id\", \"text\", \"label\"]) train, test = data.randomSplit([0.7, 0.3], seed=12345) # Configure an estimator pipeline, tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") lr = LogisticRegression(maxIter=10, regParam=0.001) pipeline = Pipeline(stages=[tokenizer, hashingTF, lr]) # Fit the pipeline to train model = pipeline.fit(train)","title":"The Training Pipeline"},{"location":"notes/l11_spark/#the-transformer-pipeline","text":"The inference pipeline on the other hand is known as the transformer # Configure an Inference pipeline # Note now model include tokenizterm hashingTF, and lr pipeline_model = PipelineModel(stages=[model]) prediction = pipeline_model.transform(test) result = prediction.select(\"id\", \"text\", \"probability\", \"prediction\") result.show()","title":"The transformer pipeline"},{"location":"notes/l11_spark/#sample-code","text":"https://colab.research.google.com/drive/1ZI-BG2XaB3AOyzPrXeqO7xqNGYycCJ-U?usp=sharing","title":"Sample code"},{"location":"notes/l11_spark/#spark-streaming","text":"Spark offers Streaming API which handles real time (infinite) input data. The real-timeness is approxmiated by chopping the data stream into small batches. These small batches are fed to the spark application. For example the following is a simplified version of a data streaming application that computes the page views by URL over time. from pyspark import SparkContext from pyspark.streaming import StreamingContext sc = SparkContext(\"local[2]\", \"PageView\") ssc = StreamingContext(sc, 1) lines = ssc.socketTextStream(\"localhost\", 9999) pageViews = lines.map(lambda l:parse(l)) ones = pageViews.map(lambda x: (x.url, 1)) counts = ones.runningReduce(lambda x,y: x+y)","title":"Spark Streaming"},{"location":"notes/l11_spark/#additional-references","text":"Spark Dataframe https://spark.apache.org/docs/latest/sql-getting-started.html Spark MLLib and ML package https://spark.apache.org/docs/latest/ml-guide.html Spark Streaming https://spark.apache.org/docs/latest/streaming-programming-guide.html","title":"Additional References"},{"location":"notes/l12_yarn/","text":"50.043 Yarn Learning Outcomes By the end of this lesson, you are able to Articulate the purpose of Hadoop YARN Identify the shortcoming of JobTracker and TaskTracker in Hadoop V1 Describe how YARN addresses the above issues List and differentiate the different YARN schedulers Hadoop and Eco System We have many tools in the big data tool box Hadoop HDFS - Main Storage Hadoop MapReduce - Low CPU / Low RAM consumption batch job Spark Job - High CPU / GPU / RAM batch job Spark Stream - Realtime stream ... Spark and Hadoop v1 have incompatible resource managers, they don't understand each other. It is crictical to have a unified resource management tool for all kinds of jobs. Hadoop v1 Resource Management System Hadoop v1 Resource Management System has many issues. The namenode acts as the master node and the job tracker. It is overloaded with * Resource Management * Scheduling * Job monitoring * Job lifecycle * Fault-tolerence It soon becomes the bottle neck of the system. It does not scale more than 4000 nodes and 40,000 tasks. The datanodes acts as the taskers. They have static task slots, for instance, 2 slots for mapper task and 2 slots for reducer task. The mapper slots cannot be used for reducer tasks. This leads to additional constraints in task schedules and limitation for load balancing. YARN YARN is a general resource management system shipped with Hadoop V2 on wards. It is designed with Hadoop and its eco system. It overcomes most of the limitations found in Hadoop V1. It provides an extensible APIs for non-Hadoop tasks. Hadoop V1 vs YARN In the following diagram we find the correspondence between Hadoop V1 resource manager and YARN {width=80%} On the topmost leve, the JobTracker is replaced by an application master which in charge of managing an application running on the cluster. The Application master it is not running on the namenode, instead it runs in one of the task slots in a worker node. The resource managemer which coordinates among the hardware resources and many application masters and node managers, is running on namenode. This addresses the \"jack-of-all-trades\" problem with Hadoop V1 namenode. The task trackers in v1 are replaced by node managers. Each worker node has a node manager running. The node manager manages the resource and task status for the worker node. The fixed number of mapper slots and reducer slots are replaced by a container in the worker node, which has the flexibility of running any task (mapper, reducer, application master, and etc), scheduled by the scheduler. Yarn Job submission work flow When a client needs to submit an application to YARN, the following steps are taken. Client submits an application Application Manager (RM) allocates a container to start Application Master Application Master registers with Resource Manager Application Master asks containers from Resource Manager Application Master notifies Node Manager to launch containers Application code is executed in the container Client contacts Application Master to monitor application status Application Master unregisters with Resource Manager YARN Scheduler Next we consider how YARN schedule jobs. Job scheduling is tough multiple objective optimization problem. It often needs to meet all or most of the following requirement It needs to offer the capacity guarantee to the cluster users It needs to fulfill the service level agreement, if the cluster is in subscription model It needs to be fairn. It needs to ensure the utilization level of the cluster ... Things get trickier when the applicaiton masters are allowed to request for resource up-front statically or dynamically during the execution. YARN is shipped with a few scheduler templates, namely * FIFO - Single queue, first in first out * Capacity - optimize for capacity and resources by different queues * FairScheduler - similar to Capacity, but allows applications to move between queues during execution Besides these, users are allowed to define their own scheduler policy for their own clusters. However to ensure faireness in general is a hard problem too. One of the popular approach is to define a standard metric for multi-resource request for all the incoming application so that an order can be fixed. For instance Dominant Resource Fairness. https://cs.stanford.edu/~matei/papers/2011/nsdi_drf.pdf","title":"50.043 Yarn"},{"location":"notes/l12_yarn/#50043-yarn","text":"","title":"50.043 Yarn"},{"location":"notes/l12_yarn/#learning-outcomes","text":"By the end of this lesson, you are able to Articulate the purpose of Hadoop YARN Identify the shortcoming of JobTracker and TaskTracker in Hadoop V1 Describe how YARN addresses the above issues List and differentiate the different YARN schedulers","title":"Learning Outcomes"},{"location":"notes/l12_yarn/#hadoop-and-eco-system","text":"We have many tools in the big data tool box Hadoop HDFS - Main Storage Hadoop MapReduce - Low CPU / Low RAM consumption batch job Spark Job - High CPU / GPU / RAM batch job Spark Stream - Realtime stream ... Spark and Hadoop v1 have incompatible resource managers, they don't understand each other. It is crictical to have a unified resource management tool for all kinds of jobs.","title":"Hadoop and Eco System"},{"location":"notes/l12_yarn/#hadoop-v1-resource-management-system","text":"Hadoop v1 Resource Management System has many issues. The namenode acts as the master node and the job tracker. It is overloaded with * Resource Management * Scheduling * Job monitoring * Job lifecycle * Fault-tolerence It soon becomes the bottle neck of the system. It does not scale more than 4000 nodes and 40,000 tasks. The datanodes acts as the taskers. They have static task slots, for instance, 2 slots for mapper task and 2 slots for reducer task. The mapper slots cannot be used for reducer tasks. This leads to additional constraints in task schedules and limitation for load balancing.","title":"Hadoop v1 Resource Management System"},{"location":"notes/l12_yarn/#yarn","text":"YARN is a general resource management system shipped with Hadoop V2 on wards. It is designed with Hadoop and its eco system. It overcomes most of the limitations found in Hadoop V1. It provides an extensible APIs for non-Hadoop tasks.","title":"YARN"},{"location":"notes/l12_yarn/#hadoop-v1-vs-yarn","text":"In the following diagram we find the correspondence between Hadoop V1 resource manager and YARN {width=80%} On the topmost leve, the JobTracker is replaced by an application master which in charge of managing an application running on the cluster. The Application master it is not running on the namenode, instead it runs in one of the task slots in a worker node. The resource managemer which coordinates among the hardware resources and many application masters and node managers, is running on namenode. This addresses the \"jack-of-all-trades\" problem with Hadoop V1 namenode. The task trackers in v1 are replaced by node managers. Each worker node has a node manager running. The node manager manages the resource and task status for the worker node. The fixed number of mapper slots and reducer slots are replaced by a container in the worker node, which has the flexibility of running any task (mapper, reducer, application master, and etc), scheduled by the scheduler.","title":"Hadoop V1 vs YARN"},{"location":"notes/l12_yarn/#yarn-job-submission-work-flow","text":"When a client needs to submit an application to YARN, the following steps are taken. Client submits an application Application Manager (RM) allocates a container to start Application Master Application Master registers with Resource Manager Application Master asks containers from Resource Manager Application Master notifies Node Manager to launch containers Application code is executed in the container Client contacts Application Master to monitor application status Application Master unregisters with Resource Manager","title":"Yarn Job submission work flow"},{"location":"notes/l12_yarn/#yarn-scheduler","text":"Next we consider how YARN schedule jobs. Job scheduling is tough multiple objective optimization problem. It often needs to meet all or most of the following requirement It needs to offer the capacity guarantee to the cluster users It needs to fulfill the service level agreement, if the cluster is in subscription model It needs to be fairn. It needs to ensure the utilization level of the cluster ... Things get trickier when the applicaiton masters are allowed to request for resource up-front statically or dynamically during the execution. YARN is shipped with a few scheduler templates, namely * FIFO - Single queue, first in first out * Capacity - optimize for capacity and resources by different queues * FairScheduler - similar to Capacity, but allows applications to move between queues during execution Besides these, users are allowed to define their own scheduler policy for their own clusters. However to ensure faireness in general is a hard problem too. One of the popular approach is to define a standard metric for multi-resource request for all the incoming application so that an order can be fixed. For instance Dominant Resource Fairness. https://cs.stanford.edu/~matei/papers/2011/nsdi_drf.pdf","title":"YARN Scheduler"},{"location":"notes/l1_course_handout/","text":"50.043 Database Systems and Big Data Course Handout This page will be updated regularly. Sync up often. Course Description Database systems manage data which is at the heart of modern computing applications. This course covers the fundamentals of traditional databases, such as Oracle and MySQL, and core ideas of recent big data systems. Students will learn important problems in data management that these systems are designed to solve. They will experience the internal design and implementation of relational databases. They will also understand the internals of state\u2010of\u2010the\u2010art big data platforms, namely Apache Spark, and use them on Amazon cloud (Amazon Web Service). The students will be able to determine the advantages and limitations of different database systems. Resource The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics. Abraham Siberschatz, Henry Korth, S Sudarshan. Database System Concepts. 6th edition. (DSC) Raghu Ramakrishnan, Johannes Gehrke. Database management systems. 3rd edition (DBM) Hector Garcia-Molina, Jeffrey D. Ullman, Jennifer Widom. Database systems, the complete book. 2nd edition. (DS) Instructors Dorien Herremans (dorien_herremans@sutd.edu.sg) Office Hour: Kenny Lu (kenny_lu@sutd.edu.sg) Office Hour: Monday 3:00-4:30pm (please send email to arrange) TAs Tran Ly Tu (Fiona) Le (tranlytu_le@mymail.sutd.edu.sg) Xiao Du (xiao_du@mymail.sutd.edu.sg) Communication If you have course/assignment/project related questions, please post it on the dedicated MS teams channel. Grading Your final grade is computed as follows: Homework: 12% There will be 2 homework assignments, 6 points each. Project: 60% Group project, up to 3 per group. Unless notifying the instructors otherwise, all group members have the same grade for the project. Class participation: 3% Ask/answer questions during classes, spot mistakes, etc. Final: 25% Things you need to prepare If you are using Windows 10 or Windows 11, please install ubuntu subsystems Win10 Win11 If you are using Linux, it should be perfect. If you are using Mac, please install homebrew . Make sure Java >8 is installed and ant is installed. Ubuntu: sudo apt install ant ant-contrib Mac: brew install ant ant-contrib When you have the AWS educate invitaiton email. Self-study Cohort Class 0 . Project Please refer to the project page . Submission Policy and Plagiarism You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else. You will not post any solutions related to this course to a private/public repository that is accessible by the public/others. Students are allowed to have a private repository for their assignment which no one can access. For projects, students can only invite their partners as collaborators to a private repository. Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work. Schedule (21 Jan 2024 - 29 Apr 2024) Week Lecture Cohort Reference Remarks 0 (14/1) AWS academy (AWS edu) setup 1 (21/1) Intro , ER Model ER Model DBM: Chapter 1-2, DSC: Chapter 7 2 (28/1) Relational Model , Relational Algebra Relational Model, Relational Algebra DBM: Chapter 3-4, DSC: Chapter 2 & 6 Project Team Submission (4/2 23:59) 3 (4/2) SQL , NoSQL SQL DBM: Chapter 4-5, DSC: Chapter 2-4 4 (11/2) Functional Dependency , Normal Forms Functional Dependency, Normal Forms DBM: Chapter 19, DSC: Chapter 8 Monday is a public holiday. Lecture 1 is pre-recorded. Assignment 1 Submission (18/2 23:59) 5 (18/2) Storage , Index Strorage, Index DBM: Chapter 19, DSC: Chapter 8 6 (25/2) Query Operations Query Operations DBM: Chapter 12-14, DSC: Chapter 12 Project Lab 1 Submission (3/3 23:59) 7 (3/3) Recess Week Recess Week Self-study flintrock and spark cluster setup (edimension video tutorial) 8 (10/3) Query Optimization Query Optimization DBM: Chapter 15 , DSC: Chapter 13 Project Lab 2 Submission (17/3 23:59) 9 (17/3) Transaction Recovery and Concurrency Transactions DBM: Chapter 16-18, DSC: Chapter 14-16 10 (24/3) HDFS , MapReduce HDFS, MapReduce Friday is a public Holiday. Students from CC2 please try to join the other two sessions (Thu 1:30 at TT9 or Thu 4:00 at CC14) 11 (31/3) Spark Spark Project Lab 3 Submission (7/4 23:59) 12 (7/4) Yarn Spark 2 Assignment 2 Submission (14/4 23:59) 13 (14/4) Consultation Guest Lecture Project Lab 4 Submission (21/4 23:59) 14 (21/4) Make Up and Alternative Assessment Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test.","title":"Handout"},{"location":"notes/l1_course_handout/#50043-database-systems-and-big-data-course-handout","text":"","title":"50.043 Database Systems and Big Data  Course Handout"},{"location":"notes/l1_course_handout/#this-page-will-be-updated-regularly-sync-up-often","text":"","title":"This page will be updated regularly. Sync up often."},{"location":"notes/l1_course_handout/#course-description","text":"Database systems manage data which is at the heart of modern computing applications. This course covers the fundamentals of traditional databases, such as Oracle and MySQL, and core ideas of recent big data systems. Students will learn important problems in data management that these systems are designed to solve. They will experience the internal design and implementation of relational databases. They will also understand the internals of state\u2010of\u2010the\u2010art big data platforms, namely Apache Spark, and use them on Amazon cloud (Amazon Web Service). The students will be able to determine the advantages and limitations of different database systems.","title":"Course Description"},{"location":"notes/l1_course_handout/#resource","text":"The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics. Abraham Siberschatz, Henry Korth, S Sudarshan. Database System Concepts. 6th edition. (DSC) Raghu Ramakrishnan, Johannes Gehrke. Database management systems. 3rd edition (DBM) Hector Garcia-Molina, Jeffrey D. Ullman, Jennifer Widom. Database systems, the complete book. 2nd edition. (DS)","title":"Resource"},{"location":"notes/l1_course_handout/#instructors","text":"Dorien Herremans (dorien_herremans@sutd.edu.sg) Office Hour: Kenny Lu (kenny_lu@sutd.edu.sg) Office Hour: Monday 3:00-4:30pm (please send email to arrange)","title":"Instructors"},{"location":"notes/l1_course_handout/#tas","text":"Tran Ly Tu (Fiona) Le (tranlytu_le@mymail.sutd.edu.sg) Xiao Du (xiao_du@mymail.sutd.edu.sg)","title":"TAs"},{"location":"notes/l1_course_handout/#communication","text":"If you have course/assignment/project related questions, please post it on the dedicated MS teams channel.","title":"Communication"},{"location":"notes/l1_course_handout/#grading","text":"Your final grade is computed as follows: Homework: 12% There will be 2 homework assignments, 6 points each. Project: 60% Group project, up to 3 per group. Unless notifying the instructors otherwise, all group members have the same grade for the project. Class participation: 3% Ask/answer questions during classes, spot mistakes, etc. Final: 25%","title":"Grading"},{"location":"notes/l1_course_handout/#things-you-need-to-prepare","text":"If you are using Windows 10 or Windows 11, please install ubuntu subsystems Win10 Win11 If you are using Linux, it should be perfect. If you are using Mac, please install homebrew . Make sure Java >8 is installed and ant is installed. Ubuntu: sudo apt install ant ant-contrib Mac: brew install ant ant-contrib When you have the AWS educate invitaiton email. Self-study Cohort Class 0 .","title":"Things you need to prepare"},{"location":"notes/l1_course_handout/#project","text":"Please refer to the project page .","title":"Project"},{"location":"notes/l1_course_handout/#submission-policy-and-plagiarism","text":"You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else. You will not post any solutions related to this course to a private/public repository that is accessible by the public/others. Students are allowed to have a private repository for their assignment which no one can access. For projects, students can only invite their partners as collaborators to a private repository. Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work.","title":"Submission Policy and Plagiarism"},{"location":"notes/l1_course_handout/#schedule-21-jan-2024-29-apr-2024","text":"Week Lecture Cohort Reference Remarks 0 (14/1) AWS academy (AWS edu) setup 1 (21/1) Intro , ER Model ER Model DBM: Chapter 1-2, DSC: Chapter 7 2 (28/1) Relational Model , Relational Algebra Relational Model, Relational Algebra DBM: Chapter 3-4, DSC: Chapter 2 & 6 Project Team Submission (4/2 23:59) 3 (4/2) SQL , NoSQL SQL DBM: Chapter 4-5, DSC: Chapter 2-4 4 (11/2) Functional Dependency , Normal Forms Functional Dependency, Normal Forms DBM: Chapter 19, DSC: Chapter 8 Monday is a public holiday. Lecture 1 is pre-recorded. Assignment 1 Submission (18/2 23:59) 5 (18/2) Storage , Index Strorage, Index DBM: Chapter 19, DSC: Chapter 8 6 (25/2) Query Operations Query Operations DBM: Chapter 12-14, DSC: Chapter 12 Project Lab 1 Submission (3/3 23:59) 7 (3/3) Recess Week Recess Week Self-study flintrock and spark cluster setup (edimension video tutorial) 8 (10/3) Query Optimization Query Optimization DBM: Chapter 15 , DSC: Chapter 13 Project Lab 2 Submission (17/3 23:59) 9 (17/3) Transaction Recovery and Concurrency Transactions DBM: Chapter 16-18, DSC: Chapter 14-16 10 (24/3) HDFS , MapReduce HDFS, MapReduce Friday is a public Holiday. Students from CC2 please try to join the other two sessions (Thu 1:30 at TT9 or Thu 4:00 at CC14) 11 (31/3) Spark Spark Project Lab 3 Submission (7/4 23:59) 12 (7/4) Yarn Spark 2 Assignment 2 Submission (14/4 23:59) 13 (14/4) Consultation Guest Lecture Project Lab 4 Submission (21/4 23:59) 14 (21/4)","title":"Schedule (21 Jan 2024 - 29 Apr 2024)"},{"location":"notes/l1_course_handout/#make-up-and-alternative-assessment","text":"Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test.","title":"Make Up and Alternative Assessment"},{"location":"notes/l1_er/","text":"50.043 Entity Relationship Model Learning Outcomes By this end of this unit, you should be able to Identify components of an Entity Relationship Diagram Interpret the design requirements given an ER diagram Draw ER diagrams based on user requirements Components of an ER Diagram In the most common scenario, an Entity Relationship Model is described in a form a diagram, AKA ER Diagram. An ER Diagram may consists of some of the following Entity set. An entity set captures a set of objects or items to be stored. It is represented as a rectangular box with the entity name inside. Attribute. An attribute describe a property of an entity. An attribute is represented as an oval shape with the attribute name inside. Attribute serves as (part of) the primary key of the entity will be underlined. Relationship. A relationship defines the relationship between entities. It is represented as a diamond shape with the relationship name inside. Relationships are often annotated with cardinality constraints. We will discuss it shortly. For example in the above ER diagram, we find two entities, i.e. Student and Class . Each Student entity has three attributes, NRIC , Name and DoB , with NRIC as the primary key. Likewise, each Class entity has three attributes, Number (as primary key), Name and Location . There exists a binary relationship between Student and Class entities, Enrolled in . It describes a business constraints that students can be enrolled into classes. The annotation N describes that each class may have more than 1 students. The annotation M defines that each student may take more than one modules. Note that in this module, for simplicity, we only require the upper bound to be specified in cardinality constraints, which could be either 1 or N (or M ). Note that N and M are just meta terms to represent \"many\". Self-referencing relationship There is an edge case in which a relationship may be self-referencing w.r.t to an entity. In the above ER diagram, we omit the attributes of the Article entity for simplicity. Each article can be referenced by many other articles. Each article can reference many other articles. The extra annotation Citing and Cited label the role of the article involved in the relationship. Tertiary relationship There are situations in which a relationship involves more than two entities. In the above ER diagram, the publish relationship involves three entities, Article , Book and Publisher . The cardinality constraints should be interpreted by pairing up two source entities and one target entity. In the above example, we have Given an article and a book, they can only be published by 1 publisher. Given an article and a publisher, the article can only appeared one book (published by that publisher). Given a book and a publisher, the book may contain many different articles.","title":"50.043 Entity Relationship Model"},{"location":"notes/l1_er/#50043-entity-relationship-model","text":"","title":"50.043 Entity Relationship Model"},{"location":"notes/l1_er/#learning-outcomes","text":"By this end of this unit, you should be able to Identify components of an Entity Relationship Diagram Interpret the design requirements given an ER diagram Draw ER diagrams based on user requirements","title":"Learning Outcomes"},{"location":"notes/l1_er/#components-of-an-er-diagram","text":"In the most common scenario, an Entity Relationship Model is described in a form a diagram, AKA ER Diagram. An ER Diagram may consists of some of the following Entity set. An entity set captures a set of objects or items to be stored. It is represented as a rectangular box with the entity name inside. Attribute. An attribute describe a property of an entity. An attribute is represented as an oval shape with the attribute name inside. Attribute serves as (part of) the primary key of the entity will be underlined. Relationship. A relationship defines the relationship between entities. It is represented as a diamond shape with the relationship name inside. Relationships are often annotated with cardinality constraints. We will discuss it shortly. For example in the above ER diagram, we find two entities, i.e. Student and Class . Each Student entity has three attributes, NRIC , Name and DoB , with NRIC as the primary key. Likewise, each Class entity has three attributes, Number (as primary key), Name and Location . There exists a binary relationship between Student and Class entities, Enrolled in . It describes a business constraints that students can be enrolled into classes. The annotation N describes that each class may have more than 1 students. The annotation M defines that each student may take more than one modules. Note that in this module, for simplicity, we only require the upper bound to be specified in cardinality constraints, which could be either 1 or N (or M ). Note that N and M are just meta terms to represent \"many\".","title":"Components of an ER Diagram"},{"location":"notes/l1_er/#self-referencing-relationship","text":"There is an edge case in which a relationship may be self-referencing w.r.t to an entity. In the above ER diagram, we omit the attributes of the Article entity for simplicity. Each article can be referenced by many other articles. Each article can reference many other articles. The extra annotation Citing and Cited label the role of the article involved in the relationship.","title":"Self-referencing relationship"},{"location":"notes/l1_er/#tertiary-relationship","text":"There are situations in which a relationship involves more than two entities. In the above ER diagram, the publish relationship involves three entities, Article , Book and Publisher . The cardinality constraints should be interpreted by pairing up two source entities and one target entity. In the above example, we have Given an article and a book, they can only be published by 1 publisher. Given an article and a publisher, the article can only appeared one book (published by that publisher). Given a book and a publisher, the book may contain many different articles.","title":"Tertiary relationship"},{"location":"notes/l1_intro/","text":"50.043 Introduction to Database System Learning Outcomes By the end of this unit, you should be able to List the problems handled by database management systems\u200b Describe the techniques used in database system to solve these problems\u200b \u200b What is a database? Generally speaking, a database is an organized collection of data. For instance, your excel sheet that keep tracks of your monthly finance. a text file that stores the list of items you want to buy for christmas or CNY. a collection of student records in an LMS. a collection of product information and stock level for a minimart chain. What is a database management system? It is a system (hopefully a software system) that manages databases. Many DBMS orchestrates and manages multiple databases simultanenously. For instance, Oracle, MySQL, MS SQL, MongoDB, Firebase, Amazon RDs and etc. It is confusing when people use to the term \"database\" to refer to a DBMS. Characteristics of a database management system For a DBMS to be pragmatically useful, here are some of the characteristics that it may possess It should be efficient. Queries and data operation should be evaluated and performed in an efficient way. It should be crash consistent. In the event of unexpected crash and system distruption, the data stored should remain consistent. It should support concurrent access. There are many users and softwares that could access the data without interfering each other. It should support data abstraction. Data and their relationships are allowed to be described in a logical manner without the users / software to be exposed with the detail implementation. Common Techniques available in database management systems Storage and Index. DBMSes have their own internal storage system. It often differs from those available in the operating system, due to use case difference. Internal storage system enables DBMSes to organize and retrieve data in a systematic and efficient way. Indices are inspired from data structure and algorithms. With indices, data queries and operation can be further optimized. Transaction. Many DBMSes support transactional operations. Trasaction groups multiple operations to be performed as an atomic sequence of actions to be performed. Conflicts caused by concurrent access can be resolved by studying and manipulating multiple transactions. Transactions are logged, so that in the event of system crashes, partial data changes can be reverted back to the nearest consistent state. Data Model and SQL. Many tranditional DBMSes support data model and SQL. This combination is a simple and expressive data abstraction for many applications. Softwares developed based on an existing database (system) could be possible migrate to another with marginal technical effort. Recently many big data systems extend their support to these techniques and it proves the essentiality of them. Overview of Data Modelling For the first part of this module, we dwell into the details of data modelling. In the upper diagram, we describe the user interaction with the database as blackbox. In the lower diagram, we define the processes of data modelling. Conceptual Model - in this step, we try to capture the user needs from user study, and identify what data the application has and need. Logical Model - in this step, supported by the user needs identified, we study how data are been accessed, updated and maintained from the business perspective. We design a high level structure of the data storage and the set of possible constraints over the data to enforce varies integrity derived from the business logics. Physical Model - in this step, we translate the logical model into a specific database (supported by the database management system that has been deployed).","title":"50.043 Introduction to Database System"},{"location":"notes/l1_intro/#50043-introduction-to-database-system","text":"","title":"50.043 Introduction to Database System"},{"location":"notes/l1_intro/#learning-outcomes","text":"By the end of this unit, you should be able to List the problems handled by database management systems\u200b Describe the techniques used in database system to solve these problems\u200b \u200b","title":"Learning Outcomes"},{"location":"notes/l1_intro/#what-is-a-database","text":"Generally speaking, a database is an organized collection of data. For instance, your excel sheet that keep tracks of your monthly finance. a text file that stores the list of items you want to buy for christmas or CNY. a collection of student records in an LMS. a collection of product information and stock level for a minimart chain.","title":"What is a database?"},{"location":"notes/l1_intro/#what-is-a-database-management-system","text":"It is a system (hopefully a software system) that manages databases. Many DBMS orchestrates and manages multiple databases simultanenously. For instance, Oracle, MySQL, MS SQL, MongoDB, Firebase, Amazon RDs and etc. It is confusing when people use to the term \"database\" to refer to a DBMS.","title":"What is a database management system?"},{"location":"notes/l1_intro/#characteristics-of-a-database-management-system","text":"For a DBMS to be pragmatically useful, here are some of the characteristics that it may possess It should be efficient. Queries and data operation should be evaluated and performed in an efficient way. It should be crash consistent. In the event of unexpected crash and system distruption, the data stored should remain consistent. It should support concurrent access. There are many users and softwares that could access the data without interfering each other. It should support data abstraction. Data and their relationships are allowed to be described in a logical manner without the users / software to be exposed with the detail implementation.","title":"Characteristics of a database management system"},{"location":"notes/l1_intro/#common-techniques-available-in-database-management-systems","text":"Storage and Index. DBMSes have their own internal storage system. It often differs from those available in the operating system, due to use case difference. Internal storage system enables DBMSes to organize and retrieve data in a systematic and efficient way. Indices are inspired from data structure and algorithms. With indices, data queries and operation can be further optimized. Transaction. Many DBMSes support transactional operations. Trasaction groups multiple operations to be performed as an atomic sequence of actions to be performed. Conflicts caused by concurrent access can be resolved by studying and manipulating multiple transactions. Transactions are logged, so that in the event of system crashes, partial data changes can be reverted back to the nearest consistent state. Data Model and SQL. Many tranditional DBMSes support data model and SQL. This combination is a simple and expressive data abstraction for many applications. Softwares developed based on an existing database (system) could be possible migrate to another with marginal technical effort. Recently many big data systems extend their support to these techniques and it proves the essentiality of them.","title":"Common Techniques available in database management systems"},{"location":"notes/l1_intro/#overview-of-data-modelling","text":"For the first part of this module, we dwell into the details of data modelling. In the upper diagram, we describe the user interaction with the database as blackbox. In the lower diagram, we define the processes of data modelling. Conceptual Model - in this step, we try to capture the user needs from user study, and identify what data the application has and need. Logical Model - in this step, supported by the user needs identified, we study how data are been accessed, updated and maintained from the business perspective. We design a high level structure of the data storage and the set of possible constraints over the data to enforce varies integrity derived from the business logics. Physical Model - in this step, we translate the logical model into a specific database (supported by the database management system that has been deployed).","title":"Overview of Data Modelling"},{"location":"notes/l2_relational_algebra/","text":"50.043 - Relational Algebra Learning Outcomes By the end of this unit, you should be able to Interpret relational algebra terms (queries) Define relational algebra terms to query a relational model Relational Algebra Given that relational model is a logical model (abstracting away the implementation details), we need something operations defined on the same layout to manipulate the data in a relational model. (So that we don't need to deal with the implementation details yet.) Like math algebra, Relational Algebra is a way to express data operation through symbols and relation manipulations. The data manipulation operations are defined in terms of a sequence of operation applications. Each symbolic operator takes one or more relation(s) (or intermediate relations) as operands and produces one result relation. For example, given a relation of Publish( article_id, book_id, publisher_id ) The instance of a Publish relation is given article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 The query \"finding all articles that are published by both publishers p1 and p2\" can be expressed as the following relational algebra term. \\[ \\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p1')}(Publish)) \\cap \\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p2')}(Publish)) \\] Selection A selection operator, \\(\\sigma_{P}(R)\\) , takes a logical predicate \\(P\\) and a relation \\(R\\) and returns all the tuple in \\(R\\) that satisfy \\(P\\) . Note that \\(P\\) can be a simple predicate such as a equality test $name = \"tom\" or a conjunction or disjunction of predicates, e.g. \\(name = \"tom\"\\ {\\tt AND} \\ age > 21\\) . For example, the following relational algebra expression returns a relation with all tuples from \\(Publish\\) with \\(publisher\\_id\\) equals to p1 . \\[\\sigma_{(publisher\\_id='p1')}(Publish)\\] Projection A projection operator \\(\\Pi_{A_1,A_2,...}(R)\\) , takes a set of attribute names \\(A_1,...,A_n\\) and a relation \\(R\\) , and returns a relation that contains the data of columns \\(A_1,...,A_n\\) in \\(R\\) . For example, the following expression returns a relation of all \\(article\\_id\\) s from \\(Publish\\) with \\(publisher\\_id\\) equals to p1 . \\[\\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p1')}(Publish))\\] Intersection An intersection operation \\(R \\cap S\\) finds all common tuples from \\(R\\) and \\(S\\) , assuming \\(R\\) and \\(S\\) sharing a common schema. We have seen an example of this earlier. Union A union operation \\(R \\cup S\\) returns a relation containing all tuples from \\(R\\) and all tuples from \\(S\\) , assuming \\(R\\) and \\(S\\) sharing a common schema. Difference A difference operation \\(R - S\\) returns a relation containing all tuples from \\(R\\) that are not in \\(S\\) , assuming \\(R\\) and \\(S\\) sharing a common schema. Cartesian Product A cartesian product operation \\(R \\times S\\) returns a relation containing all possible combination of some tuple from \\(R\\) and some tuple from \\(S\\) . (Note that \\(R\\) and \\(S\\) might have different schema.) For example, consider \\(R(A,B)\\) and \\(S(C,D)\\) A B a1 101 a2 102 C D a3 103 a4 104 \\(R\\times S\\) yields R.A R.B S.C S.D a1 101 a3 103 a1 101 a4 104 a2 102 a3 103 a2 102 a4 104 Cartesian Product is one of the four possible join operators. Let's discuss the other three. Inner Join The inner join operator \\((R \\bowtie_{R.A = S.B, R.C=S.D,...} S)\\) , returns a relation that containing tuples from \\(R\\times S\\) that satisfy \\(R.A = S.B, R.C=S.D,...\\) . Let \\(R(A,B,C)\\) and \\(S(D,E,F)\\) be relations. A B C a1 101 0 a2 102 1 a3 103 0 D E F a3 103 'a' a1 107 'b' a5 105 'c' \\(R \\bowtie_{R.A =S.D} S\\) produces R.A R.B R.C S.D S.E S.F a1 101 0 a1 107 'b' a3 103 0 a3 103 'a' Natural Join The natural join operator \\((R \\bowtie S)\\) , returns a relation that containing tuples from \\(R\\times S\\) that satisfy \\(R.A = S.A, R.B=S.B,...\\) , where \\(A\\) , \\(B\\) , ... are the common attributes between \\(R\\) and \\(S\\) . Note that the common column are merged after natural join. Let \\(R(A,B,C)\\) and \\(S(D,E,F)\\) be relations. A B C a1 101 0 a2 102 1 a3 103 0 A E F a3 103 'a' a1 107 'b' a5 105 'c' \\(R \\bowtie S\\) produces R.A R.B R.C S.E S.F a1 101 0 107 'b' a3 103 0 103 'a' Right outer join Right outer join \\(R \u27d6_{R.A=S.B} S\\) produces a relation containing all entries from the inner join and all the remaining tupel from \\(S\\) which we could not find a match from \\(R\\) , whose attributes will be filled up with NULL. A B C a1 101 0 a2 102 1 a3 103 0 D E F a3 103 'a' a1 107 'b' a5 105 'c' \\(R \u27d6_{R.A =S.D} S\\) produces R.A R.B R.C S.D S.E S.F a1 101 0 a1 107 'b' a3 103 0 a3 103 'a' NULL NULL NULL a5 105 'c' Likewise for left outer join. Renaming Renaming operation \\(\\rho_{R'(A_1,...,A_n)}(R)\\) produces a new relation \\(R'(A_1,...,A_n)\\) containing all tuples from \\(R\\) , assuming the \\(S_R\\) and \\(S_{R'}\\) share the same types. We omit the attribute name \\(A_1,...,A_n\\) when we only want to rename the relation name. Likewise we omit the relation name if we only want to rename the attributes. Aggregation Aggregation operation \\(A\\) 1 ,..., \\(A\\) n \\(\\gamma_{F_1(B_1),F_2(B_2),...} (R))\\) produces a new relation \\(R'\\) which contains \\(A_1,...,A_n\\) as the attribute names to group by \\(F_1(B_1),...,F_m(B_m)\\) as the aggregated values where \\(F_1, ..., F_m\\) are aggregation functions such as SUM() , AVG() , MIN() , MAX() , COUNT() . \\(A_1, ..., A_n\\) , \\(B_1, ..., B_m\\) are attributes from \\(R\\) . For example, given \\(R(A,B,C)\\) A B C a1 101 0 a2 102 1 a3 103 0 \\(\\rho_{(C,CNT)}(\\) \\(C\\) \\(\\gamma_{{\\tt COUNT}(B)}(R))\\) produces C CNT 0 2 1 1 Sometimes we can rewrite the above expression as \\[_{C} \\gamma_{{\\tt COUNT}(B)\\ {\\tt as}\\ CNT}(R)\\] without using the renaming operator. Alternatives Alternative to relational algebra, relational calculus is designed to serve a similiar idea. The difference between them is that relational algebra is more procedural (like C, Java and Python) where relational calculus is more declarative (like CSS and SQL). If you are interested to find out more please refer to the text books. Note that relational calculus will not be assessed in this module.","title":"50.043 -  Relational Algebra"},{"location":"notes/l2_relational_algebra/#50043-relational-algebra","text":"","title":"50.043 -  Relational Algebra"},{"location":"notes/l2_relational_algebra/#learning-outcomes","text":"By the end of this unit, you should be able to Interpret relational algebra terms (queries) Define relational algebra terms to query a relational model","title":"Learning Outcomes"},{"location":"notes/l2_relational_algebra/#relational-algebra","text":"Given that relational model is a logical model (abstracting away the implementation details), we need something operations defined on the same layout to manipulate the data in a relational model. (So that we don't need to deal with the implementation details yet.) Like math algebra, Relational Algebra is a way to express data operation through symbols and relation manipulations. The data manipulation operations are defined in terms of a sequence of operation applications. Each symbolic operator takes one or more relation(s) (or intermediate relations) as operands and produces one result relation. For example, given a relation of Publish( article_id, book_id, publisher_id ) The instance of a Publish relation is given article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 The query \"finding all articles that are published by both publishers p1 and p2\" can be expressed as the following relational algebra term. \\[ \\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p1')}(Publish)) \\cap \\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p2')}(Publish)) \\]","title":"Relational Algebra"},{"location":"notes/l2_relational_algebra/#selection","text":"A selection operator, \\(\\sigma_{P}(R)\\) , takes a logical predicate \\(P\\) and a relation \\(R\\) and returns all the tuple in \\(R\\) that satisfy \\(P\\) . Note that \\(P\\) can be a simple predicate such as a equality test $name = \"tom\" or a conjunction or disjunction of predicates, e.g. \\(name = \"tom\"\\ {\\tt AND} \\ age > 21\\) . For example, the following relational algebra expression returns a relation with all tuples from \\(Publish\\) with \\(publisher\\_id\\) equals to p1 . \\[\\sigma_{(publisher\\_id='p1')}(Publish)\\]","title":"Selection"},{"location":"notes/l2_relational_algebra/#projection","text":"A projection operator \\(\\Pi_{A_1,A_2,...}(R)\\) , takes a set of attribute names \\(A_1,...,A_n\\) and a relation \\(R\\) , and returns a relation that contains the data of columns \\(A_1,...,A_n\\) in \\(R\\) . For example, the following expression returns a relation of all \\(article\\_id\\) s from \\(Publish\\) with \\(publisher\\_id\\) equals to p1 . \\[\\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p1')}(Publish))\\]","title":"Projection"},{"location":"notes/l2_relational_algebra/#intersection","text":"An intersection operation \\(R \\cap S\\) finds all common tuples from \\(R\\) and \\(S\\) , assuming \\(R\\) and \\(S\\) sharing a common schema. We have seen an example of this earlier.","title":"Intersection"},{"location":"notes/l2_relational_algebra/#union","text":"A union operation \\(R \\cup S\\) returns a relation containing all tuples from \\(R\\) and all tuples from \\(S\\) , assuming \\(R\\) and \\(S\\) sharing a common schema.","title":"Union"},{"location":"notes/l2_relational_algebra/#difference","text":"A difference operation \\(R - S\\) returns a relation containing all tuples from \\(R\\) that are not in \\(S\\) , assuming \\(R\\) and \\(S\\) sharing a common schema.","title":"Difference"},{"location":"notes/l2_relational_algebra/#cartesian-product","text":"A cartesian product operation \\(R \\times S\\) returns a relation containing all possible combination of some tuple from \\(R\\) and some tuple from \\(S\\) . (Note that \\(R\\) and \\(S\\) might have different schema.) For example, consider \\(R(A,B)\\) and \\(S(C,D)\\) A B a1 101 a2 102 C D a3 103 a4 104 \\(R\\times S\\) yields R.A R.B S.C S.D a1 101 a3 103 a1 101 a4 104 a2 102 a3 103 a2 102 a4 104 Cartesian Product is one of the four possible join operators. Let's discuss the other three.","title":"Cartesian Product"},{"location":"notes/l2_relational_algebra/#inner-join","text":"The inner join operator \\((R \\bowtie_{R.A = S.B, R.C=S.D,...} S)\\) , returns a relation that containing tuples from \\(R\\times S\\) that satisfy \\(R.A = S.B, R.C=S.D,...\\) . Let \\(R(A,B,C)\\) and \\(S(D,E,F)\\) be relations. A B C a1 101 0 a2 102 1 a3 103 0 D E F a3 103 'a' a1 107 'b' a5 105 'c' \\(R \\bowtie_{R.A =S.D} S\\) produces R.A R.B R.C S.D S.E S.F a1 101 0 a1 107 'b' a3 103 0 a3 103 'a'","title":"Inner Join"},{"location":"notes/l2_relational_algebra/#natural-join","text":"The natural join operator \\((R \\bowtie S)\\) , returns a relation that containing tuples from \\(R\\times S\\) that satisfy \\(R.A = S.A, R.B=S.B,...\\) , where \\(A\\) , \\(B\\) , ... are the common attributes between \\(R\\) and \\(S\\) . Note that the common column are merged after natural join. Let \\(R(A,B,C)\\) and \\(S(D,E,F)\\) be relations. A B C a1 101 0 a2 102 1 a3 103 0 A E F a3 103 'a' a1 107 'b' a5 105 'c' \\(R \\bowtie S\\) produces R.A R.B R.C S.E S.F a1 101 0 107 'b' a3 103 0 103 'a'","title":"Natural Join"},{"location":"notes/l2_relational_algebra/#right-outer-join","text":"Right outer join \\(R \u27d6_{R.A=S.B} S\\) produces a relation containing all entries from the inner join and all the remaining tupel from \\(S\\) which we could not find a match from \\(R\\) , whose attributes will be filled up with NULL. A B C a1 101 0 a2 102 1 a3 103 0 D E F a3 103 'a' a1 107 'b' a5 105 'c' \\(R \u27d6_{R.A =S.D} S\\) produces R.A R.B R.C S.D S.E S.F a1 101 0 a1 107 'b' a3 103 0 a3 103 'a' NULL NULL NULL a5 105 'c' Likewise for left outer join.","title":"Right outer join"},{"location":"notes/l2_relational_algebra/#renaming","text":"Renaming operation \\(\\rho_{R'(A_1,...,A_n)}(R)\\) produces a new relation \\(R'(A_1,...,A_n)\\) containing all tuples from \\(R\\) , assuming the \\(S_R\\) and \\(S_{R'}\\) share the same types. We omit the attribute name \\(A_1,...,A_n\\) when we only want to rename the relation name. Likewise we omit the relation name if we only want to rename the attributes.","title":"Renaming"},{"location":"notes/l2_relational_algebra/#aggregation","text":"Aggregation operation \\(A\\) 1 ,..., \\(A\\) n \\(\\gamma_{F_1(B_1),F_2(B_2),...} (R))\\) produces a new relation \\(R'\\) which contains \\(A_1,...,A_n\\) as the attribute names to group by \\(F_1(B_1),...,F_m(B_m)\\) as the aggregated values where \\(F_1, ..., F_m\\) are aggregation functions such as SUM() , AVG() , MIN() , MAX() , COUNT() . \\(A_1, ..., A_n\\) , \\(B_1, ..., B_m\\) are attributes from \\(R\\) . For example, given \\(R(A,B,C)\\) A B C a1 101 0 a2 102 1 a3 103 0 \\(\\rho_{(C,CNT)}(\\) \\(C\\) \\(\\gamma_{{\\tt COUNT}(B)}(R))\\) produces C CNT 0 2 1 1 Sometimes we can rewrite the above expression as \\[_{C} \\gamma_{{\\tt COUNT}(B)\\ {\\tt as}\\ CNT}(R)\\] without using the renaming operator.","title":"Aggregation"},{"location":"notes/l2_relational_algebra/#alternatives","text":"Alternative to relational algebra, relational calculus is designed to serve a similiar idea. The difference between them is that relational algebra is more procedural (like C, Java and Python) where relational calculus is more declarative (like CSS and SQL). If you are interested to find out more please refer to the text books. Note that relational calculus will not be assessed in this module.","title":"Alternatives"},{"location":"notes/l2_relational_model/","text":"50.043 - Relational Model Learning Outcomes By the end of this unit, you should be able to Describe the components of a relational model Translate an ER model into a relational model Relational Model Recall the following diagram Relational Model is an instance of Logical Modelling. Unlike Conceptual Modelling, Logical Modelling focuses on how data are being stored and processed in the abstraction level. The implementation details can be decided at a latter stage. It is analogous to software development, we could decide the data structure and algorithms without thinking about how the code should be written. What are the other options? Relational Model is not the only logical model available. Alternatively we find graph model and key-value model are the other popular options. Gist of Relational Model Relational Model defines data in Relation . Each Relation is an unordered set containing relationship of attributes. Attributes in Relational Model are sometimes referred to as fields . Note that we should not confuse them with the relationship and attributes mentioned in ER model. Relational model is a logical model, describing how data are stored and queried. ER model is a conceptual model, describing what data the application have / need. Relation in Relational model is mathematical relation, while the relationship in ER model is the connection in business domain. Relation is an set of Tuple s. Each Tuple is a sequence of attribute values in the Relation . For instance Student Number Name Email DoB 1234 James james@istd 1/1/2000 5678 Vansesa vanessa@epd 2/4/1999 3093 David david@esd 3/7/2000 The above table is an instance of a Relation , of student profile. * The first row in the Schema of the Relation . The second rows onwards are tuples. * Each column defines an attribute (or a field). Formal Definitions Definition (Relation) Let \\(D_1\\) , ..., \\(D_n\\) be domains (of attribute values). We define a relation \\(R\\) as follows \\[ R \\subseteq D_1 \\times D_2 \\times ... \\times D_n \\] Definition (Tuple) Let \\(R\\) be a relation \\(R \\subseteq D_1 \\times D_2 \\times ... \\times D_n\\) . Then a tuple \\(t\\) is an element of \\(R\\) \\(t = (d_1,...,d_n) \\in R\\) where for all \\(i\\in [1,n]\\) we have \\(d_i \\in D_i\\) . Definition (Schema) Let \\(R\\) be a relation \\(R \\subseteq D_1 \\times D_2 \\times ... \\times D_n\\) . Then the schema of \\(R\\) , denoted as \\(S_R\\) is a mapping of attribute names to domains (i.e. \\(D_1, ..., D_n\\) ) For instance, recall the earlier example, the student profile relation \\(R \\subseteq (Int \\times String \\times String \\times Date)\\) . where \\(S_R = \\{Student Number : Int, Name : String, Email : String, DoB : Date \\}\\) . The tuples in the above example forms an instance of the student profile relation. \\[ I_R = \\left \\{ \\begin{array}{cccc} (1234,& James,& james@istd,& 1/1/2000 ), \\\\ (5678,& Vanessa,& vanessa@epd, & 2/4/1999), \\\\ (3093,& David, & david@esd, &3/7/2000) \\end{array} \\right \\} \\] Mapping Relational Model to Tables There are many ways of translating a relational model into a set of database tables (from logical model to physical model). One simplest way is to translate each relation into a table translate each domain into a table column, having the domain constraint being satisfied by the table column type. translate tuples of the relation instance into a row of a table. relational schema can be converted into table schema (directly derived from step 2). translate additional constraints into table constraints or foreign key constraints. ER to Relational translation Converting an ER model (conceptual) to a relation model (logical) is more involved. The translation is iteratively defined by the following rules. Rule 1: map an entity set to a relation and preserve its primary key and fields. Rule 2: translate a relationship to a relation by combining all keys from entity sets to make the new primary key for the result relation. the final composite primary key is determined by the cardinality constraints of the relationship. Rule 3: merge multiple relations with the same key into a single relation Pros: avoids data redundancy Cons: introduces NULL values Let's consider an example, recall the following ER diagram Let's assume each entity has a primary key id, and a non-primary-key attribute name. All fields are of type String. We apply the first rule to obtain the first 3 relations in the following, Article( id , name) Book( id ,name) Publisher( id , name) Publish( article_id, book_id, publisher_id ) The 4th relation is obtained by applying the second rule to translation the publish relationship. Note that by default we take all three ids to form a composite primary key. The cardinality constraint suggests that an article_id and a book_id can determine a publisher_id an article_id and a publisher_id can determine a book_id Hence we can reduce the primary key composition of 4. to be Publish( article_id, book_id , publisher_id); or Publish( article_id , book_id, publisher_id ) It is a common mistake to think of fixing article_id as a primary key would suffice. Note that the above ER diagram provides no cardinality information between Book and Publisher alone. And the following instance is a counter example to the argument article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 The Rule 3 is not applicable in this example, hence we are done.","title":"50.043 - Relational Model"},{"location":"notes/l2_relational_model/#50043-relational-model","text":"","title":"50.043 - Relational Model"},{"location":"notes/l2_relational_model/#learning-outcomes","text":"By the end of this unit, you should be able to Describe the components of a relational model Translate an ER model into a relational model","title":"Learning Outcomes"},{"location":"notes/l2_relational_model/#relational-model","text":"Recall the following diagram Relational Model is an instance of Logical Modelling. Unlike Conceptual Modelling, Logical Modelling focuses on how data are being stored and processed in the abstraction level. The implementation details can be decided at a latter stage. It is analogous to software development, we could decide the data structure and algorithms without thinking about how the code should be written.","title":"Relational Model"},{"location":"notes/l2_relational_model/#what-are-the-other-options","text":"Relational Model is not the only logical model available. Alternatively we find graph model and key-value model are the other popular options.","title":"What are the other options?"},{"location":"notes/l2_relational_model/#gist-of-relational-model","text":"Relational Model defines data in Relation . Each Relation is an unordered set containing relationship of attributes. Attributes in Relational Model are sometimes referred to as fields . Note that we should not confuse them with the relationship and attributes mentioned in ER model. Relational model is a logical model, describing how data are stored and queried. ER model is a conceptual model, describing what data the application have / need. Relation in Relational model is mathematical relation, while the relationship in ER model is the connection in business domain. Relation is an set of Tuple s. Each Tuple is a sequence of attribute values in the Relation . For instance Student Number Name Email DoB 1234 James james@istd 1/1/2000 5678 Vansesa vanessa@epd 2/4/1999 3093 David david@esd 3/7/2000 The above table is an instance of a Relation , of student profile. * The first row in the Schema of the Relation . The second rows onwards are tuples. * Each column defines an attribute (or a field).","title":"Gist of Relational Model"},{"location":"notes/l2_relational_model/#formal-definitions","text":"","title":"Formal Definitions"},{"location":"notes/l2_relational_model/#definition-relation","text":"Let \\(D_1\\) , ..., \\(D_n\\) be domains (of attribute values). We define a relation \\(R\\) as follows \\[ R \\subseteq D_1 \\times D_2 \\times ... \\times D_n \\]","title":"Definition (Relation)"},{"location":"notes/l2_relational_model/#definition-tuple","text":"Let \\(R\\) be a relation \\(R \\subseteq D_1 \\times D_2 \\times ... \\times D_n\\) . Then a tuple \\(t\\) is an element of \\(R\\) \\(t = (d_1,...,d_n) \\in R\\) where for all \\(i\\in [1,n]\\) we have \\(d_i \\in D_i\\) .","title":"Definition (Tuple)"},{"location":"notes/l2_relational_model/#definition-schema","text":"Let \\(R\\) be a relation \\(R \\subseteq D_1 \\times D_2 \\times ... \\times D_n\\) . Then the schema of \\(R\\) , denoted as \\(S_R\\) is a mapping of attribute names to domains (i.e. \\(D_1, ..., D_n\\) ) For instance, recall the earlier example, the student profile relation \\(R \\subseteq (Int \\times String \\times String \\times Date)\\) . where \\(S_R = \\{Student Number : Int, Name : String, Email : String, DoB : Date \\}\\) . The tuples in the above example forms an instance of the student profile relation. \\[ I_R = \\left \\{ \\begin{array}{cccc} (1234,& James,& james@istd,& 1/1/2000 ), \\\\ (5678,& Vanessa,& vanessa@epd, & 2/4/1999), \\\\ (3093,& David, & david@esd, &3/7/2000) \\end{array} \\right \\} \\]","title":"Definition (Schema)"},{"location":"notes/l2_relational_model/#mapping-relational-model-to-tables","text":"There are many ways of translating a relational model into a set of database tables (from logical model to physical model). One simplest way is to translate each relation into a table translate each domain into a table column, having the domain constraint being satisfied by the table column type. translate tuples of the relation instance into a row of a table. relational schema can be converted into table schema (directly derived from step 2). translate additional constraints into table constraints or foreign key constraints.","title":"Mapping Relational Model to Tables"},{"location":"notes/l2_relational_model/#er-to-relational-translation","text":"Converting an ER model (conceptual) to a relation model (logical) is more involved. The translation is iteratively defined by the following rules. Rule 1: map an entity set to a relation and preserve its primary key and fields. Rule 2: translate a relationship to a relation by combining all keys from entity sets to make the new primary key for the result relation. the final composite primary key is determined by the cardinality constraints of the relationship. Rule 3: merge multiple relations with the same key into a single relation Pros: avoids data redundancy Cons: introduces NULL values Let's consider an example, recall the following ER diagram Let's assume each entity has a primary key id, and a non-primary-key attribute name. All fields are of type String. We apply the first rule to obtain the first 3 relations in the following, Article( id , name) Book( id ,name) Publisher( id , name) Publish( article_id, book_id, publisher_id ) The 4th relation is obtained by applying the second rule to translation the publish relationship. Note that by default we take all three ids to form a composite primary key. The cardinality constraint suggests that an article_id and a book_id can determine a publisher_id an article_id and a publisher_id can determine a book_id Hence we can reduce the primary key composition of 4. to be Publish( article_id, book_id , publisher_id); or Publish( article_id , book_id, publisher_id ) It is a common mistake to think of fixing article_id as a primary key would suffice. Note that the above ER diagram provides no cardinality information between Book and Publisher alone. And the following instance is a counter example to the argument article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 The Rule 3 is not applicable in this example, hence we are done.","title":"ER to Relational translation"},{"location":"notes/l3_nosql/","text":"50.043 - NoSQL Learning Outcomes By the end of this unit, you should be able to articulate the trade-off made between relational model and other data model. contrast the different use-cases for SQL and NoSQL. List the pros and cons for different NoSQL-driven data storage. Motivation NoSQL existed long ago. It recently received more attention due to the shift of infrastruture from on-premise to cloud, from single server to distributed system. Recall from week 1 lesson, we learnt that database system should be efficient, crash consitent, supporting concurrent access, offering data abstraction. As we scale out to multiple machines, we have to give up some of the above. It is technically challenging to have a distributed system that is consistent. Another less alarming issue is that it is hard to perform efficient data join across machines (though there has been progress made). Due to various reasons, developers turn to the older alternative, key-value data structure. Key-value store Key-value store is simple. Many of us have seen the in-memory version, e.g. dictionary of Python, HashMap of Java, etc. The idea of key-value storage is to put data associated with a key in a look-up table. Keys must be unique, otherwise existing data will be overwritten. Storing data in key-value pairs offers some obvious advantages 1. easy to retrieve, like part of your code 2. fit in many applications, mobile app, web app. 3. fast, since it is just a lookup and/or set. 4. easy to scale, since we could split the data by key-space. However key-value pair storage has several limitations. 1. limited API, we can only get or set. There is no range query. 2. Join query is out of the question. 3. it does not differetiate the data type you store by default everything is binary. 4. it does not support inner structure in your data. 5. there is no immediate consistency gauranteed, i.e. sometimes your change takes a while to be reflected on the web app. Due to the above reasons, it pushes the processing load to the application-tier (which is your source code, in Python, Java or C etc.) Document store Unstructured Document In this category, the data store saves data in an unstructured manner. Treating everything as text or sequence of bytes. The advantage is it saves every thing, like your file system. The disadantage is that it offers no processing API. Semi-structured Document It is a tree-like structure. Data are stored like JSON/XML document. Some implementation offers limited support of indices and range search, e.g. MongoDB. The advantage is that semi-structured data are close to their in-memory counter-parts living in the software application, (binary tree, graph, JSON). It offers flexibility of going back and fro between database and app without the need of joining and object-relational mapping. Most of the semi-structured data are self-explanatory, respecting data types and data structure. One obvious disadvantage is that semi-strutured data do not eliminate data redundancy. Immediate consistency is handled loosely. Range queries could be harder to implememented.","title":"50.043 - NoSQL"},{"location":"notes/l3_nosql/#50043-nosql","text":"","title":"50.043 - NoSQL"},{"location":"notes/l3_nosql/#learning-outcomes","text":"By the end of this unit, you should be able to articulate the trade-off made between relational model and other data model. contrast the different use-cases for SQL and NoSQL. List the pros and cons for different NoSQL-driven data storage.","title":"Learning Outcomes"},{"location":"notes/l3_nosql/#motivation","text":"NoSQL existed long ago. It recently received more attention due to the shift of infrastruture from on-premise to cloud, from single server to distributed system. Recall from week 1 lesson, we learnt that database system should be efficient, crash consitent, supporting concurrent access, offering data abstraction. As we scale out to multiple machines, we have to give up some of the above. It is technically challenging to have a distributed system that is consistent. Another less alarming issue is that it is hard to perform efficient data join across machines (though there has been progress made). Due to various reasons, developers turn to the older alternative, key-value data structure.","title":"Motivation"},{"location":"notes/l3_nosql/#key-value-store","text":"Key-value store is simple. Many of us have seen the in-memory version, e.g. dictionary of Python, HashMap of Java, etc. The idea of key-value storage is to put data associated with a key in a look-up table. Keys must be unique, otherwise existing data will be overwritten. Storing data in key-value pairs offers some obvious advantages 1. easy to retrieve, like part of your code 2. fit in many applications, mobile app, web app. 3. fast, since it is just a lookup and/or set. 4. easy to scale, since we could split the data by key-space. However key-value pair storage has several limitations. 1. limited API, we can only get or set. There is no range query. 2. Join query is out of the question. 3. it does not differetiate the data type you store by default everything is binary. 4. it does not support inner structure in your data. 5. there is no immediate consistency gauranteed, i.e. sometimes your change takes a while to be reflected on the web app. Due to the above reasons, it pushes the processing load to the application-tier (which is your source code, in Python, Java or C etc.)","title":"Key-value store"},{"location":"notes/l3_nosql/#document-store","text":"","title":"Document store"},{"location":"notes/l3_nosql/#unstructured-document","text":"In this category, the data store saves data in an unstructured manner. Treating everything as text or sequence of bytes. The advantage is it saves every thing, like your file system. The disadantage is that it offers no processing API.","title":"Unstructured Document"},{"location":"notes/l3_nosql/#semi-structured-document","text":"It is a tree-like structure. Data are stored like JSON/XML document. Some implementation offers limited support of indices and range search, e.g. MongoDB. The advantage is that semi-structured data are close to their in-memory counter-parts living in the software application, (binary tree, graph, JSON). It offers flexibility of going back and fro between database and app without the need of joining and object-relational mapping. Most of the semi-structured data are self-explanatory, respecting data types and data structure. One obvious disadvantage is that semi-strutured data do not eliminate data redundancy. Immediate consistency is handled loosely. Range queries could be harder to implememented.","title":"Semi-structured Document"},{"location":"notes/l3_sql/","text":"50.043 - SQL Learning Outcomes By the end of this unit you should be able to use SQL to create and alter databases and tables create and alter table constraints inject data into tables retrieve data from tables update/delete data from tables SQL SQL is a high-level language for data definition and data manipulation. By high-level, we mean it is * expressive * closer to the programmers SQL is a declarative programming language. By declarative, we use SQL to specify what to do but not how to do. (The \"how-to\" parts are left to the underlying runtime to decide, i.e. the DBMS query operation module). SQL is almost universal and cross platforms. Modern big data and non-relation databases extend to support SQL. Note that different DBMSes have different subset of SQL statements. In this unit, we try to cover the common ones. Data definition language Let's consider the DDL of SQL. Create and drop database Note that in a DBMS, there may be many different databases, identified by their names. To create a database, we may use the following SQL statement. create database if not exists db_name; where db_name is the name of the database; if not exists means the create statement only applies when there is no existing database in the DBMS with name db_name . Note that SQL is case-insensitive. To drop a database drop database if exists db_name; To rename a database alter database db_name rename to my_db; However in some DBMS, e.g. MySQL, the above statement is rejected. Instead, we need to dump the old database and load it into the new database. Create table create table if not exists my_db.article ( id char(100) primary key, name char(100) ); In the above statement, we create the article table in the database my_db . article has two attributes, id and name . Both attributes are of type char(100) , i.e. character sequence with max length = 100. id is set to be primary key of the table. Similarly, we create the book , publisher and publish tables. create table if not exists my_db.book ( id char(100) primary key, name char(100) ); create table if not exists my_db.publisher ( id char(100) primary key, name char(100) ); Finally, we create the table publish which was translated from a tertiary relationship. create table if not exists my_db.publish ( article_id char(100), book_id char(100), publisher_id char(100), primary key (article_id, book_id), foreign key (article_id) references my_db.article(id), foreign key (book_id) references my_db.book(id), foreign key (publisher_id) references my_db.publisher(id) ); Since the primary key is a composite key, it is specified in a separate clause (line 5). In addition, we have to create three foreign key constraints on article_id , book_id and publisher_id to ensure their existence in the entity table article , book and publisher . Alter table We can alter a table (e.g. name, attribute name, attribute type, constraints) using the alter statement. For instance, -- drop primary key alter table my_db.publish drop primary key; -- recreate primary key alter table my_db.publish add primary key (article_id, publisher_id); The first alter statement drop the primary key, and the second one recreate a new primary using (article_id, publisher_id) composition. Some DBMS Implementation Specific Details Note that in some DBMS implementation, we need to drop the foreign key constraints before we can drop the primary key. For instance MySQL, the foreign key constraint automatically create an index on the attribute, in this case article_id and book_id . It uses the existing index from the existing primary key constraint. In case of MySQL, we need to first find out the name of the foreign key constraints select table_schema, table_name, column_name, constraint_name from information_schema.key_column_usage where table_name = 'publish'; +--------------+------------+--------------+-----------------+ | TABLE_SCHEMA | TABLE_NAME | COLUMN_NAME | CONSTRAINT_NAME | +--------------+------------+--------------+-----------------+ | my_db | publish | article_id | PRIMARY | | my_db | publish | book_id | PRIMARY | | my_db | publish | article_id | publish_ibfk_1 | | my_db | publish | book_id | publish_ibfk_2 | | my_db | publish | publisher_id | publish_ibfk_3 | +--------------+------------+--------------+-----------------+ 5 rows in set (0.00 sec) Then execute the following statements before the drop primary key statement. alter table my_db.publish drop foreign key publish_ibfk_1; alter table my_db.publish drop foreign key publish_ibfk_2; Then execute the following statements after the add primary key statement alter table my_db.publish add foreign key (article_id) references my_db.article(id) ; alter table my_db.publish add foreign key (book_id) references my_db.book(id); For more detals of alter table statement for MySQL, refer to the MySQL documentation . Drop table To drop a table, we could run drop table my_db.publish; -- but don't do it, it is irreversible, instead we should probably to rename it to something else. Injecting value To inject values, we use the insert statement. insert into my_db.article (id, name) values ('a1', 'article 1'), ('a2', 'article 2'); insert into my_db.book (id, name) values ('b1', 'book 1'), ('b2', 'book 2'); insert into my_db.publisher (id, name) values ('p1', 'publisher 1'), ('p2', 'publisher 2'); Note that we can omit the schema (id, name) when values for all columsn are present. Furthermore, when inserting values into a table with foreign key constraint, e.g. publish references article , book and publisher , the values to be inserted must respect the existence of the referenced keys from the referenced tables. insert into my_db.publish (article_id, book_id, publisher_id) values ('a1', 'b1', 'p1'), ('a2', 'b1', 'p1'), ('a1', 'b2', 'p2'); Mass importing and exporting In some situation, it is inefficient to inject values one by one via insert statement. Many DBMS implementations offer means to import and export data from text file or other format. For MySQL, please refer to this document and this document . Querying table To retrieve data stored in a table, we use the select statement. select article_id, book_id, publisher_id from my_db.publish; In the above statement we retrieve all records (tuples) from the publish table. In this case, since we are retrieving all columns, we could re-write the above as follows, select * from my_db.publish; Export to CSV In some implemntation, such as MySQL, we could use the select statement to export the data in a table into a CSV file. select * from my_db.publish into outfile '/tmp/publish.csv' fields terminated by ','; Join-Query When querying multiple table, we would use the inner join. select * from my_db.publish inner join my_db.article on my_db.publish.article_id = my_db.article.id; For breivity, we could give aliases to the tables being joined. select * from my_db.publish p inner join my_db.article a on p.article_id = a.id; The above queries produce +------------+---------+--------------+----+-----------+ | article_id | book_id | publisher_id | id | name | +------------+---------+--------------+----+-----------+ | a1 | b1 | p1 | a1 | article 1 | | a1 | b2 | p2 | a1 | article 1 | | a2 | b1 | p1 | a2 | article 2 | +------------+---------+--------------+----+-----------+ The left- and right- outter join queries can be expressed in a similar way by replacing inner by left or right . Where clause Suppose we would like to find all the article names that are published by publisher p1 . select a.name from my_db.publish p inner join my_db.article a on p.article_id = a.id where p.publisher_id = 'p1'; which produces +-----------+ | name | +-----------+ | article 1 | | article 2 | +-----------+ Note that instead of inner join , we can rewrite the above query using equi-join which is pushing the id matching to the filtering operation. select a.name from my_db.publish p, my_db.article a where p.article_id = a.id and p.publisher_id = 'p1'; This is because the following equation holds in relational algebra \\[ publish \\bowtie_{publish.article_id = article.id} article = \\sigma_{publish.article_id = article.id}(publish \\times article) \\] Self join Suppose we want to find all articles that are published by both publisher p1 and p2 . The following query select a.* from my_db.publish p, my_db.article a where p.article_id = a.id and p.publisher_id = 'p1' and p.publisher_id = 'p2'; won't produce any result. This is due to the fac that the entire conjunction predicate in the where clause is applied to per tuple level. Since there is no tuple having publisher_id as p1 and p2 at the same time, the result is an empty set. In such situation, we need to join a table to itself. select a.* from my_db.publish p1, my_db.publish p2, my_db.article a where p1.article_id = a.id and p2.article_id = a.id and p1.publisher_id = 'p1' and p2.publisher_id = 'p2'; In the above, we \"clone\" the publish table twice, then the join are performed among the two clones and the article table. Nested query Alternatively to self-join, we could express the above query using nested query. select a1.* from my_db.publish p1, my_db.article a1 where p1.article_id = a1.id and p1.publisher_id = 'p1' and a1.id in (select a2.id from my_db.publish p2, my_db.article a2 where p2.article_id = a2.id and p2.publisher_id = 'p2' ); In the above, we find a nested query, the outer query joins publish with article and filters out those tuples with publisher_id as p1 . The last predicate checks the article id must be found in the result of the nested query. The nested query joins the clones of the two tables and filter tuples with publisher_id equal to p2 . Aggregation For analytic purpose, we need to aggregate values by group. For example, the following statement counts the number of tuples in the publish table. select count(*) from my_db.publish; Suppose we would like to counts the number of published article published by publisher p1 . select publisher_id, count(*) from my_db.publish group by publisher_id; +--------------+----------+ | publisher_id | count(*) | +--------------+----------+ | p1 | 2 | | p2 | 1 | +--------------+----------+ In the above the group by clause specifies the attribute publisher_id is the attribute the groups created by, i.e. tuples within each group should have the same publisher_id . The above SQL statement is equivalent to the following relational algebra express \\[ _{publisher\\_id}\\gamma_{count(*)}(publish) \\] Sorting Suppose we want to sort the result of the last query by the counts in ascending order. select publisher_id, count(*) as cnt from my_db.publish group by publisher_id order by cnt asc; In the above the as cnt creates an alias for the column count(*) for the ease of references. The order by clause specifies the order of the returned results. +--------------+-----+ | publisher_id | cnt | +--------------+-----+ | p2 | 1 | | p1 | 2 | +--------------+-----+ Update and delete To update tuples/records in table, we use the update statement. update my_db.publisher set name = 'publisher one' where name = 'publisher 1'; The above SQL statement updates all tuples's name to publisher one in publisher table with the existing name as publisher 1 . To delete tuples/records, we use the delete statement. delete from my_db.publisher where name = 'publisher 1';","title":"50.043 - SQL"},{"location":"notes/l3_sql/#50043-sql","text":"","title":"50.043 - SQL"},{"location":"notes/l3_sql/#learning-outcomes","text":"By the end of this unit you should be able to use SQL to create and alter databases and tables create and alter table constraints inject data into tables retrieve data from tables update/delete data from tables","title":"Learning Outcomes"},{"location":"notes/l3_sql/#sql","text":"SQL is a high-level language for data definition and data manipulation. By high-level, we mean it is * expressive * closer to the programmers SQL is a declarative programming language. By declarative, we use SQL to specify what to do but not how to do. (The \"how-to\" parts are left to the underlying runtime to decide, i.e. the DBMS query operation module). SQL is almost universal and cross platforms. Modern big data and non-relation databases extend to support SQL. Note that different DBMSes have different subset of SQL statements. In this unit, we try to cover the common ones.","title":"SQL"},{"location":"notes/l3_sql/#data-definition-language","text":"Let's consider the DDL of SQL.","title":"Data definition language"},{"location":"notes/l3_sql/#create-and-drop-database","text":"Note that in a DBMS, there may be many different databases, identified by their names. To create a database, we may use the following SQL statement. create database if not exists db_name; where db_name is the name of the database; if not exists means the create statement only applies when there is no existing database in the DBMS with name db_name . Note that SQL is case-insensitive. To drop a database drop database if exists db_name; To rename a database alter database db_name rename to my_db; However in some DBMS, e.g. MySQL, the above statement is rejected. Instead, we need to dump the old database and load it into the new database.","title":"Create and drop database"},{"location":"notes/l3_sql/#create-table","text":"create table if not exists my_db.article ( id char(100) primary key, name char(100) ); In the above statement, we create the article table in the database my_db . article has two attributes, id and name . Both attributes are of type char(100) , i.e. character sequence with max length = 100. id is set to be primary key of the table. Similarly, we create the book , publisher and publish tables. create table if not exists my_db.book ( id char(100) primary key, name char(100) ); create table if not exists my_db.publisher ( id char(100) primary key, name char(100) ); Finally, we create the table publish which was translated from a tertiary relationship. create table if not exists my_db.publish ( article_id char(100), book_id char(100), publisher_id char(100), primary key (article_id, book_id), foreign key (article_id) references my_db.article(id), foreign key (book_id) references my_db.book(id), foreign key (publisher_id) references my_db.publisher(id) ); Since the primary key is a composite key, it is specified in a separate clause (line 5). In addition, we have to create three foreign key constraints on article_id , book_id and publisher_id to ensure their existence in the entity table article , book and publisher .","title":"Create table"},{"location":"notes/l3_sql/#alter-table","text":"We can alter a table (e.g. name, attribute name, attribute type, constraints) using the alter statement. For instance, -- drop primary key alter table my_db.publish drop primary key; -- recreate primary key alter table my_db.publish add primary key (article_id, publisher_id); The first alter statement drop the primary key, and the second one recreate a new primary using (article_id, publisher_id) composition.","title":"Alter table"},{"location":"notes/l3_sql/#some-dbms-implementation-specific-details","text":"Note that in some DBMS implementation, we need to drop the foreign key constraints before we can drop the primary key. For instance MySQL, the foreign key constraint automatically create an index on the attribute, in this case article_id and book_id . It uses the existing index from the existing primary key constraint. In case of MySQL, we need to first find out the name of the foreign key constraints select table_schema, table_name, column_name, constraint_name from information_schema.key_column_usage where table_name = 'publish'; +--------------+------------+--------------+-----------------+ | TABLE_SCHEMA | TABLE_NAME | COLUMN_NAME | CONSTRAINT_NAME | +--------------+------------+--------------+-----------------+ | my_db | publish | article_id | PRIMARY | | my_db | publish | book_id | PRIMARY | | my_db | publish | article_id | publish_ibfk_1 | | my_db | publish | book_id | publish_ibfk_2 | | my_db | publish | publisher_id | publish_ibfk_3 | +--------------+------------+--------------+-----------------+ 5 rows in set (0.00 sec) Then execute the following statements before the drop primary key statement. alter table my_db.publish drop foreign key publish_ibfk_1; alter table my_db.publish drop foreign key publish_ibfk_2; Then execute the following statements after the add primary key statement alter table my_db.publish add foreign key (article_id) references my_db.article(id) ; alter table my_db.publish add foreign key (book_id) references my_db.book(id); For more detals of alter table statement for MySQL, refer to the MySQL documentation .","title":"Some DBMS Implementation Specific Details"},{"location":"notes/l3_sql/#drop-table","text":"To drop a table, we could run drop table my_db.publish; -- but don't do it, it is irreversible, instead we should probably to rename it to something else.","title":"Drop table"},{"location":"notes/l3_sql/#injecting-value","text":"To inject values, we use the insert statement. insert into my_db.article (id, name) values ('a1', 'article 1'), ('a2', 'article 2'); insert into my_db.book (id, name) values ('b1', 'book 1'), ('b2', 'book 2'); insert into my_db.publisher (id, name) values ('p1', 'publisher 1'), ('p2', 'publisher 2'); Note that we can omit the schema (id, name) when values for all columsn are present. Furthermore, when inserting values into a table with foreign key constraint, e.g. publish references article , book and publisher , the values to be inserted must respect the existence of the referenced keys from the referenced tables. insert into my_db.publish (article_id, book_id, publisher_id) values ('a1', 'b1', 'p1'), ('a2', 'b1', 'p1'), ('a1', 'b2', 'p2');","title":"Injecting value"},{"location":"notes/l3_sql/#mass-importing-and-exporting","text":"In some situation, it is inefficient to inject values one by one via insert statement. Many DBMS implementations offer means to import and export data from text file or other format. For MySQL, please refer to this document and this document .","title":"Mass importing and exporting"},{"location":"notes/l3_sql/#querying-table","text":"To retrieve data stored in a table, we use the select statement. select article_id, book_id, publisher_id from my_db.publish; In the above statement we retrieve all records (tuples) from the publish table. In this case, since we are retrieving all columns, we could re-write the above as follows, select * from my_db.publish;","title":"Querying table"},{"location":"notes/l3_sql/#export-to-csv","text":"In some implemntation, such as MySQL, we could use the select statement to export the data in a table into a CSV file. select * from my_db.publish into outfile '/tmp/publish.csv' fields terminated by ',';","title":"Export to CSV"},{"location":"notes/l3_sql/#join-query","text":"When querying multiple table, we would use the inner join. select * from my_db.publish inner join my_db.article on my_db.publish.article_id = my_db.article.id; For breivity, we could give aliases to the tables being joined. select * from my_db.publish p inner join my_db.article a on p.article_id = a.id; The above queries produce +------------+---------+--------------+----+-----------+ | article_id | book_id | publisher_id | id | name | +------------+---------+--------------+----+-----------+ | a1 | b1 | p1 | a1 | article 1 | | a1 | b2 | p2 | a1 | article 1 | | a2 | b1 | p1 | a2 | article 2 | +------------+---------+--------------+----+-----------+ The left- and right- outter join queries can be expressed in a similar way by replacing inner by left or right .","title":"Join-Query"},{"location":"notes/l3_sql/#where-clause","text":"Suppose we would like to find all the article names that are published by publisher p1 . select a.name from my_db.publish p inner join my_db.article a on p.article_id = a.id where p.publisher_id = 'p1'; which produces +-----------+ | name | +-----------+ | article 1 | | article 2 | +-----------+ Note that instead of inner join , we can rewrite the above query using equi-join which is pushing the id matching to the filtering operation. select a.name from my_db.publish p, my_db.article a where p.article_id = a.id and p.publisher_id = 'p1'; This is because the following equation holds in relational algebra \\[ publish \\bowtie_{publish.article_id = article.id} article = \\sigma_{publish.article_id = article.id}(publish \\times article) \\]","title":"Where clause"},{"location":"notes/l3_sql/#self-join","text":"Suppose we want to find all articles that are published by both publisher p1 and p2 . The following query select a.* from my_db.publish p, my_db.article a where p.article_id = a.id and p.publisher_id = 'p1' and p.publisher_id = 'p2'; won't produce any result. This is due to the fac that the entire conjunction predicate in the where clause is applied to per tuple level. Since there is no tuple having publisher_id as p1 and p2 at the same time, the result is an empty set. In such situation, we need to join a table to itself. select a.* from my_db.publish p1, my_db.publish p2, my_db.article a where p1.article_id = a.id and p2.article_id = a.id and p1.publisher_id = 'p1' and p2.publisher_id = 'p2'; In the above, we \"clone\" the publish table twice, then the join are performed among the two clones and the article table.","title":"Self join"},{"location":"notes/l3_sql/#nested-query","text":"Alternatively to self-join, we could express the above query using nested query. select a1.* from my_db.publish p1, my_db.article a1 where p1.article_id = a1.id and p1.publisher_id = 'p1' and a1.id in (select a2.id from my_db.publish p2, my_db.article a2 where p2.article_id = a2.id and p2.publisher_id = 'p2' ); In the above, we find a nested query, the outer query joins publish with article and filters out those tuples with publisher_id as p1 . The last predicate checks the article id must be found in the result of the nested query. The nested query joins the clones of the two tables and filter tuples with publisher_id equal to p2 .","title":"Nested query"},{"location":"notes/l3_sql/#aggregation","text":"For analytic purpose, we need to aggregate values by group. For example, the following statement counts the number of tuples in the publish table. select count(*) from my_db.publish; Suppose we would like to counts the number of published article published by publisher p1 . select publisher_id, count(*) from my_db.publish group by publisher_id; +--------------+----------+ | publisher_id | count(*) | +--------------+----------+ | p1 | 2 | | p2 | 1 | +--------------+----------+ In the above the group by clause specifies the attribute publisher_id is the attribute the groups created by, i.e. tuples within each group should have the same publisher_id . The above SQL statement is equivalent to the following relational algebra express \\[ _{publisher\\_id}\\gamma_{count(*)}(publish) \\]","title":"Aggregation"},{"location":"notes/l3_sql/#sorting","text":"Suppose we want to sort the result of the last query by the counts in ascending order. select publisher_id, count(*) as cnt from my_db.publish group by publisher_id order by cnt asc; In the above the as cnt creates an alias for the column count(*) for the ease of references. The order by clause specifies the order of the returned results. +--------------+-----+ | publisher_id | cnt | +--------------+-----+ | p2 | 1 | | p1 | 2 | +--------------+-----+","title":"Sorting"},{"location":"notes/l3_sql/#update-and-delete","text":"To update tuples/records in table, we use the update statement. update my_db.publisher set name = 'publisher one' where name = 'publisher 1'; The above SQL statement updates all tuples's name to publisher one in publisher table with the existing name as publisher 1 . To delete tuples/records, we use the delete statement. delete from my_db.publisher where name = 'publisher 1';","title":"Update and delete"},{"location":"notes/l4_fd/","text":"50.043 - Functional Dependency Learning Outcomes By the end of this unit, you should be able to explain functional dependencies existing in data compute the closure of a set of functional dependencies compute the canonical cover of a set of functional dependencies identify candidate keys from a relation Data Anomalies One of the challenges in designing database (not DBMS) is to eliminate data anomalies. The following are the most common kinds of data anomalies: update anomalies insert anomalies delete anomalies A motivating scenario From this point onwards, we treat the terms \"relation\" and \"table\" interchangable. Let's recall the example of article-book-publisher In the above we reuse the running example from the earlier class, with one adjustment. Your client suggests that they want to include the date of publish whenever an article is published. Hence naturally you include a date attribute to the publish relationship. Apply the same steps of ER-to-Relational translation, we have the following relations Article( id , name) Book( id ,name) Publisher( id , name) Publish( article_id, book_id , publisher_id, date) We create the tables based on the above relational model and load up the data. Specifically the table Publish looks like the following article_id book_id publisher_id date a1 b1 p1 11/5/2019 a2 b1 p1 11/5/2019 a1 b2 p2 21/3/2020 Then we realized something awkward and verified its validity with the client. The date of publish is tied to the book being published. That means, we will have some duplicate dates in the Publish table. This sounds a bit unfortunate, we may shrug it off. However, something far more serious immediately follows. Suppose we would like to update the date of publish of a book, say b1 , we need to update all entries in Publish table with book_id as b1 . This is known as the update anomaly. Suppose we would like to insert a new book to Book table. However, we have nowhere to set its date until we find an article_id and a publisher_id to insert that info into the Publish table. This is known as the insert anomaly. Suppose we would like to delete all entries from the Publish table with book_id as b1 . After that, we lose the publishing date of b1 entirely. This is known as the delete anomaly. In summary, in-proper relation model design results in data anomalies. To avoid that, we need a proper method to capture this kind of business requirement, i.e. the publish date is tied to a book. Functional Dependencies Functional dependencies define a relationship among data in a relation. It is a representation of certain business requirement as we encountered in the earlier example. Formally speaking, a functional dependency is a constraint between two sets of attributes. Definition of FD Let \\(X_1,...,X_m\\) , \\(Y_1, ..., Y_n\\) be attributes. we write \\[ X_1,...,X_m \\rightarrow Y_1, ..., Y_n \\] to denote a functional dependency between \\(X_1, ..., X_m\\) and \\(Y_1, ..., Y_n\\) . We refer \\(X_1,...,X_m\\) as the antecedent and \\(Y_1, ..., Y_n\\) as the consequent . Mathematic Meaning of FD Given a FD \\(X_1,...,X_m \\rightarrow Y_1, ..., Y_n\\) , we conclude that values of \\(X_1,...,X_m\\) functionally determine the values of \\(Y_1, ..., Y_n\\) . Let \\(t\\) be a tuple, we write \\(t[X]\\) to refer to the value of attribute \\(X\\) in tuple \\(t\\) . Formally speaking, an FD \\(X_1,...,X_m \\rightarrow Y_1, ..., Y_n\\) holds in a relation \\(R\\) iff \\(\\forall t,t' \\in R\\) we have \\(t[X_1] = t'[X_1] \\wedge ... \\wedge t[X_m] = t'[X_m]\\) implies \\(t[Y_1] = t'[Y_1] \\wedge ... \\wedge t[Y_n] = t'[Y_n]\\) . For example in the article-book-publisher example, for any tuples t1 and t2 in Publish table, t1[book_id] = t2[book_id] implies t1[date] = t2[date] . Validity of FDs Since we are applying relation model to business problems. It is insufficient to observe and validate whether a FD holds in an instance of a relation (i.e. by observing the values in a table!). To verify the validity of FDs, we need to check with the domain expert, project stack holders and end users. In otherwords, FDs must come from the busines requirements, not be inferred from the data. Functional Dependency Closure Given a relation \\(R\\) and a set of FDs \\(F\\) that holds on \\(R\\) . There are many other sets of FDs that are equivalent to \\(F\\) . For instance, let \\(F = \\{ X \\rightarrow YZ \\}\\) , we find that \\(F' = \\{ X \\rightarrow Y, X \\rightarrow Z\\}\\) and \\(F \\equiv F'\\) . Some points to take note. For brevity, we omit the commas when there is no confusion, i.e. \\(YZ\\) is a short hand of \\(Y,Z\\) . Informally, we say \\(F \\equiv F'\\) if \\(F\\) and \\(F'\\) impose the same set constraint on the values in \\(R\\) . Similarly, if we let \\(F'' = F \\cup F'\\) , we have \\(F \\equiv F' \\equiv F''\\) . Now we consider finding the greatest superset \\(F^+\\) such that \\(F \\equiv F\\) and for any other \\(G\\) that is \\(F \\equiv G\\) , we have \\(G \\subseteq F^+\\) . \\(F^+\\) is called the closure of \\(F\\) . Computing \\(F^+\\) To compute \\(F^+\\) we need some rule-based rewriting system. Reflexivity rule Let \\(Y\\) and \\(X\\) be sets of attributes, such that \\(Y \\subseteq X\\) . Then \\(X \\rightarrow Y\\) . For instance, { date } \\(\\subseteq\\) { book_id, date }, thus we have book_id,date \\(\\rightarrow\\) date Augmentation rule Let \\(Y\\) , \\(Z\\) and \\(X\\) be sets of attributes, such that \\(X \\rightarrow Y\\) . Then \\(XZ \\rightarrow YZ\\) . (Note \\(XZ\\) is shorthand for \\(X\\cup Z\\) ). For instance, given book_id \\(\\rightarrow\\) date , we have book_id,publisher_id \\(\\rightarrow\\) date,publisher . Transitivity rule Let \\(Y\\) , \\(Z\\) and \\(X\\) be sets of attributes, such that \\(X \\rightarrow Y\\) and \\(Y \\rightarrow Z\\) . Then \\(X \\rightarrow Z\\) . Question - Split rule Given the above three rules, can you prove \\(X \\rightarrow YZ\\) implies \\(X \\rightarrow Y \\wedge X \\rightarrow Z\\) ? This derived rule is also known as the split rule. Algorithm At the start, let \\(F^+ = F\\) . Find pick one of the three Axioms to apply, to generate a FD, let's say \\(X\\rightarrow Y\\) . Let \\(F^+ = F^+ \\cup \\{X\\rightarrow Y\\}\\) . repeat step 2 until \\(F^+\\) does not change any more. For example, given \\(F = \\{ book\\_id \\rightarrow date \\}\\) , we compute \\(F^+\\) as follows, step new FD rule 1 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow article\\_id\\) Ref 2 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow book\\_id\\) Ref 3 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 4 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow date\\) Ref 5 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow article\\_id\\) Ref 6 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow book\\_id\\) Ref 7 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow publisher\\_id\\) Ref 8 \\(book\\_id, publisher\\_id, date \\rightarrow book\\_id\\) Ref 9 \\(book\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 10 \\(book\\_id, publisher\\_id, date \\rightarrow date\\) Ref 11 \\(article\\_id, publisher\\_id, date \\rightarrow article\\_id\\) Ref 12 \\(article\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 13 \\(article\\_id, publisher\\_id, date \\rightarrow date\\) Ref 14 \\(article\\_id, book\\_id, date \\rightarrow article\\_id\\) Ref 15 \\(article\\_id, book\\_id, date \\rightarrow book\\_id\\) Ref 16 \\(article\\_id, book\\_id, date \\rightarrow date\\) Ref 17 \\(article\\_id, book\\_id \\rightarrow article\\_id\\) Ref 18 \\(article\\_id, book\\_id \\rightarrow book\\_id\\) Ref 19 \\(article\\_id, publisher\\_id \\rightarrow article\\_id\\) Ref 20 \\(article\\_id, publisher\\_id \\rightarrow publisher\\_id\\) Ref 21 \\(article\\_id, date \\rightarrow article\\_id\\) Ref 22 \\(article\\_id, date \\rightarrow date\\) Ref 23 \\(book\\_id, date \\rightarrow book\\_id\\) Ref 24 \\(book\\_id, date \\rightarrow date\\) Ref 25 \\(publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 25 \\(publisher\\_id, date \\rightarrow date\\) Ref No other rules are applicable, then we are done. Canonical Cover A canonical cover of \\(F\\) is the smallest possible subset of \\(F\\) such that its closure is \\(F^+\\) . The above statement is intuitive but not precise. To be precise, we need to define the standard form of FDs. Standard form definition An FD is in standard form iff its RHS is a single attribute. It follows that for any set of FDs, we can convert it into an equivalent set with standard form FDs. (Hint: we know \\(X\\rightarrow YZ\\) implies \\(X \\rightarrow Y\\) and \\(X \\rightarrow Z\\) holds.) Formal definition Let \\(F\\) denote a set of FDs, we say \\(F_c\\) is the canonical cover iff All FDs in \\(F_c\\) are in standard form; and \\(F_c^+ \\subseteq F^+ \\wedge F^+ \\subseteq F_c^+\\) ; and \\(\\neg \\exists G \\subset F_c\\) such that \\(G^+ \\subseteq F^+ \\wedge F^+ \\subseteq G^+\\) Algorithm to compute \\(F_c\\) Convert \\(F\\) to standard form. Minimize the lhs of each FD, by applying Reflexitivity, Augmentation and Transitivity. Remove redundant FDs, by applying Reflexitivity, Augmentation and Transitivity. For example, consider \\[ F = \\left [ \\begin{array}{ccc} AB & \\rightarrow & C & (1) \\\\ A & \\rightarrow & BC & (2) \\\\ B & \\rightarrow & C & (3) \\\\ A & \\rightarrow & B & (4) \\end{array} \\right ] \\] First applying split rule to (2) \\[ F = \\left [ \\begin{array}{ccc} AB & \\rightarrow & C & (1) \\\\ A & \\rightarrow & B & (2) \\\\ A & \\rightarrow & C & (2') \\\\ B & \\rightarrow & C & (3) \\\\ A & \\rightarrow & B & (4) \\end{array} \\right ] \\] Then we apply Augmentation and Transitivity rules to (1) and (2) to minimize LHS of rule (1) \\[ F = \\left [ \\begin{array}{ccc} A & \\rightarrow & C & (1) \\\\ A & \\rightarrow & B & (2) \\\\ A & \\rightarrow & C & (2') \\\\ B & \\rightarrow & C & (3) \\\\ A & \\rightarrow & B & (4) \\end{array} \\right ] \\] Now we find that (2') is a duplicate of (1) and (4) is a duplicate of (2). \\[ F = \\left [ \\begin{array}{ccc} A & \\rightarrow & C & (1) \\\\ A & \\rightarrow & B & (2) \\\\ B & \\rightarrow & C & (3) \\end{array} \\right ] \\] Finally we find that (1) is derivable by applying transitivity to (2) and (3). \\[ F = \\left [ \\begin{array}{ccc} A & \\rightarrow & B & (2) \\\\ B & \\rightarrow & C & (3) \\end{array} \\right ] \\] The above is minimal. Note that the algorithm described above is non-confluent, i.e. depending on the order of FDs being picked a different canonical cover might be generated. Application of canonical cover Canonical cover is very useful. We can use it to reduce the number of constraints (which is expensive to verified). We leverage on Canonical cover to identify candidate key for a relation. Some extra terminologies - different kinds of keys In database, we call a set of attribute of a relation as a Super key if it functionally determines all other attributes Candidate key if it is a minimal set of attributes that functionally determines all other attributes. Primary key if it is one of the candidate key. (We just fix one.)","title":"50.043 - Functional Dependency"},{"location":"notes/l4_fd/#50043-functional-dependency","text":"","title":"50.043 - Functional Dependency"},{"location":"notes/l4_fd/#learning-outcomes","text":"By the end of this unit, you should be able to explain functional dependencies existing in data compute the closure of a set of functional dependencies compute the canonical cover of a set of functional dependencies identify candidate keys from a relation","title":"Learning Outcomes"},{"location":"notes/l4_fd/#data-anomalies","text":"One of the challenges in designing database (not DBMS) is to eliminate data anomalies. The following are the most common kinds of data anomalies: update anomalies insert anomalies delete anomalies","title":"Data Anomalies"},{"location":"notes/l4_fd/#a-motivating-scenario","text":"From this point onwards, we treat the terms \"relation\" and \"table\" interchangable. Let's recall the example of article-book-publisher In the above we reuse the running example from the earlier class, with one adjustment. Your client suggests that they want to include the date of publish whenever an article is published. Hence naturally you include a date attribute to the publish relationship. Apply the same steps of ER-to-Relational translation, we have the following relations Article( id , name) Book( id ,name) Publisher( id , name) Publish( article_id, book_id , publisher_id, date) We create the tables based on the above relational model and load up the data. Specifically the table Publish looks like the following article_id book_id publisher_id date a1 b1 p1 11/5/2019 a2 b1 p1 11/5/2019 a1 b2 p2 21/3/2020 Then we realized something awkward and verified its validity with the client. The date of publish is tied to the book being published. That means, we will have some duplicate dates in the Publish table. This sounds a bit unfortunate, we may shrug it off. However, something far more serious immediately follows. Suppose we would like to update the date of publish of a book, say b1 , we need to update all entries in Publish table with book_id as b1 . This is known as the update anomaly. Suppose we would like to insert a new book to Book table. However, we have nowhere to set its date until we find an article_id and a publisher_id to insert that info into the Publish table. This is known as the insert anomaly. Suppose we would like to delete all entries from the Publish table with book_id as b1 . After that, we lose the publishing date of b1 entirely. This is known as the delete anomaly. In summary, in-proper relation model design results in data anomalies. To avoid that, we need a proper method to capture this kind of business requirement, i.e. the publish date is tied to a book.","title":"A motivating scenario"},{"location":"notes/l4_fd/#functional-dependencies","text":"Functional dependencies define a relationship among data in a relation. It is a representation of certain business requirement as we encountered in the earlier example. Formally speaking, a functional dependency is a constraint between two sets of attributes.","title":"Functional Dependencies"},{"location":"notes/l4_fd/#definition-of-fd","text":"Let \\(X_1,...,X_m\\) , \\(Y_1, ..., Y_n\\) be attributes. we write \\[ X_1,...,X_m \\rightarrow Y_1, ..., Y_n \\] to denote a functional dependency between \\(X_1, ..., X_m\\) and \\(Y_1, ..., Y_n\\) . We refer \\(X_1,...,X_m\\) as the antecedent and \\(Y_1, ..., Y_n\\) as the consequent .","title":"Definition of FD"},{"location":"notes/l4_fd/#mathematic-meaning-of-fd","text":"Given a FD \\(X_1,...,X_m \\rightarrow Y_1, ..., Y_n\\) , we conclude that values of \\(X_1,...,X_m\\) functionally determine the values of \\(Y_1, ..., Y_n\\) . Let \\(t\\) be a tuple, we write \\(t[X]\\) to refer to the value of attribute \\(X\\) in tuple \\(t\\) . Formally speaking, an FD \\(X_1,...,X_m \\rightarrow Y_1, ..., Y_n\\) holds in a relation \\(R\\) iff \\(\\forall t,t' \\in R\\) we have \\(t[X_1] = t'[X_1] \\wedge ... \\wedge t[X_m] = t'[X_m]\\) implies \\(t[Y_1] = t'[Y_1] \\wedge ... \\wedge t[Y_n] = t'[Y_n]\\) . For example in the article-book-publisher example, for any tuples t1 and t2 in Publish table, t1[book_id] = t2[book_id] implies t1[date] = t2[date] .","title":"Mathematic Meaning of FD"},{"location":"notes/l4_fd/#validity-of-fds","text":"Since we are applying relation model to business problems. It is insufficient to observe and validate whether a FD holds in an instance of a relation (i.e. by observing the values in a table!). To verify the validity of FDs, we need to check with the domain expert, project stack holders and end users. In otherwords, FDs must come from the busines requirements, not be inferred from the data.","title":"Validity of FDs"},{"location":"notes/l4_fd/#functional-dependency-closure","text":"Given a relation \\(R\\) and a set of FDs \\(F\\) that holds on \\(R\\) . There are many other sets of FDs that are equivalent to \\(F\\) . For instance, let \\(F = \\{ X \\rightarrow YZ \\}\\) , we find that \\(F' = \\{ X \\rightarrow Y, X \\rightarrow Z\\}\\) and \\(F \\equiv F'\\) . Some points to take note. For brevity, we omit the commas when there is no confusion, i.e. \\(YZ\\) is a short hand of \\(Y,Z\\) . Informally, we say \\(F \\equiv F'\\) if \\(F\\) and \\(F'\\) impose the same set constraint on the values in \\(R\\) . Similarly, if we let \\(F'' = F \\cup F'\\) , we have \\(F \\equiv F' \\equiv F''\\) . Now we consider finding the greatest superset \\(F^+\\) such that \\(F \\equiv F\\) and for any other \\(G\\) that is \\(F \\equiv G\\) , we have \\(G \\subseteq F^+\\) . \\(F^+\\) is called the closure of \\(F\\) .","title":"Functional Dependency Closure"},{"location":"notes/l4_fd/#computing-f","text":"To compute \\(F^+\\) we need some rule-based rewriting system.","title":"Computing \\(F^+\\)"},{"location":"notes/l4_fd/#reflexivity-rule","text":"Let \\(Y\\) and \\(X\\) be sets of attributes, such that \\(Y \\subseteq X\\) . Then \\(X \\rightarrow Y\\) . For instance, { date } \\(\\subseteq\\) { book_id, date }, thus we have book_id,date \\(\\rightarrow\\) date","title":"Reflexivity rule"},{"location":"notes/l4_fd/#augmentation-rule","text":"Let \\(Y\\) , \\(Z\\) and \\(X\\) be sets of attributes, such that \\(X \\rightarrow Y\\) . Then \\(XZ \\rightarrow YZ\\) . (Note \\(XZ\\) is shorthand for \\(X\\cup Z\\) ). For instance, given book_id \\(\\rightarrow\\) date , we have book_id,publisher_id \\(\\rightarrow\\) date,publisher .","title":"Augmentation rule"},{"location":"notes/l4_fd/#transitivity-rule","text":"Let \\(Y\\) , \\(Z\\) and \\(X\\) be sets of attributes, such that \\(X \\rightarrow Y\\) and \\(Y \\rightarrow Z\\) . Then \\(X \\rightarrow Z\\) .","title":"Transitivity rule"},{"location":"notes/l4_fd/#question-split-rule","text":"Given the above three rules, can you prove \\(X \\rightarrow YZ\\) implies \\(X \\rightarrow Y \\wedge X \\rightarrow Z\\) ? This derived rule is also known as the split rule.","title":"Question - Split rule"},{"location":"notes/l4_fd/#algorithm","text":"At the start, let \\(F^+ = F\\) . Find pick one of the three Axioms to apply, to generate a FD, let's say \\(X\\rightarrow Y\\) . Let \\(F^+ = F^+ \\cup \\{X\\rightarrow Y\\}\\) . repeat step 2 until \\(F^+\\) does not change any more. For example, given \\(F = \\{ book\\_id \\rightarrow date \\}\\) , we compute \\(F^+\\) as follows, step new FD rule 1 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow article\\_id\\) Ref 2 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow book\\_id\\) Ref 3 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 4 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow date\\) Ref 5 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow article\\_id\\) Ref 6 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow book\\_id\\) Ref 7 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow publisher\\_id\\) Ref 8 \\(book\\_id, publisher\\_id, date \\rightarrow book\\_id\\) Ref 9 \\(book\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 10 \\(book\\_id, publisher\\_id, date \\rightarrow date\\) Ref 11 \\(article\\_id, publisher\\_id, date \\rightarrow article\\_id\\) Ref 12 \\(article\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 13 \\(article\\_id, publisher\\_id, date \\rightarrow date\\) Ref 14 \\(article\\_id, book\\_id, date \\rightarrow article\\_id\\) Ref 15 \\(article\\_id, book\\_id, date \\rightarrow book\\_id\\) Ref 16 \\(article\\_id, book\\_id, date \\rightarrow date\\) Ref 17 \\(article\\_id, book\\_id \\rightarrow article\\_id\\) Ref 18 \\(article\\_id, book\\_id \\rightarrow book\\_id\\) Ref 19 \\(article\\_id, publisher\\_id \\rightarrow article\\_id\\) Ref 20 \\(article\\_id, publisher\\_id \\rightarrow publisher\\_id\\) Ref 21 \\(article\\_id, date \\rightarrow article\\_id\\) Ref 22 \\(article\\_id, date \\rightarrow date\\) Ref 23 \\(book\\_id, date \\rightarrow book\\_id\\) Ref 24 \\(book\\_id, date \\rightarrow date\\) Ref 25 \\(publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 25 \\(publisher\\_id, date \\rightarrow date\\) Ref No other rules are applicable, then we are done.","title":"Algorithm"},{"location":"notes/l4_fd/#canonical-cover","text":"A canonical cover of \\(F\\) is the smallest possible subset of \\(F\\) such that its closure is \\(F^+\\) . The above statement is intuitive but not precise. To be precise, we need to define the standard form of FDs.","title":"Canonical Cover"},{"location":"notes/l4_fd/#standard-form-definition","text":"An FD is in standard form iff its RHS is a single attribute. It follows that for any set of FDs, we can convert it into an equivalent set with standard form FDs. (Hint: we know \\(X\\rightarrow YZ\\) implies \\(X \\rightarrow Y\\) and \\(X \\rightarrow Z\\) holds.)","title":"Standard form definition"},{"location":"notes/l4_fd/#formal-definition","text":"Let \\(F\\) denote a set of FDs, we say \\(F_c\\) is the canonical cover iff All FDs in \\(F_c\\) are in standard form; and \\(F_c^+ \\subseteq F^+ \\wedge F^+ \\subseteq F_c^+\\) ; and \\(\\neg \\exists G \\subset F_c\\) such that \\(G^+ \\subseteq F^+ \\wedge F^+ \\subseteq G^+\\)","title":"Formal definition"},{"location":"notes/l4_fd/#algorithm-to-compute-f_c","text":"Convert \\(F\\) to standard form. Minimize the lhs of each FD, by applying Reflexitivity, Augmentation and Transitivity. Remove redundant FDs, by applying Reflexitivity, Augmentation and Transitivity. For example, consider \\[ F = \\left [ \\begin{array}{ccc} AB & \\rightarrow & C & (1) \\\\ A & \\rightarrow & BC & (2) \\\\ B & \\rightarrow & C & (3) \\\\ A & \\rightarrow & B & (4) \\end{array} \\right ] \\] First applying split rule to (2) \\[ F = \\left [ \\begin{array}{ccc} AB & \\rightarrow & C & (1) \\\\ A & \\rightarrow & B & (2) \\\\ A & \\rightarrow & C & (2') \\\\ B & \\rightarrow & C & (3) \\\\ A & \\rightarrow & B & (4) \\end{array} \\right ] \\] Then we apply Augmentation and Transitivity rules to (1) and (2) to minimize LHS of rule (1) \\[ F = \\left [ \\begin{array}{ccc} A & \\rightarrow & C & (1) \\\\ A & \\rightarrow & B & (2) \\\\ A & \\rightarrow & C & (2') \\\\ B & \\rightarrow & C & (3) \\\\ A & \\rightarrow & B & (4) \\end{array} \\right ] \\] Now we find that (2') is a duplicate of (1) and (4) is a duplicate of (2). \\[ F = \\left [ \\begin{array}{ccc} A & \\rightarrow & C & (1) \\\\ A & \\rightarrow & B & (2) \\\\ B & \\rightarrow & C & (3) \\end{array} \\right ] \\] Finally we find that (1) is derivable by applying transitivity to (2) and (3). \\[ F = \\left [ \\begin{array}{ccc} A & \\rightarrow & B & (2) \\\\ B & \\rightarrow & C & (3) \\end{array} \\right ] \\] The above is minimal. Note that the algorithm described above is non-confluent, i.e. depending on the order of FDs being picked a different canonical cover might be generated.","title":"Algorithm to compute \\(F_c\\)"},{"location":"notes/l4_fd/#application-of-canonical-cover","text":"Canonical cover is very useful. We can use it to reduce the number of constraints (which is expensive to verified). We leverage on Canonical cover to identify candidate key for a relation.","title":"Application of canonical cover"},{"location":"notes/l4_fd/#some-extra-terminologies-different-kinds-of-keys","text":"In database, we call a set of attribute of a relation as a Super key if it functionally determines all other attributes Candidate key if it is a minimal set of attributes that functionally determines all other attributes. Primary key if it is one of the candidate key. (We just fix one.)","title":"Some extra terminologies - different kinds of keys"},{"location":"notes/l4_normal_form/","text":"50.043 - Normal Forms Learning Outcomes By the end of this unit, you should be able to apply lossless decomposition to a relation verify a relation is in 1NF/2NF/BCNF/3NF decompose a relation into 2NF/BCNF/3NF Decomposition By now, we know that having an FD in which the LHS of the constraint is not a the primary key of the relation imposes data anomalies. Recall the relation Publish( article_id, book_id , publisher_id, date) with the fillowing data article_id book_id publisher_id date a1 b1 p1 11/5/2019 a2 b1 p1 11/5/2019 a1 b2 p2 21/3/2020 There exists an FD book_id \\(\\rightarrow\\) date. Note that book\\_id is not the primary key (though it is part of the primary key). To fix the issue, we need to decompose publish into two smaller relations. But how? The idea is to decompose it based on the FD. From the FD, we find that book_id determines date. We should move book_id and date into another relation. At the same time we should keep the book_id in publish so that the relationship between author_id, book_id and publisher_id is not lost. Lossless decomposition Give a relation \\(R\\) , a decompsition of \\(R\\) , say \\(R_1\\) and \\(R_2\\) is lossless, iff \\(R_1 \\bowtie R_2 \\equiv R\\) . To ensure a decomposition is lossless, we pick an FD constraint \\(X \\rightarrow Y\\) of \\(R\\) then let \\(R_1 = R_1(XY)\\) and \\(R_2 = R_2( attr(R) - Y)\\) , assuming \\(X \\cap Y = \\emptyset\\) . We write \\(attr(R)\\) to compute the set of attributes of \\(R\\) . It follows that \\(X\\) is the common attributes among \\(R_1\\) and \\(R_2\\) , the natural join between \\(R_1\\) and \\(R_2\\) will leverage \\(X\\) hence \\(R_1 \\bowtie R_2 \\equiv R\\) . For instance, in the Publish relation above, we may decompose it by the FD book_id \\(\\rightarrow\\) date. Publish1( article_id, book_id , publisher_id) Publish2( book_id , date) article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 book_id date b1 11/5/2019 b2 21/3/2020 Note that we eliminate the data anomalies. (Eventually, we might merge Publish2 with the Book relation, which is a seperate topic.) Normal Forms The next question we need to consider is how far should we decompose relation? Normal forms define a set of criteria which allows us to check whether the result of decomposition is good enough . 1NF A relation is in 1NF iff its schema is flat, (i.e. contains no sub-structure) and there is no repeating group (i.e. there is no repeating column). For example the following relations are not in 1NF student_id name phones 1234 Peter Parker [95598221, 82335354] This relation's schema is not flat. student_id name phone1 phone2 1234 Peter Parker 95598221 82335354 This relation has a set of repeating columns, phone1 , phone2 . (Though in reality, we could be lenient here, maybe we could rename it to primary contact , secondary contact .) 2NF A relation is in 2NF iff it is in 1NF and all non-key attributes are fully dependent on candidate key. In other words, the relation is at least 1NF and there should be no partial dependency. For example, in the running example Publish( article_id, book_id , publisher_id, date) is in 1NF but not in 2NF, because the attribute date is not fully dependent on the primary key article_id,book_id . It is partially dependent on book_id . Boyd-Codd Normal Form (BCNF) Given a relation \\(R\\) with FDs \\(F\\) , \\(R\\) is in BCNF iff for all non-trivial dependency \\(X \\rightarrow Y \\in F^+\\) , \\(X\\) is a super key. An FD is trivial iff its lhs is a superset of the rhs. For example, Publish1( article_id, book_id , publisher_id) Publish2( book_id , date) are in BCNF, because the only non trial FDs are 1. article_id,book_id \\(\\rightarrow\\) publisher_id 2. article_id,publisher_id \\(\\rightarrow\\) book_id (recall the ER diagram) 3. book_id \\(\\rightarrow\\) date . Note that FD #2 does not violate the BCNF requirement, because article_id,publisher_id is a candidate key of Publish1 hence also a super key. Lemma: A relation \\(R\\) is in BCNF implies \\(R\\) is in 2NF. The proof is omitted. You are encouraged to try proving it. Algorithm to decompose into BCNF Given a relation \\(R\\) and a set of FDs \\(F\\) . The algorithm of decomposing \\(R\\) into BCNF is described as follows. Compute \\(F^+\\) Let \\(Result = \\{R\\}\\) While \\(R_i \\in Result\\) not in BCNF, do 3.1. Choose \\(X\\rightarrow Y \\in F^+\\) such that \\(X\\) and \\(Y\\) are attribtues in \\(R_i\\) but \\(X\\) is not a super key of \\(R_i\\) . 3.2. Decompose \\(R_i\\) into \\(R_{i1}\\) \\(R_{i2}\\) with \\(X\\rightarrow Y\\) . 3.3. Update \\(Result = Result - \\{ R_i\\} \\cup \\{ R_{i1}, R_{i2} \\}\\) A slightly more optimized algorithm def \\(normalize(R)\\) Let \\(C = attr(R)\\) find an attribute set \\(X\\) such that \\(X^+ \\neq X\\) and \\(X^+ \\neq C\\) . if \\(X\\) is not found, then \\(R\\) is in BCNF else decompose \\(R\\) into \\(R_1(X^+)\\) and \\(R_2(C-X^+ \\cup X)\\) \\(normalize(R_1)\\) \\(normalize(R_2)\\) \\(normalize(R)\\) Consider \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\) . First we find all attribute closures. \\(A^+ = AD\\) \\(B^+ = B\\) \\(C^+ = CB\\) \\(D^+ = D\\) \\(AB^+ = ABCD\\) \\(ABC^+ = ABCD\\) ... We find that \\(AB\\) is a candidate key of \\(R\\) . At step 1.2, we found \\(A^+\\) , since \\(A^+ = AD \\neq ABCD\\) . We decompose \\(R\\) into \\(R_1(A,D)\\) \\(R_2(A,B,C)\\) \\(R_1\\) is already in BCNF. \\(R_2\\) is not, because found \\(C^+ = BC\\) . We decompose \\(R_2\\) into \\(R_{21}(B,C)\\) \\(R_{22}(A,C)\\) Then we are done. 3NF Given a relation \\(R\\) with FDs \\(F\\) , \\(R\\) is in 3NF iff for all non-trivial dependency \\(X \\rightarrow Y \\in F^+\\) , 1. \\(X\\) is a super key or 2. \\(Y\\) is part of a candidate key The following diagram shows some example In the first diagram, \\(X \\rightarrow A\\) , assuming \\(KEY\\) is the only candidate key, hence \\(X\\) is not a super key. Further more \\(A\\) is not part of a candidate key. Thus it is a counter example of 3NF. In the second diagram, \\(X\\rightarrow A\\) , \\(X\\) is not a supere key and \\(A\\) is part of a candidate key. Thus it is in 3NF. Lemma: A relation in 3NF is also in BCNF. It can be proven from by the definitions. BCNF vs 3NF BCNF is easier to compute, we just keep finding a FD that violates the definition and keep decomposing until none is found. Though BCNF decomposition is lossless, it is not dependency preserving. A FD set \\(F_i\\) is preserved by \\(R_i\\) iff for each \\(X_1...X_n \\rightarrow Y \\in F_i\\) , \\(X_1,...,X_n,Y\\) are attributes of \\(R_i\\) . Recall the previous example \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\) . Applying BCNF-decomposition will yield \\(R_1(A,D), R_{21}(B,C), R_{22}(A,B)\\) which do not preserve \\(AB\\rightarrow C\\) . Algorithm to compute 3NF With that difference in mind, we present the algorithm to compute 3NF as folows. Apply the BCNF algorithm to decompose \\(R\\) , let's say the result is a set of relations \\(R_1, ..., R_n\\) . Let \\(F_1,...,F_n\\) be the list of FDs preserved by \\(R_1, ..., R_n\\) . Compute \\((F_1 \\cup ... \\cup F_n)^{+}\\) . Let \\(\\bar{F} = F - (F_1 \\cup ... \\cup F_n)^{+}\\) . For each \\(X_1...,X_n\\rightarrow Y \\in \\bar{F}\\) , create a new relation \\(R'(X_1,...,X_n,Y)\\) For example, recall the previous example \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\) . After the BCNF decomposition, we realize \\(R_1(A,D), R_{21}(B,C), R_{22}(A,B)\\) do not preserve \\(AB\\rightarrow C\\) . We create (or restore) \\(R_2(A,B,C)\\) . \\(R_2\\) subsumes \\(R_{21}(B,C)\\) and \\(R_{22}(A,B)\\) , hence we remove \\(R_{21}\\) and \\(R_{22}\\) and keep \\(R_2\\) . Alternatively, we could have used the BCNF algorithm but do not decompose \\(R_2\\) since it does not violate 3NF.","title":"50.043 - Normal Forms"},{"location":"notes/l4_normal_form/#50043-normal-forms","text":"","title":"50.043 - Normal Forms"},{"location":"notes/l4_normal_form/#learning-outcomes","text":"By the end of this unit, you should be able to apply lossless decomposition to a relation verify a relation is in 1NF/2NF/BCNF/3NF decompose a relation into 2NF/BCNF/3NF","title":"Learning Outcomes"},{"location":"notes/l4_normal_form/#decomposition","text":"By now, we know that having an FD in which the LHS of the constraint is not a the primary key of the relation imposes data anomalies. Recall the relation Publish( article_id, book_id , publisher_id, date) with the fillowing data article_id book_id publisher_id date a1 b1 p1 11/5/2019 a2 b1 p1 11/5/2019 a1 b2 p2 21/3/2020 There exists an FD book_id \\(\\rightarrow\\) date. Note that book\\_id is not the primary key (though it is part of the primary key). To fix the issue, we need to decompose publish into two smaller relations. But how? The idea is to decompose it based on the FD. From the FD, we find that book_id determines date. We should move book_id and date into another relation. At the same time we should keep the book_id in publish so that the relationship between author_id, book_id and publisher_id is not lost.","title":"Decomposition"},{"location":"notes/l4_normal_form/#lossless-decomposition","text":"Give a relation \\(R\\) , a decompsition of \\(R\\) , say \\(R_1\\) and \\(R_2\\) is lossless, iff \\(R_1 \\bowtie R_2 \\equiv R\\) . To ensure a decomposition is lossless, we pick an FD constraint \\(X \\rightarrow Y\\) of \\(R\\) then let \\(R_1 = R_1(XY)\\) and \\(R_2 = R_2( attr(R) - Y)\\) , assuming \\(X \\cap Y = \\emptyset\\) . We write \\(attr(R)\\) to compute the set of attributes of \\(R\\) . It follows that \\(X\\) is the common attributes among \\(R_1\\) and \\(R_2\\) , the natural join between \\(R_1\\) and \\(R_2\\) will leverage \\(X\\) hence \\(R_1 \\bowtie R_2 \\equiv R\\) . For instance, in the Publish relation above, we may decompose it by the FD book_id \\(\\rightarrow\\) date. Publish1( article_id, book_id , publisher_id) Publish2( book_id , date) article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 book_id date b1 11/5/2019 b2 21/3/2020 Note that we eliminate the data anomalies. (Eventually, we might merge Publish2 with the Book relation, which is a seperate topic.)","title":"Lossless decomposition"},{"location":"notes/l4_normal_form/#normal-forms","text":"The next question we need to consider is how far should we decompose relation? Normal forms define a set of criteria which allows us to check whether the result of decomposition is good enough .","title":"Normal Forms"},{"location":"notes/l4_normal_form/#1nf","text":"A relation is in 1NF iff its schema is flat, (i.e. contains no sub-structure) and there is no repeating group (i.e. there is no repeating column). For example the following relations are not in 1NF student_id name phones 1234 Peter Parker [95598221, 82335354] This relation's schema is not flat. student_id name phone1 phone2 1234 Peter Parker 95598221 82335354 This relation has a set of repeating columns, phone1 , phone2 . (Though in reality, we could be lenient here, maybe we could rename it to primary contact , secondary contact .)","title":"1NF"},{"location":"notes/l4_normal_form/#2nf","text":"A relation is in 2NF iff it is in 1NF and all non-key attributes are fully dependent on candidate key. In other words, the relation is at least 1NF and there should be no partial dependency. For example, in the running example Publish( article_id, book_id , publisher_id, date) is in 1NF but not in 2NF, because the attribute date is not fully dependent on the primary key article_id,book_id . It is partially dependent on book_id .","title":"2NF"},{"location":"notes/l4_normal_form/#boyd-codd-normal-form-bcnf","text":"Given a relation \\(R\\) with FDs \\(F\\) , \\(R\\) is in BCNF iff for all non-trivial dependency \\(X \\rightarrow Y \\in F^+\\) , \\(X\\) is a super key. An FD is trivial iff its lhs is a superset of the rhs. For example, Publish1( article_id, book_id , publisher_id) Publish2( book_id , date) are in BCNF, because the only non trial FDs are 1. article_id,book_id \\(\\rightarrow\\) publisher_id 2. article_id,publisher_id \\(\\rightarrow\\) book_id (recall the ER diagram) 3. book_id \\(\\rightarrow\\) date . Note that FD #2 does not violate the BCNF requirement, because article_id,publisher_id is a candidate key of Publish1 hence also a super key.","title":"Boyd-Codd Normal Form (BCNF)"},{"location":"notes/l4_normal_form/#lemma-a-relation-r-is-in-bcnf-implies-r-is-in-2nf","text":"The proof is omitted. You are encouraged to try proving it.","title":"Lemma: A relation \\(R\\) is in BCNF implies \\(R\\) is in 2NF."},{"location":"notes/l4_normal_form/#algorithm-to-decompose-into-bcnf","text":"Given a relation \\(R\\) and a set of FDs \\(F\\) . The algorithm of decomposing \\(R\\) into BCNF is described as follows. Compute \\(F^+\\) Let \\(Result = \\{R\\}\\) While \\(R_i \\in Result\\) not in BCNF, do 3.1. Choose \\(X\\rightarrow Y \\in F^+\\) such that \\(X\\) and \\(Y\\) are attribtues in \\(R_i\\) but \\(X\\) is not a super key of \\(R_i\\) . 3.2. Decompose \\(R_i\\) into \\(R_{i1}\\) \\(R_{i2}\\) with \\(X\\rightarrow Y\\) . 3.3. Update \\(Result = Result - \\{ R_i\\} \\cup \\{ R_{i1}, R_{i2} \\}\\)","title":"Algorithm to decompose into BCNF"},{"location":"notes/l4_normal_form/#a-slightly-more-optimized-algorithm","text":"def \\(normalize(R)\\) Let \\(C = attr(R)\\) find an attribute set \\(X\\) such that \\(X^+ \\neq X\\) and \\(X^+ \\neq C\\) . if \\(X\\) is not found, then \\(R\\) is in BCNF else decompose \\(R\\) into \\(R_1(X^+)\\) and \\(R_2(C-X^+ \\cup X)\\) \\(normalize(R_1)\\) \\(normalize(R_2)\\) \\(normalize(R)\\) Consider \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\) . First we find all attribute closures. \\(A^+ = AD\\) \\(B^+ = B\\) \\(C^+ = CB\\) \\(D^+ = D\\) \\(AB^+ = ABCD\\) \\(ABC^+ = ABCD\\) ... We find that \\(AB\\) is a candidate key of \\(R\\) . At step 1.2, we found \\(A^+\\) , since \\(A^+ = AD \\neq ABCD\\) . We decompose \\(R\\) into \\(R_1(A,D)\\) \\(R_2(A,B,C)\\) \\(R_1\\) is already in BCNF. \\(R_2\\) is not, because found \\(C^+ = BC\\) . We decompose \\(R_2\\) into \\(R_{21}(B,C)\\) \\(R_{22}(A,C)\\) Then we are done.","title":"A slightly more optimized algorithm"},{"location":"notes/l4_normal_form/#3nf","text":"Given a relation \\(R\\) with FDs \\(F\\) , \\(R\\) is in 3NF iff for all non-trivial dependency \\(X \\rightarrow Y \\in F^+\\) , 1. \\(X\\) is a super key or 2. \\(Y\\) is part of a candidate key The following diagram shows some example In the first diagram, \\(X \\rightarrow A\\) , assuming \\(KEY\\) is the only candidate key, hence \\(X\\) is not a super key. Further more \\(A\\) is not part of a candidate key. Thus it is a counter example of 3NF. In the second diagram, \\(X\\rightarrow A\\) , \\(X\\) is not a supere key and \\(A\\) is part of a candidate key. Thus it is in 3NF.","title":"3NF"},{"location":"notes/l4_normal_form/#lemma-a-relation-in-3nf-is-also-in-bcnf","text":"It can be proven from by the definitions.","title":"Lemma: A relation in 3NF is also in BCNF."},{"location":"notes/l4_normal_form/#bcnf-vs-3nf","text":"BCNF is easier to compute, we just keep finding a FD that violates the definition and keep decomposing until none is found. Though BCNF decomposition is lossless, it is not dependency preserving. A FD set \\(F_i\\) is preserved by \\(R_i\\) iff for each \\(X_1...X_n \\rightarrow Y \\in F_i\\) , \\(X_1,...,X_n,Y\\) are attributes of \\(R_i\\) . Recall the previous example \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\) . Applying BCNF-decomposition will yield \\(R_1(A,D), R_{21}(B,C), R_{22}(A,B)\\) which do not preserve \\(AB\\rightarrow C\\) .","title":"BCNF vs 3NF"},{"location":"notes/l4_normal_form/#algorithm-to-compute-3nf","text":"With that difference in mind, we present the algorithm to compute 3NF as folows. Apply the BCNF algorithm to decompose \\(R\\) , let's say the result is a set of relations \\(R_1, ..., R_n\\) . Let \\(F_1,...,F_n\\) be the list of FDs preserved by \\(R_1, ..., R_n\\) . Compute \\((F_1 \\cup ... \\cup F_n)^{+}\\) . Let \\(\\bar{F} = F - (F_1 \\cup ... \\cup F_n)^{+}\\) . For each \\(X_1...,X_n\\rightarrow Y \\in \\bar{F}\\) , create a new relation \\(R'(X_1,...,X_n,Y)\\) For example, recall the previous example \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\) . After the BCNF decomposition, we realize \\(R_1(A,D), R_{21}(B,C), R_{22}(A,B)\\) do not preserve \\(AB\\rightarrow C\\) . We create (or restore) \\(R_2(A,B,C)\\) . \\(R_2\\) subsumes \\(R_{21}(B,C)\\) and \\(R_{22}(A,B)\\) , hence we remove \\(R_{21}\\) and \\(R_{22}\\) and keep \\(R_2\\) . Alternatively, we could have used the BCNF algorithm but do not decompose \\(R_2\\) since it does not violate 3NF.","title":"Algorithm to compute 3NF"},{"location":"notes/l5_access_method/","text":"50.043 - Access Method Learning Outcomes By this end of this unit, you should be able to Describe the different layout options of a Heap Page Describe the different layout options of a Heap File Explain the purposes of a database index. Explain the different collision handling mechanisms of Hash Table Explain the structure of a B+ Tree Explain the lookup / insert / delete operations over a B+ Tree. Describe the difference between a clustered B+ Tree index and a unclustered B+ Tree index. Data Access Methods As discussed in the earlier classes, through SQL (and relational algebra), we scan data in a table, search for data based on filtering condition, update data, insert and delete data. We consider the actual implementation of these access methods. Like any other software implementation, we need to consider the data structure together with the algorithm. Page Internal Recall inside a data page, we find a set of tuples/records a header the index the log Let's consider the first two components. Layouts of Page There are few options of how tuples are organized in a page. Option 1 - Unordered sequential layout Tuples are stored inside a page, one after another, like a fixed size array. Elements inside the array, i.e. the pages, are unordered. We often call this page structue as a Heap Page . One advantage is that sequential layout is easy to implement, especially for sequential scanning, and insertion. The clear disadvantage is that after some tuples being deleted we see holes in between tuples the space utilization is unclear Option 2 - Heap Page with bit array header In this version we improve over the previous version of Heap Page by including a header. The header is an array of bits whose size equals to the maximum number of tuples in a page (with the meta-data discounted). The i-th bit is 0 implies the i-th tuple slot is unused, otherwise, the slot is used. With this approach we address the deletion issue. To insert a tuple, we need to scan through the header array to look for free slot. There are still some issues to be addressed. we have to assume that tuples are all in a single fixed size it is hard to search for a particular tuple except sequential scan. The first issue can be addressed by allowing long tuple spanning across multiple slots. We need to store extra header information. The second issue will be addressed altogther when we consider the internal layout of a database file File Internal From the earlier section, we note that a database file stores data belonging to a particular table. In the database file, we find a collection of pages. Layout of Files The issues related to layout of database files are similar to those with pages. We have a few options. Option 1 - Unordered sequential layout Simple approach. We store one page after another. The good news is that all pages are of the same size. A similar issue with this approach is that it is not easy to find pages with free space. Option 2 - Linked list In this approach, we maintain two linked lists in the file as meta data. One of the list stores all pages which are full. The other one stores all pages which are not yet full. Option 3 - Directory An alternative to the linked lists approach is to use a set of directories. Each directory contains a fixed set of slots. Each slot contains a pointer to (i.e. the location of) a page and a freespace bit. Directory slot may also point to another directory if expansion is required. Directories are small in size so that they could fit in the memory. As a file grows with more pages, a new directory could be created just like a data page. In the event that we would like to keep track of how much free space per page has, we could change the freespace bit into a freespace integer field. This change supports the need of having variable length tuple. Columnar Layout Another alternative we could consider is that instead of storing data as tuple, we could store them based on column, which is known as columnar storage. We omit the detail here and defer the discuss during the cohort lesson. A Quick Summary so far We have consider the internal layout of a page and the internal layout of a file. To maintain the free/occupied slots information, we maintain the meta data in pages, as well as files. It is clear that storing pages sequentially in a file has certain advantage in particular for sequential scan. The following table sumarizes the average cost of data operation performed on data file with sequential layout and one with random layout. Let \\(t_{r+s}\\) denote rotation and seek time, \\(t_{tf}\\) denote the transfer time and \\(D\\) denote the number of pages in the file. sequential random Insert a record \\(t_{s+r} + 2 \\cdot t_{tf}\\) \\(t_{s+r} + 2 \\cdot t_{tf}\\) Lookup a record \\(t_{s+r} + t_{tf} \\cdot D / 2\\) \\((t_{s+r} + t_{tf}) \\cdot D / 2\\) Insert a record \\(t_{s+r} + t_{tf} \\cdot D\\) \\((t_{s+r} + t_{tf}) \\cdot D\\) One issue yet to be addressed is to enable searching for a tuple without resort to sequential scanning which is expensive. Index A way to resolve to search issue with Heap file is to incorporate indices. An index maps values of the searching criteria into a set of record ids. Each record id stores the exact location of a tuple, i.e. page id and tuple slot number. A table may have multiple indices. The idea of indices is similar to adding labels to a thick phyiscal text book. The labels enable us to quickly \"jump\" to a section of interest. There many different options of index. Hash table A hash table maps values of searching criteria into the target set via a hash function. The input to the hash function is the value, and the output is the set of locations of the records. Desired Properties of a hash function The perfect hash function should be efficient - takes no time to compute the hash low (or better to be no) collision. For all \\(x_1 \\neq x_2\\) , \\(hash(x_1) \\neq hash(x_2)\\) . Dealing with collision In reality, collision can't be eliminated. To deal with collision, there are two approaches on the storage level Open Hashing (a.k.a. separate chaining) we store collided objects in linked-list pointed by the same key. In the above diagram the values 1000 and 9530 are hashed into the same cell of the hash table, hence 9530 is stored in a chain. Closed Hashing (a.k.a. open addressing) we try to store collided objects in the same hash table. We discuss a few more alternatives of closed hasing Linear Probing Linear Probing is a close hashing collision resolution. The idea is that if both values are hashed to the same cell of the hash table, one of them will be stored in the next available slot. In the diagram above, both values A and C are hased into the same cell of the hash table. When we try to insert C, we have to place C to the cell available right after the cell where A resides. First of all, for this to work, we need to know the max size of the table, in case the current collided cell is the last element of the table. Secondly the search time of the value given a key, is linear to the size of the hash table in the worst case. Cuckoo Hashing This hashing technique is given this name because it behaves like how parent birds feed their chicks. They often have multiple young chicks in the same nest. When feeding, they often forget which one had been fed before, so they just keep trying one by one. The cuckoo hashing operates with multiple hash functions ( \\(n\\) functions) and multiole hash tables ( \\(n\\) tables). It goes as follows, Give a value \\(v\\) to be stored, we hash it with all the hash functions. Given the hashed values \\(cell_1,...,cell_n\\) , we search for the first \\(cell_i\\) such that \\(cell_i\\) in \\(table_i\\) is free. If found, we put \\(v\\) at \\(cell_i\\) in \\(table_i\\) . otherwise, we pick one \\(cell_j\\) which is currently occupied by value \\(u\\) . we replace \\(u\\) by \\(v\\) we go to look for a new cell for \\(u\\) . (recursive call) We illustrate the idea using the following example. Let \\(n = 2\\) Step one we would like to insert \\(A\\) . We put \\(A\\) in table 1 at cell \\(H_1(A)\\) . Step two we want to insert \\(B\\) . We put \\(B\\) in table 2 at cell \\(H_2(B)\\) , because \\(H_1(B)\\) collides with \\(H_1(A)\\) . Step three we want to insert \\(C\\) . We realized that both \\(H_1(C)\\) and \\(H_2(C)\\) are occupied due to collision. We vacate \\(B\\) and insert \\(C\\) . We need to re-insert \\(B\\) . We put it at cell \\(H_1(B)\\) , and vacate \\(A\\) . We re-insert \\(A\\) at \\(H_2(A)\\) . The all values are in the hash table(s). The advantage of this approach is that the lookup cost is always \\(O(1)\\) . The downside is that we might run into infinite vacate-insert loop. We need to some mechanism to detect the loop, and rehash everything with a new set of hash functions, or add more tables (and new hash functions). Bucket Hashing One issue with Cuckoo Hashing is that eventually, we need to rehash everything. The idea behind Bucket Hashing (a.k.a extensible hashing) is to store hashed values in buckets (relative small fixed size arrays, so that the sequential search is not expensive). All buckets have the same size. use the \\(n\\) least significant bits (LSB) of the hashed value to decide in which bucket it should be placed. increase \\(n\\) and add new bucket gradually as some bucket becomes full. The Bucket Hashing algorithm starts with a global slot array, a bucket. It maintains a set of integer values of \\(n\\) LSB. \\(G\\) denotes the global value of \\(n\\) LSB, there is one and only one. \\(L\\) denotes the local values of \\(n\\) LSB, there is one \\(L\\) per bucket. For all buckets, \\(L \\leq G\\) . The algorithm start with some small numbers of \\(G\\) and \\(L\\) . Bucket Hashing Insertion For simplicity, we treat \\(X\\) the same as \\(hash(X)\\) . To insert a value \\(X\\) , lookup the bucket for \\(X\\) based on the last \\(G\\) bits of \\(X\\) . if the bucket \\(i\\) is not full, insert \\(X\\) there. otherwise if the bucket \\(i\\) having \\(L < G\\) add a new bucket \\(j\\) having same \\(L\\) as \\(i\\) , redistribute data from \\(i\\) to \\(i\\) and \\(j\\) . increase \\(L\\) for both buckets \\(i\\) and \\(j\\) . update the slot array otherwise double the slot array add a new bucket \\(j\\) having same \\(L\\) as \\(i\\) redistribute data from \\(i\\) to \\(i\\) and \\(j\\) . increase \\(L\\) for both buckets \\(i\\) and \\(j\\) . increase \\(G\\) update the slot array For example, we start with the following empty table First, we insert hashed values 00000 , 01100 and 11010 . Next, we insert a hashed value 10001 , but the bucket is full. We need to create a new bucket with \\(L=0\\) and redistribute the data, and update slot array, and increase the \\(L\\) values for both buckets. In the third step, we insert hashed values 01011 and 01111 . Both values go to the new bucket In the fourth step, we insert another value 01110 . Both the bucket store the 1-LSB as 0 is full. We add double the slot array, add a new bucket with \\(L=1\\) . We redistribute the data into the new bucket and update the references in the slot array. We set \\(G = 2\\) and increase \\(L\\) for the two affected buckets. Bucket Hashing Lookup To lookup a value \\(X\\) lookup slot array given \\(G\\) LSB of (hash of) \\(X\\) . if the bucket is found scan through the bucket for \\(X\\) sequentually otherwise, report not found Time complexity The insertion time and lookup time are both \\(O(1)\\) . B+ Tree One disadvantage of Hash table is that it only supports point query. For example, it works well with query select * from book where book_id = 'b1'; because we could hash book_id . Hashing does not help to speed up range search query. For instance, we want to search for books that are publish in the year of 2019. select * from book where date >= '2019-01-01' and date <= '2019-12-31'; Even if we hash all date , there is no guarantee the values are store in the same bucket/page/block. Recap Binary Search Tree A binary search tree is a binary tree in which all nodes from left sub-tree are smaller than the current node's value and all nodes from right sub-tree are larger than the current node's value. graph N0(\"10\")-->N1(\"7\") N0(\"10\")-->N2(\"20\") N1(\"7\")-->N3(\"5\") N1(\"7\")-->N4(\"9\") N2(\"20\")-->N5(\"15\") N2(\"20\")-->N6(\"26\") N3(\"5\")-->N7(\"1\") N3(\"5\")-->N8(\"6\") Recap a balanced tree A binary tree is balanced iff for any non-leaf node, n, the height difference between the left and the right sub-trees should not exceed 1. A binary tree is perfectly balanced iff for any non-leaf node, n, the height difference between the left and the right sub-trees should be 0. B+ Tree A B+ Tree is a generalization of a perfect balanced binary search tree, with the following adjustment Let \\(d\\) denotes the order. The root node may have \\(1\\) to \\(2*d\\) entries. Each non-root node may have \\(d\\) to \\(2*d\\) entries (half-full). The each in a node consist of a key and a value, except for the first entry i.e. \\(v_1, (k_2, v_2), (k_2, v_2), ... , (k_{n}, v_n)\\) . The values in non-leaf nodes are reference to the children. The values in the leaf nodes are reference to records in the data file/page/block. Given two consecutive entries \\((k_i, v_i), (k_{i+1}, v_{i+1})\\) , where \\(v_{i}\\) is a reference to a subtree, for all values in subtree, their index values must be in between \\(k_i\\) and \\(k_{i+1}\\) . For the first entry, its lower bound is definedby the key coming from the parent. (Similar observation applies to the last entry.) All the leaf nodes for a doublely linked list, which enables the range search operation. For example, the following figure shows a B+Tree with \\(d=2\\) Look-up To search for a value with key \\(k\\) in a B+ Tree, we start from the root node find the value \\(v\\) between \\(k_1\\) and \\(k_2\\) such that \\(k_1 \\leq k < k_2\\) . Note that \\(k_1\\) might not exist if \\(k_2\\) is the first entry's key, \\(k_2\\) might not exist if \\(k_1\\) is the last entry's key. if the current node is a non-leaf node, we move the current node to the node pointed by \\(v\\) , we repeat the process recursively if the current node is a leaf node, we return the disk data pointed by \\(v\\) . It is clear that the time complexity is \\(O(log(N))\\) Insertion To insert a value with key \\(k\\) into a B+ Tree, we follow the algorithm as follows find the leaf node \\(L\\) with the current interval where \\(k\\) should fit, (same as the look-up operation). insert_to_leaf( \\(L\\) , \\(k\\) ) def insert_to_leaf( \\(L\\) ,k) if \\(L\\) is not full, just insert, we are done! otherwise create a new node \\(L'\\) . (update linked list, only for leaf node), let \\(L'' = succ(L)\\) , then \\(succ(L) = L'\\) and \\(succ(L') = L''\\) . redistribute entries evenly between \\(L\\) and \\(L'\\) . copy up the middle key, with the value as a pointer to \\(L'\\) , that is to insert a new data entry in the parent node. insert_to_nonleaf(parent( \\(L\\) ), middle_key) def insert_to_nonleaf( \\(N\\) , k) if \\(N\\) is not full, just insert, we are done! otherwise create a ne wnode \\(N'\\) . redistribute entries evenly between \\(N\\) and \\(N'\\) . push up (note, not copy) the middle key, with the value as a pointer to \\(N'\\) . if \\(N\\) is not a root node, insert_to_nonleaf(parent(N), middle_key) otherwise, create an empty node as the new root node, insert_to_nonleaf(parent(N), middle_key) For example, we would like to insert the entry 8 into the following B+ Tree with \\(d=2\\) . For breivity we omitted the disk refernce associated with 8 . We locate the leaf node. However it is full, it has 4 entries, \\(4 = 2*d\\) . We create a new leaf node and redistribute the entries and re-join the linked list. We copy up the middle key to the parent. Now we need to insert 5 to the root node. The root node is full, we must create a new root child node, redistribute the entries, and create a new root node and push the middle key 17 to the new root node. The time complexity of the insertion algorithm is \\(O(log(N))\\) Deletion Given a node \\(N\\) , let \\(|N|\\) denote the number of entries in \\(N\\) . To delete a value with key \\(k\\) from a B+ Tree, we follow the algorithm as follows find the leaf node \\(L\\) with the current interval where \\(k\\) should fit, (same as the look-up operation). delete_from_leaf( \\(L\\) , \\(k\\) ) def delete_from_leaf( \\(L\\) , \\(k\\) ) remove the entry with \\(k\\) if \\(L\\) is at least half-full (i.e. \\(|L -\\{k\\}| \\geq d\\) ), we are done! otherwise if \\(L\\) has a sibling \\(L'\\) , and \\(k'\\) is the key from the parent that divides \\(L\\) and \\(L'\\) , such that \\(|L \\cup \\{k'\\} \\cup L'-\\{k\\}| \\geq 2*d\\) (Notes:if both left and right siblings having sufficient entries, we favor the left sibling) find the new middle key in \\(L \\cup \\{k'\\} \\cup L'-\\{k\\}\\) , say \\(k''\\) , replace \\(k'\\) by \\(k''\\) in \\(parent(L)\\) if \\(|L \\cup \\{k'\\} \\cup L'-\\{k\\}|-1 \\geq 2*d\\) re-distribute \\(L \\cup \\{k'\\} \\cup L' - \\{k,k''\\}\\) among the two leaf nodes. otherwise, re-distribute \\(L \\cup \\{k'\\} \\cup L'- \\{k\\}\\) among the two leaf nodes, i.e. \\(k''\\) is copied up. otherwise merge \\(L\\) with its sibling \\(L'\\) into a new node as \\(L \\cup \\{k'\\} \\cup L' - \\{k\\}\\) , where \\(k'\\) is the key from the parent dividing \\(L\\) and \\(L'\\) . delete_from_nonleaf( \\(parent(L)\\) , \\(k'\\) ) def delete_from_nonleaf( \\(N\\) , \\(k\\) ) remove the entry with \\(k\\) if \\(N\\) is a root node and \\(|N -\\{k\\}| > 0\\) , we are done! if \\(N\\) is a root node and \\(|N -\\{k\\}| == 0\\) , we remove \\(N\\) entirely. if \\(N\\) is not a root node and \\(N\\) is at least half full, we are done. otherwise if \\(N\\) has a sibling \\(N'\\) , and \\(k'\\) is the key from the parent that divides \\(N\\) and \\(N'\\) , such that \\(|N \\cup N' - \\{k\\}| \\geq 2*d\\) , find the new middle key in \\(N \\cup \\{k'\\} \\cup N'\\) , say \\(k''\\) , replace \\(k'\\) by \\(k''\\) in \\(parent(N)\\) , redistribute \\(|N \\cup N' - \\{k\\}|\\) among the two nodes. otherwise merge \\(N\\) with its sibling \\(N'\\) into a new node as \\(N \\cup \\{k'\\} \\cup N' - \\{k\\}\\) , where \\(k'\\) is the key from the parent dividing \\(N\\) and \\(N'\\) . delete_from_nonleaf( \\(parent(N)\\) , \\(k'\\) ) For example, continueing from what we have from the previous example, We delete the key 22 . Since the node is still half full, not merging or redistribution occurs. Next we delete key 20 , which makes the leaf node no longer half-full. We borrow entry 24 from the sibling on the right. The key 24 from the parent node is replaced by the new middle key 27 . In the third step, we delete key 24 . Merging the remaining entry 19 with 27 , 29 into a single leaf node. As a consequence, we need to delete 27 from the parent node. The merging effect cascade up, since the non-leaf node containing 30 is not half-full. The time complexity of deletion is also \\(O(log(N))\\) . Clustered vs Unclustered B+ Tree A B+ Tree is clustered iff data in the heap file are sorted according to the index attribute (keys in the B+ Tree). Due to its physical storage nature, there is only one clustered index per table. We may have many unclustered indices per table. Clustered index in general out-performs unclustered indices when being applied in range queries.","title":"50.043 - Access Method"},{"location":"notes/l5_access_method/#50043-access-method","text":"","title":"50.043 - Access Method"},{"location":"notes/l5_access_method/#learning-outcomes","text":"By this end of this unit, you should be able to Describe the different layout options of a Heap Page Describe the different layout options of a Heap File Explain the purposes of a database index. Explain the different collision handling mechanisms of Hash Table Explain the structure of a B+ Tree Explain the lookup / insert / delete operations over a B+ Tree. Describe the difference between a clustered B+ Tree index and a unclustered B+ Tree index.","title":"Learning Outcomes"},{"location":"notes/l5_access_method/#data-access-methods","text":"As discussed in the earlier classes, through SQL (and relational algebra), we scan data in a table, search for data based on filtering condition, update data, insert and delete data. We consider the actual implementation of these access methods. Like any other software implementation, we need to consider the data structure together with the algorithm.","title":"Data Access Methods"},{"location":"notes/l5_access_method/#page-internal","text":"Recall inside a data page, we find a set of tuples/records a header the index the log Let's consider the first two components.","title":"Page Internal"},{"location":"notes/l5_access_method/#layouts-of-page","text":"There are few options of how tuples are organized in a page.","title":"Layouts of Page"},{"location":"notes/l5_access_method/#option-1-unordered-sequential-layout","text":"Tuples are stored inside a page, one after another, like a fixed size array. Elements inside the array, i.e. the pages, are unordered. We often call this page structue as a Heap Page . One advantage is that sequential layout is easy to implement, especially for sequential scanning, and insertion. The clear disadvantage is that after some tuples being deleted we see holes in between tuples the space utilization is unclear","title":"Option 1 - Unordered sequential layout"},{"location":"notes/l5_access_method/#option-2-heap-page-with-bit-array-header","text":"In this version we improve over the previous version of Heap Page by including a header. The header is an array of bits whose size equals to the maximum number of tuples in a page (with the meta-data discounted). The i-th bit is 0 implies the i-th tuple slot is unused, otherwise, the slot is used. With this approach we address the deletion issue. To insert a tuple, we need to scan through the header array to look for free slot. There are still some issues to be addressed. we have to assume that tuples are all in a single fixed size it is hard to search for a particular tuple except sequential scan. The first issue can be addressed by allowing long tuple spanning across multiple slots. We need to store extra header information. The second issue will be addressed altogther when we consider the internal layout of a database file","title":"Option 2 - Heap Page with bit array header"},{"location":"notes/l5_access_method/#file-internal","text":"From the earlier section, we note that a database file stores data belonging to a particular table. In the database file, we find a collection of pages.","title":"File Internal"},{"location":"notes/l5_access_method/#layout-of-files","text":"The issues related to layout of database files are similar to those with pages. We have a few options.","title":"Layout of Files"},{"location":"notes/l5_access_method/#option-1-unordered-sequential-layout_1","text":"Simple approach. We store one page after another. The good news is that all pages are of the same size. A similar issue with this approach is that it is not easy to find pages with free space.","title":"Option 1 - Unordered sequential layout"},{"location":"notes/l5_access_method/#option-2-linked-list","text":"In this approach, we maintain two linked lists in the file as meta data. One of the list stores all pages which are full. The other one stores all pages which are not yet full.","title":"Option 2 - Linked list"},{"location":"notes/l5_access_method/#option-3-directory","text":"An alternative to the linked lists approach is to use a set of directories. Each directory contains a fixed set of slots. Each slot contains a pointer to (i.e. the location of) a page and a freespace bit. Directory slot may also point to another directory if expansion is required. Directories are small in size so that they could fit in the memory. As a file grows with more pages, a new directory could be created just like a data page. In the event that we would like to keep track of how much free space per page has, we could change the freespace bit into a freespace integer field. This change supports the need of having variable length tuple.","title":"Option 3 - Directory"},{"location":"notes/l5_access_method/#columnar-layout","text":"Another alternative we could consider is that instead of storing data as tuple, we could store them based on column, which is known as columnar storage. We omit the detail here and defer the discuss during the cohort lesson.","title":"Columnar Layout"},{"location":"notes/l5_access_method/#a-quick-summary-so-far","text":"We have consider the internal layout of a page and the internal layout of a file. To maintain the free/occupied slots information, we maintain the meta data in pages, as well as files. It is clear that storing pages sequentially in a file has certain advantage in particular for sequential scan. The following table sumarizes the average cost of data operation performed on data file with sequential layout and one with random layout. Let \\(t_{r+s}\\) denote rotation and seek time, \\(t_{tf}\\) denote the transfer time and \\(D\\) denote the number of pages in the file. sequential random Insert a record \\(t_{s+r} + 2 \\cdot t_{tf}\\) \\(t_{s+r} + 2 \\cdot t_{tf}\\) Lookup a record \\(t_{s+r} + t_{tf} \\cdot D / 2\\) \\((t_{s+r} + t_{tf}) \\cdot D / 2\\) Insert a record \\(t_{s+r} + t_{tf} \\cdot D\\) \\((t_{s+r} + t_{tf}) \\cdot D\\) One issue yet to be addressed is to enable searching for a tuple without resort to sequential scanning which is expensive.","title":"A Quick Summary so far"},{"location":"notes/l5_access_method/#index","text":"A way to resolve to search issue with Heap file is to incorporate indices. An index maps values of the searching criteria into a set of record ids. Each record id stores the exact location of a tuple, i.e. page id and tuple slot number. A table may have multiple indices. The idea of indices is similar to adding labels to a thick phyiscal text book. The labels enable us to quickly \"jump\" to a section of interest. There many different options of index.","title":"Index"},{"location":"notes/l5_access_method/#hash-table","text":"A hash table maps values of searching criteria into the target set via a hash function. The input to the hash function is the value, and the output is the set of locations of the records.","title":"Hash table"},{"location":"notes/l5_access_method/#desired-properties-of-a-hash-function","text":"The perfect hash function should be efficient - takes no time to compute the hash low (or better to be no) collision. For all \\(x_1 \\neq x_2\\) , \\(hash(x_1) \\neq hash(x_2)\\) .","title":"Desired Properties of a hash function"},{"location":"notes/l5_access_method/#dealing-with-collision","text":"In reality, collision can't be eliminated. To deal with collision, there are two approaches on the storage level Open Hashing (a.k.a. separate chaining) we store collided objects in linked-list pointed by the same key. In the above diagram the values 1000 and 9530 are hashed into the same cell of the hash table, hence 9530 is stored in a chain. Closed Hashing (a.k.a. open addressing) we try to store collided objects in the same hash table. We discuss a few more alternatives of closed hasing","title":"Dealing with collision"},{"location":"notes/l5_access_method/#linear-probing","text":"Linear Probing is a close hashing collision resolution. The idea is that if both values are hashed to the same cell of the hash table, one of them will be stored in the next available slot. In the diagram above, both values A and C are hased into the same cell of the hash table. When we try to insert C, we have to place C to the cell available right after the cell where A resides. First of all, for this to work, we need to know the max size of the table, in case the current collided cell is the last element of the table. Secondly the search time of the value given a key, is linear to the size of the hash table in the worst case.","title":"Linear Probing"},{"location":"notes/l5_access_method/#cuckoo-hashing","text":"This hashing technique is given this name because it behaves like how parent birds feed their chicks. They often have multiple young chicks in the same nest. When feeding, they often forget which one had been fed before, so they just keep trying one by one. The cuckoo hashing operates with multiple hash functions ( \\(n\\) functions) and multiole hash tables ( \\(n\\) tables). It goes as follows, Give a value \\(v\\) to be stored, we hash it with all the hash functions. Given the hashed values \\(cell_1,...,cell_n\\) , we search for the first \\(cell_i\\) such that \\(cell_i\\) in \\(table_i\\) is free. If found, we put \\(v\\) at \\(cell_i\\) in \\(table_i\\) . otherwise, we pick one \\(cell_j\\) which is currently occupied by value \\(u\\) . we replace \\(u\\) by \\(v\\) we go to look for a new cell for \\(u\\) . (recursive call) We illustrate the idea using the following example. Let \\(n = 2\\) Step one we would like to insert \\(A\\) . We put \\(A\\) in table 1 at cell \\(H_1(A)\\) . Step two we want to insert \\(B\\) . We put \\(B\\) in table 2 at cell \\(H_2(B)\\) , because \\(H_1(B)\\) collides with \\(H_1(A)\\) . Step three we want to insert \\(C\\) . We realized that both \\(H_1(C)\\) and \\(H_2(C)\\) are occupied due to collision. We vacate \\(B\\) and insert \\(C\\) . We need to re-insert \\(B\\) . We put it at cell \\(H_1(B)\\) , and vacate \\(A\\) . We re-insert \\(A\\) at \\(H_2(A)\\) . The all values are in the hash table(s). The advantage of this approach is that the lookup cost is always \\(O(1)\\) . The downside is that we might run into infinite vacate-insert loop. We need to some mechanism to detect the loop, and rehash everything with a new set of hash functions, or add more tables (and new hash functions).","title":"Cuckoo Hashing"},{"location":"notes/l5_access_method/#bucket-hashing","text":"One issue with Cuckoo Hashing is that eventually, we need to rehash everything. The idea behind Bucket Hashing (a.k.a extensible hashing) is to store hashed values in buckets (relative small fixed size arrays, so that the sequential search is not expensive). All buckets have the same size. use the \\(n\\) least significant bits (LSB) of the hashed value to decide in which bucket it should be placed. increase \\(n\\) and add new bucket gradually as some bucket becomes full. The Bucket Hashing algorithm starts with a global slot array, a bucket. It maintains a set of integer values of \\(n\\) LSB. \\(G\\) denotes the global value of \\(n\\) LSB, there is one and only one. \\(L\\) denotes the local values of \\(n\\) LSB, there is one \\(L\\) per bucket. For all buckets, \\(L \\leq G\\) . The algorithm start with some small numbers of \\(G\\) and \\(L\\) .","title":"Bucket Hashing"},{"location":"notes/l5_access_method/#bucket-hashing-insertion","text":"For simplicity, we treat \\(X\\) the same as \\(hash(X)\\) . To insert a value \\(X\\) , lookup the bucket for \\(X\\) based on the last \\(G\\) bits of \\(X\\) . if the bucket \\(i\\) is not full, insert \\(X\\) there. otherwise if the bucket \\(i\\) having \\(L < G\\) add a new bucket \\(j\\) having same \\(L\\) as \\(i\\) , redistribute data from \\(i\\) to \\(i\\) and \\(j\\) . increase \\(L\\) for both buckets \\(i\\) and \\(j\\) . update the slot array otherwise double the slot array add a new bucket \\(j\\) having same \\(L\\) as \\(i\\) redistribute data from \\(i\\) to \\(i\\) and \\(j\\) . increase \\(L\\) for both buckets \\(i\\) and \\(j\\) . increase \\(G\\) update the slot array For example, we start with the following empty table First, we insert hashed values 00000 , 01100 and 11010 . Next, we insert a hashed value 10001 , but the bucket is full. We need to create a new bucket with \\(L=0\\) and redistribute the data, and update slot array, and increase the \\(L\\) values for both buckets. In the third step, we insert hashed values 01011 and 01111 . Both values go to the new bucket In the fourth step, we insert another value 01110 . Both the bucket store the 1-LSB as 0 is full. We add double the slot array, add a new bucket with \\(L=1\\) . We redistribute the data into the new bucket and update the references in the slot array. We set \\(G = 2\\) and increase \\(L\\) for the two affected buckets.","title":"Bucket Hashing Insertion"},{"location":"notes/l5_access_method/#bucket-hashing-lookup","text":"To lookup a value \\(X\\) lookup slot array given \\(G\\) LSB of (hash of) \\(X\\) . if the bucket is found scan through the bucket for \\(X\\) sequentually otherwise, report not found","title":"Bucket Hashing Lookup"},{"location":"notes/l5_access_method/#time-complexity","text":"The insertion time and lookup time are both \\(O(1)\\) .","title":"Time complexity"},{"location":"notes/l5_access_method/#b-tree","text":"One disadvantage of Hash table is that it only supports point query. For example, it works well with query select * from book where book_id = 'b1'; because we could hash book_id . Hashing does not help to speed up range search query. For instance, we want to search for books that are publish in the year of 2019. select * from book where date >= '2019-01-01' and date <= '2019-12-31'; Even if we hash all date , there is no guarantee the values are store in the same bucket/page/block.","title":"B+ Tree"},{"location":"notes/l5_access_method/#recap-binary-search-tree","text":"A binary search tree is a binary tree in which all nodes from left sub-tree are smaller than the current node's value and all nodes from right sub-tree are larger than the current node's value. graph N0(\"10\")-->N1(\"7\") N0(\"10\")-->N2(\"20\") N1(\"7\")-->N3(\"5\") N1(\"7\")-->N4(\"9\") N2(\"20\")-->N5(\"15\") N2(\"20\")-->N6(\"26\") N3(\"5\")-->N7(\"1\") N3(\"5\")-->N8(\"6\")","title":"Recap Binary Search Tree"},{"location":"notes/l5_access_method/#recap-a-balanced-tree","text":"A binary tree is balanced iff for any non-leaf node, n, the height difference between the left and the right sub-trees should not exceed 1. A binary tree is perfectly balanced iff for any non-leaf node, n, the height difference between the left and the right sub-trees should be 0.","title":"Recap a balanced tree"},{"location":"notes/l5_access_method/#b-tree_1","text":"A B+ Tree is a generalization of a perfect balanced binary search tree, with the following adjustment Let \\(d\\) denotes the order. The root node may have \\(1\\) to \\(2*d\\) entries. Each non-root node may have \\(d\\) to \\(2*d\\) entries (half-full). The each in a node consist of a key and a value, except for the first entry i.e. \\(v_1, (k_2, v_2), (k_2, v_2), ... , (k_{n}, v_n)\\) . The values in non-leaf nodes are reference to the children. The values in the leaf nodes are reference to records in the data file/page/block. Given two consecutive entries \\((k_i, v_i), (k_{i+1}, v_{i+1})\\) , where \\(v_{i}\\) is a reference to a subtree, for all values in subtree, their index values must be in between \\(k_i\\) and \\(k_{i+1}\\) . For the first entry, its lower bound is definedby the key coming from the parent. (Similar observation applies to the last entry.) All the leaf nodes for a doublely linked list, which enables the range search operation. For example, the following figure shows a B+Tree with \\(d=2\\)","title":"B+ Tree"},{"location":"notes/l5_access_method/#look-up","text":"To search for a value with key \\(k\\) in a B+ Tree, we start from the root node find the value \\(v\\) between \\(k_1\\) and \\(k_2\\) such that \\(k_1 \\leq k < k_2\\) . Note that \\(k_1\\) might not exist if \\(k_2\\) is the first entry's key, \\(k_2\\) might not exist if \\(k_1\\) is the last entry's key. if the current node is a non-leaf node, we move the current node to the node pointed by \\(v\\) , we repeat the process recursively if the current node is a leaf node, we return the disk data pointed by \\(v\\) . It is clear that the time complexity is \\(O(log(N))\\)","title":"Look-up"},{"location":"notes/l5_access_method/#insertion","text":"To insert a value with key \\(k\\) into a B+ Tree, we follow the algorithm as follows find the leaf node \\(L\\) with the current interval where \\(k\\) should fit, (same as the look-up operation). insert_to_leaf( \\(L\\) , \\(k\\) ) def insert_to_leaf( \\(L\\) ,k) if \\(L\\) is not full, just insert, we are done! otherwise create a new node \\(L'\\) . (update linked list, only for leaf node), let \\(L'' = succ(L)\\) , then \\(succ(L) = L'\\) and \\(succ(L') = L''\\) . redistribute entries evenly between \\(L\\) and \\(L'\\) . copy up the middle key, with the value as a pointer to \\(L'\\) , that is to insert a new data entry in the parent node. insert_to_nonleaf(parent( \\(L\\) ), middle_key) def insert_to_nonleaf( \\(N\\) , k) if \\(N\\) is not full, just insert, we are done! otherwise create a ne wnode \\(N'\\) . redistribute entries evenly between \\(N\\) and \\(N'\\) . push up (note, not copy) the middle key, with the value as a pointer to \\(N'\\) . if \\(N\\) is not a root node, insert_to_nonleaf(parent(N), middle_key) otherwise, create an empty node as the new root node, insert_to_nonleaf(parent(N), middle_key) For example, we would like to insert the entry 8 into the following B+ Tree with \\(d=2\\) . For breivity we omitted the disk refernce associated with 8 . We locate the leaf node. However it is full, it has 4 entries, \\(4 = 2*d\\) . We create a new leaf node and redistribute the entries and re-join the linked list. We copy up the middle key to the parent. Now we need to insert 5 to the root node. The root node is full, we must create a new root child node, redistribute the entries, and create a new root node and push the middle key 17 to the new root node. The time complexity of the insertion algorithm is \\(O(log(N))\\)","title":"Insertion"},{"location":"notes/l5_access_method/#deletion","text":"Given a node \\(N\\) , let \\(|N|\\) denote the number of entries in \\(N\\) . To delete a value with key \\(k\\) from a B+ Tree, we follow the algorithm as follows find the leaf node \\(L\\) with the current interval where \\(k\\) should fit, (same as the look-up operation). delete_from_leaf( \\(L\\) , \\(k\\) ) def delete_from_leaf( \\(L\\) , \\(k\\) ) remove the entry with \\(k\\) if \\(L\\) is at least half-full (i.e. \\(|L -\\{k\\}| \\geq d\\) ), we are done! otherwise if \\(L\\) has a sibling \\(L'\\) , and \\(k'\\) is the key from the parent that divides \\(L\\) and \\(L'\\) , such that \\(|L \\cup \\{k'\\} \\cup L'-\\{k\\}| \\geq 2*d\\) (Notes:if both left and right siblings having sufficient entries, we favor the left sibling) find the new middle key in \\(L \\cup \\{k'\\} \\cup L'-\\{k\\}\\) , say \\(k''\\) , replace \\(k'\\) by \\(k''\\) in \\(parent(L)\\) if \\(|L \\cup \\{k'\\} \\cup L'-\\{k\\}|-1 \\geq 2*d\\) re-distribute \\(L \\cup \\{k'\\} \\cup L' - \\{k,k''\\}\\) among the two leaf nodes. otherwise, re-distribute \\(L \\cup \\{k'\\} \\cup L'- \\{k\\}\\) among the two leaf nodes, i.e. \\(k''\\) is copied up. otherwise merge \\(L\\) with its sibling \\(L'\\) into a new node as \\(L \\cup \\{k'\\} \\cup L' - \\{k\\}\\) , where \\(k'\\) is the key from the parent dividing \\(L\\) and \\(L'\\) . delete_from_nonleaf( \\(parent(L)\\) , \\(k'\\) ) def delete_from_nonleaf( \\(N\\) , \\(k\\) ) remove the entry with \\(k\\) if \\(N\\) is a root node and \\(|N -\\{k\\}| > 0\\) , we are done! if \\(N\\) is a root node and \\(|N -\\{k\\}| == 0\\) , we remove \\(N\\) entirely. if \\(N\\) is not a root node and \\(N\\) is at least half full, we are done. otherwise if \\(N\\) has a sibling \\(N'\\) , and \\(k'\\) is the key from the parent that divides \\(N\\) and \\(N'\\) , such that \\(|N \\cup N' - \\{k\\}| \\geq 2*d\\) , find the new middle key in \\(N \\cup \\{k'\\} \\cup N'\\) , say \\(k''\\) , replace \\(k'\\) by \\(k''\\) in \\(parent(N)\\) , redistribute \\(|N \\cup N' - \\{k\\}|\\) among the two nodes. otherwise merge \\(N\\) with its sibling \\(N'\\) into a new node as \\(N \\cup \\{k'\\} \\cup N' - \\{k\\}\\) , where \\(k'\\) is the key from the parent dividing \\(N\\) and \\(N'\\) . delete_from_nonleaf( \\(parent(N)\\) , \\(k'\\) ) For example, continueing from what we have from the previous example, We delete the key 22 . Since the node is still half full, not merging or redistribution occurs. Next we delete key 20 , which makes the leaf node no longer half-full. We borrow entry 24 from the sibling on the right. The key 24 from the parent node is replaced by the new middle key 27 . In the third step, we delete key 24 . Merging the remaining entry 19 with 27 , 29 into a single leaf node. As a consequence, we need to delete 27 from the parent node. The merging effect cascade up, since the non-leaf node containing 30 is not half-full. The time complexity of deletion is also \\(O(log(N))\\) .","title":"Deletion"},{"location":"notes/l5_access_method/#clustered-vs-unclustered-b-tree","text":"A B+ Tree is clustered iff data in the heap file are sorted according to the index attribute (keys in the B+ Tree). Due to its physical storage nature, there is only one clustered index per table. We may have many unclustered indices per table. Clustered index in general out-performs unclustered indices when being applied in range queries.","title":"Clustered vs Unclustered B+ Tree"},{"location":"notes/l5_storage/","text":"50.043 - Storage Learning Outcomes By this end of this unit you should be able to Explain the internal structure a hard disk Compute the estimated cost for a data operation Explain the roles and functionality of a disk manager Explain the roles and functionality of a Buffer Pool Describe the LRU and Clock Replacement Policy Motivation Starting from this week onwards, we look deep into the internal of the database manage systems. In particular, we want to find out is how queries are executed and how data are fetched from and updated back to the database. The answer the question, we first need to understand how data are stored in the database. Hardware For simplicity, we only consider the mechanincal hard disk storage. We do not consider in memory database because it is not common, due to its cost. We do not consider SSD and non-volatile memory because they similar to hard disk storage except that there is no rotational delay and seek time delay is marginal, but the general cost computation model is still applicable. Mechanical Rotational Hard disk (image from Database Management Systems, R. RamaKrishnan and J. Gehrke) The figure above illustrate the internal of a mechanical hard disk, which is a cylindrical structure. It consists of multiple platter of disks stacked up vertically. Each platter consists of multiple tracks. Each track is divided into multiple sectors. Disk heads read from/write to disk track. Only one head is allowed to operate at any point in time. Data are organized into blocks (which is determined by the governing system, DBMS storage or OS file system). A block might span across multiple consecutive sectors. Different system use different block size. We break down the time needed for each type of disk head movement that related to disk I/O. Seek time \\(t_s\\) : to move arm to correct platters & track\u200b, it is roughly 20ms\u200b. Rotate time \\(t_r\\) : to rotate to correct sector\u200b, roughly 10ms\u200b Transfer time \\(t_{tf}\\) : to read/write a block of data\u200b, 0.1ms\u200b Random vs sequential access cost Given \\(D\\) number of blocks of data, the cost of accessing the data randomly is \\[ T_1 = D * (t_s + t_r + t_{tf}) \\] and the cost of accessing the data sequentially is \\[ T_2 = t_s+t_r + D * t_{tf} \\] Note For SSD, seek time is a lot faster and there is zero rotate time. Transfer time is also magnitude faster. But the basic components of the breakdown remain the same. Typically we see significant improvement in random access time when upgrading to SSD. Database Storage Given the estimated cost of data access, it is clear that we should minimize the factor being multipled with \\(t_s\\) and \\(t_r\\) as much as we can. For most the DBMS storage system, we favor a larger block size as it reduces the number of blocks given the same set of data in size and data related to each other tend to be fetched together in one block. When some records (or tuples) are needed, the entire block of data is read from the disk. Same idea applies when the records needed to be written back to the disk. To minimize the overhead of reading/writing the data blocks back and forth and to boost the performance of the data processing, the DBMS need to 1. have some logical mapping over the physical storage - disk manager 2. have some in memory cache - buffer pool Disk Manager Different DBMSes have different ways of managing the logical mapping. One common approach is to store data as a collection of files. The disk manager keeps track of the mapping from database tables to files, which is known as the system catalogue . These database files are no ordinary ones which can be accessed by other software in the OS. They have to be access through the Disk Manager. Each file managed by the Disk Manager is organized as a set of page s. Page vs Block Page is the basic unit of data storage from the memory (RAM) perspective, determined by the DBMS configuration. Block is the basic unit of data storage from the physical disk (hard disk) perspective, determined by the OS configuration. These two might or might not be in sync in terms of size. From now onwards, for the ease of reasoning, we assume a page is of the same size of a block unless we specifically define otherwise. What is Inside a Page Besides a set of tuples/records, a page contains the meta information (also known as the header) the index and the log we will defer the discussion until the next unit. Cache system - Buffer Pool Knowing that fetching data block (or page) to and fro is expensive. We need a fixed set of working space in the RAM, which serves as the Buffer Pool. Any read operation from the DBMS must check whether the target page is in the Buffer Pool, if it is in the pool, the operation should be performed on the cached copy. When the cached copy needs to be evicted, it will be only be written to the disk if it is dirty , i.e. some data has changed. Otherwise, it will be discarded. The Buffer Pool has a fixed set of slots (known as frames). Each page is loaded into a frame when it is fetched from the disk. The Buffer Pool maintains the following information, Page to frame mapping Whether a page is pinned . A page is pinned means it is being accessed currently. When a data operation (query/insert/update/delete) is performed it will pin all the required pages. When the operation is completed, the pages are unpinned. Note that in a full scale system, we might see concurrent accesses to pages, thus, in most of the situation, the Buffer Pool maintains a pin counter per slot. Whether a page is dirty. Extra information to support the eviction policy. Eviction Policy When a new page needs to be fetched from the disk but the Buffer Pool is full, we need to free up some slot. An eviction policy helps to decide which frame slot should be freed. In the ideal situation, the (dream) eviction policy should evict the cached page requested farthest in the future, because we won't need it again so soon compared to the rest. In reality, such policy does not exist as we can't see the future. Instead, we use several policies based on the situation LRU Policy The Least Recently Used (LRU) Policy works as follows, pinned frames should not be evicted keep track of the timestamp when frame's pin was set to 0. evict the page in the frame with the smallest (oldest) timestamp. In details steps. Initialize all frames to be empty with pincount 0 when a page is being pinned if the page is already in the pool, increase the pincount, return the page in the frame. otherwise, find the frames with 0 pincount, find the one oldest timestamp, write it to disk if it is dirty. load the new page to this vacant frame, set pincount = 1 when a page is being unpinned, decrease the pincount and update the timestamp to the current time. Example For example, consider the a buffer pool with 3 frame slots. The folloing sequence of pages are requested in order (pin, then unpin immediately) \\[ 1,2,3,4,1,2 \\] pg = page, pc = pincount, ts = timestamp time page req'ed frame 0 frame 1 frame 2 1 1 pg:1, pc:0, ts: 1 2 2 pg:1, pc:0, ts: 1 pg:2, pc:0, ts:2 3 3 pg:1, pc:0, ts: 1 pg:2, pc:0, ts:2 pg:3, pc:0, ts:3 4 4 pg:4, pc:0, ts: 4 pg:2, pc:0, ts:2 pg:3, pc:0, ts:3 5 1 pg:4, pc:0, ts: 4 pg:1, pc:0, ts:5 pg:3, pc:0, ts:3 6 2 pg:4, pc:0, ts: 4 pg:1, pc:0, ts:5 pg:2, pc:0, ts:6 Clock Replacement Policy While LRU works, its implementation could be less complex. The Clock Replacement Policy is a simpler approach which approximates LRU. (The formal proof is not discussed in this module.) The Clock Replacement Policy does keep track of timestamps, instead it keep track of the reference bit (0 or 1) per frame and a hand (index to frame slots). During the initialization phase, all frames are empty with pincount and reference bit set to 0, the hand is set to 0. The Clock Replacement Policy works as follows, when a page is being pinned if the page is already in the pool, set the frame's reference bit to 1, increase the pincount, return the page in the frame. otherwise, we need to find a slot for it, by checking the frame pointed by the hand if it is pinned, (somebody is using it), advance hand to the next frame otherwise if the frame's reference bit is 1, set it to 0, advance hand otherwise write the current payload to disk if it is dirty load the new page to this slot set reference bit to 1, and set pincount to 1 advance the hand to the next frame when a page is being unpinned, decrease the pincount. Compared to LRU, the Clock Replacement Policy uses less memory, reference bit vs timestamp. The Clock Replacement Policy is more efficient, because there is no need to search for the oldest timestamp. Example Let's re-run the same example \\(1,2,3,4,1,2\\) with Clock Replacement Policy pg = page, pc = pincount, ref = reference bit time page req'ed frame 0 frame 1 frame 2 hand 1 1 pg:1, pc:0, ref: 1 1 2 2 pg:1, pc:0, ref: 1 pg:2, pc:0, ref: 1 2 3 3 pg:1, pc:0, ref: 1 pg:2, pc:0, ref: 1 pg:3, pc:0, ref: 1 0 4 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:1 pg:3, pc:0, ref:1 1 5 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:0 pg:3, pc:0, ref:1 2 6 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:0 pg:3, pc:0, ref:0 0 7 4 pg:4, pc:0, ref: 1 pg:2, pc:0, ref:0 pg:3, pc:0, ref:0 1 8 1 pg:4, pc:0, ref: 1 pg:1, pc:0, ref:1 pg:3, pc:0, ref:0 2 9 2 pg:4, pc:0, ref: 1 pg:1, pc:0, ref:1 pg:2, pc:0, ref:1 0 Other optimization There is no one size fit all solution here. Modern databases often employ some heuristics, called prefetching. The idea is to anticipate subsequent pages will soon be needed when an operation requests for a page (say, a sequential scan of table). This eliminates some of the cost of rotation time and seek time.","title":"50.043 - Storage"},{"location":"notes/l5_storage/#50043-storage","text":"","title":"50.043 - Storage"},{"location":"notes/l5_storage/#learning-outcomes","text":"By this end of this unit you should be able to Explain the internal structure a hard disk Compute the estimated cost for a data operation Explain the roles and functionality of a disk manager Explain the roles and functionality of a Buffer Pool Describe the LRU and Clock Replacement Policy","title":"Learning Outcomes"},{"location":"notes/l5_storage/#motivation","text":"Starting from this week onwards, we look deep into the internal of the database manage systems. In particular, we want to find out is how queries are executed and how data are fetched from and updated back to the database. The answer the question, we first need to understand how data are stored in the database.","title":"Motivation"},{"location":"notes/l5_storage/#hardware","text":"For simplicity, we only consider the mechanincal hard disk storage. We do not consider in memory database because it is not common, due to its cost. We do not consider SSD and non-volatile memory because they similar to hard disk storage except that there is no rotational delay and seek time delay is marginal, but the general cost computation model is still applicable.","title":"Hardware"},{"location":"notes/l5_storage/#mechanical-rotational-hard-disk","text":"(image from Database Management Systems, R. RamaKrishnan and J. Gehrke) The figure above illustrate the internal of a mechanical hard disk, which is a cylindrical structure. It consists of multiple platter of disks stacked up vertically. Each platter consists of multiple tracks. Each track is divided into multiple sectors. Disk heads read from/write to disk track. Only one head is allowed to operate at any point in time. Data are organized into blocks (which is determined by the governing system, DBMS storage or OS file system). A block might span across multiple consecutive sectors. Different system use different block size. We break down the time needed for each type of disk head movement that related to disk I/O. Seek time \\(t_s\\) : to move arm to correct platters & track\u200b, it is roughly 20ms\u200b. Rotate time \\(t_r\\) : to rotate to correct sector\u200b, roughly 10ms\u200b Transfer time \\(t_{tf}\\) : to read/write a block of data\u200b, 0.1ms\u200b","title":"Mechanical Rotational Hard disk"},{"location":"notes/l5_storage/#random-vs-sequential-access-cost","text":"Given \\(D\\) number of blocks of data, the cost of accessing the data randomly is \\[ T_1 = D * (t_s + t_r + t_{tf}) \\] and the cost of accessing the data sequentially is \\[ T_2 = t_s+t_r + D * t_{tf} \\]","title":"Random vs sequential access cost"},{"location":"notes/l5_storage/#note","text":"For SSD, seek time is a lot faster and there is zero rotate time. Transfer time is also magnitude faster. But the basic components of the breakdown remain the same. Typically we see significant improvement in random access time when upgrading to SSD.","title":"Note"},{"location":"notes/l5_storage/#database-storage","text":"Given the estimated cost of data access, it is clear that we should minimize the factor being multipled with \\(t_s\\) and \\(t_r\\) as much as we can. For most the DBMS storage system, we favor a larger block size as it reduces the number of blocks given the same set of data in size and data related to each other tend to be fetched together in one block. When some records (or tuples) are needed, the entire block of data is read from the disk. Same idea applies when the records needed to be written back to the disk. To minimize the overhead of reading/writing the data blocks back and forth and to boost the performance of the data processing, the DBMS need to 1. have some logical mapping over the physical storage - disk manager 2. have some in memory cache - buffer pool","title":"Database Storage"},{"location":"notes/l5_storage/#disk-manager","text":"Different DBMSes have different ways of managing the logical mapping. One common approach is to store data as a collection of files. The disk manager keeps track of the mapping from database tables to files, which is known as the system catalogue . These database files are no ordinary ones which can be accessed by other software in the OS. They have to be access through the Disk Manager. Each file managed by the Disk Manager is organized as a set of page s.","title":"Disk Manager"},{"location":"notes/l5_storage/#page-vs-block","text":"Page is the basic unit of data storage from the memory (RAM) perspective, determined by the DBMS configuration. Block is the basic unit of data storage from the physical disk (hard disk) perspective, determined by the OS configuration. These two might or might not be in sync in terms of size. From now onwards, for the ease of reasoning, we assume a page is of the same size of a block unless we specifically define otherwise.","title":"Page vs Block"},{"location":"notes/l5_storage/#what-is-inside-a-page","text":"Besides a set of tuples/records, a page contains the meta information (also known as the header) the index and the log we will defer the discussion until the next unit.","title":"What is Inside a Page"},{"location":"notes/l5_storage/#cache-system-buffer-pool","text":"Knowing that fetching data block (or page) to and fro is expensive. We need a fixed set of working space in the RAM, which serves as the Buffer Pool. Any read operation from the DBMS must check whether the target page is in the Buffer Pool, if it is in the pool, the operation should be performed on the cached copy. When the cached copy needs to be evicted, it will be only be written to the disk if it is dirty , i.e. some data has changed. Otherwise, it will be discarded. The Buffer Pool has a fixed set of slots (known as frames). Each page is loaded into a frame when it is fetched from the disk. The Buffer Pool maintains the following information, Page to frame mapping Whether a page is pinned . A page is pinned means it is being accessed currently. When a data operation (query/insert/update/delete) is performed it will pin all the required pages. When the operation is completed, the pages are unpinned. Note that in a full scale system, we might see concurrent accesses to pages, thus, in most of the situation, the Buffer Pool maintains a pin counter per slot. Whether a page is dirty. Extra information to support the eviction policy.","title":"Cache system - Buffer Pool"},{"location":"notes/l5_storage/#eviction-policy","text":"When a new page needs to be fetched from the disk but the Buffer Pool is full, we need to free up some slot. An eviction policy helps to decide which frame slot should be freed. In the ideal situation, the (dream) eviction policy should evict the cached page requested farthest in the future, because we won't need it again so soon compared to the rest. In reality, such policy does not exist as we can't see the future. Instead, we use several policies based on the situation","title":"Eviction Policy"},{"location":"notes/l5_storage/#lru-policy","text":"The Least Recently Used (LRU) Policy works as follows, pinned frames should not be evicted keep track of the timestamp when frame's pin was set to 0. evict the page in the frame with the smallest (oldest) timestamp. In details steps. Initialize all frames to be empty with pincount 0 when a page is being pinned if the page is already in the pool, increase the pincount, return the page in the frame. otherwise, find the frames with 0 pincount, find the one oldest timestamp, write it to disk if it is dirty. load the new page to this vacant frame, set pincount = 1 when a page is being unpinned, decrease the pincount and update the timestamp to the current time.","title":"LRU Policy"},{"location":"notes/l5_storage/#example","text":"For example, consider the a buffer pool with 3 frame slots. The folloing sequence of pages are requested in order (pin, then unpin immediately) \\[ 1,2,3,4,1,2 \\] pg = page, pc = pincount, ts = timestamp time page req'ed frame 0 frame 1 frame 2 1 1 pg:1, pc:0, ts: 1 2 2 pg:1, pc:0, ts: 1 pg:2, pc:0, ts:2 3 3 pg:1, pc:0, ts: 1 pg:2, pc:0, ts:2 pg:3, pc:0, ts:3 4 4 pg:4, pc:0, ts: 4 pg:2, pc:0, ts:2 pg:3, pc:0, ts:3 5 1 pg:4, pc:0, ts: 4 pg:1, pc:0, ts:5 pg:3, pc:0, ts:3 6 2 pg:4, pc:0, ts: 4 pg:1, pc:0, ts:5 pg:2, pc:0, ts:6","title":"Example"},{"location":"notes/l5_storage/#clock-replacement-policy","text":"While LRU works, its implementation could be less complex. The Clock Replacement Policy is a simpler approach which approximates LRU. (The formal proof is not discussed in this module.) The Clock Replacement Policy does keep track of timestamps, instead it keep track of the reference bit (0 or 1) per frame and a hand (index to frame slots). During the initialization phase, all frames are empty with pincount and reference bit set to 0, the hand is set to 0. The Clock Replacement Policy works as follows, when a page is being pinned if the page is already in the pool, set the frame's reference bit to 1, increase the pincount, return the page in the frame. otherwise, we need to find a slot for it, by checking the frame pointed by the hand if it is pinned, (somebody is using it), advance hand to the next frame otherwise if the frame's reference bit is 1, set it to 0, advance hand otherwise write the current payload to disk if it is dirty load the new page to this slot set reference bit to 1, and set pincount to 1 advance the hand to the next frame when a page is being unpinned, decrease the pincount. Compared to LRU, the Clock Replacement Policy uses less memory, reference bit vs timestamp. The Clock Replacement Policy is more efficient, because there is no need to search for the oldest timestamp.","title":"Clock Replacement Policy"},{"location":"notes/l5_storage/#example_1","text":"Let's re-run the same example \\(1,2,3,4,1,2\\) with Clock Replacement Policy pg = page, pc = pincount, ref = reference bit time page req'ed frame 0 frame 1 frame 2 hand 1 1 pg:1, pc:0, ref: 1 1 2 2 pg:1, pc:0, ref: 1 pg:2, pc:0, ref: 1 2 3 3 pg:1, pc:0, ref: 1 pg:2, pc:0, ref: 1 pg:3, pc:0, ref: 1 0 4 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:1 pg:3, pc:0, ref:1 1 5 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:0 pg:3, pc:0, ref:1 2 6 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:0 pg:3, pc:0, ref:0 0 7 4 pg:4, pc:0, ref: 1 pg:2, pc:0, ref:0 pg:3, pc:0, ref:0 1 8 1 pg:4, pc:0, ref: 1 pg:1, pc:0, ref:1 pg:3, pc:0, ref:0 2 9 2 pg:4, pc:0, ref: 1 pg:1, pc:0, ref:1 pg:2, pc:0, ref:1 0","title":"Example"},{"location":"notes/l5_storage/#other-optimization","text":"There is no one size fit all solution here. Modern databases often employ some heuristics, called prefetching. The idea is to anticipate subsequent pages will soon be needed when an operation requests for a page (say, a sequential scan of table). This eliminates some of the cost of rotation time and seek time.","title":"Other optimization"},{"location":"notes/l6_query_operation/","text":"50.043 Query Operations Learning Outcomes By the end of this unit, you should be able to assess estimated cost of database operation, namely, select, sort and join describe the external sort algorithm describe nested loop join, block nested loop join, index nested join, sort merge join and hash join Recap Relational Algebra Recall that in week 2, we investigated into relation model and relational alegbra. We argue that relational algebra is a good abstraction to describe data processing operation over a relational model. Given that we know a bit more of the internal implementation of a physical database, (the data structure aspect) we start look into possible ways of implemeting the relational algebra operations in a physical datase (the algorithm aspect). Selection Operation Let \\(R\\) denote a relation (or in other words, a table), we consider the selection operation \\(\\sigma_{C}(R)\\) where \\(C\\) is a logical predicate. The result of this operation is a new relation whose tuples are those coming from \\(R\\) and satisfying the predicate \\(C\\) . There are several ways to implement this operation. Sequential Scanning Knowing that the data in \\(R\\) are stored in a Heap file as a set of Heap pages, we could scan \\(R\\) by reading the data from \\(R\\) page by page. For each tuple in the page, we remove it when it does not satisfy \\(C\\) and retain it otherwise. The cost (in terms of number of Disk I/Os) of using the sequential scanning approach is \\(B(R)\\) , where \\(B(R)\\) denotes the number of pages storing \\(R\\) . Scanning with B+ Tree index A smarter (but not always) way is to make use to the B+ tree index created over the attributes mentioned in \\(C\\) . Case 1: Clustered index In this case the B+ Tree index is clustered, i.e. the data in the pages are stored according to the order of the index attribute. We have to traverse down the B+ Tree to find the boundaries of \\(C\\) , and for each leaf node value (reference) we retrieve the page storing that tuple and check whether that tuple satisfies \\(C\\) . Assuming each node in the B+ Tree occupies a page, the cost of this approach is \\[ log_d(|R|/d) + \\alpha(C,R) \\cdot B(R) \\] Where \\(|R|\\) denotes the number of tuples in \\(R\\) , \\(d\\) denotes the order of the B+Tree. \\((|R|/d)\\) is number of leaf nodes in the B+ Tree, which is the worst case, every node is just half-full. For simplicity, we assume root node is having at least \\(d\\) entries. \\(\\alpha(C,R)\\) denotes the selectivity of the predicate \\(C\\) over \\(R\\) , i.e. the number of tuples \\(R\\) satisfying \\(C\\) divided by \\(|R|\\) . The first term \\(log_d(|R|/d)\\) estimates the cost of traversing the B+ Tree to find the correct starting leaf node. \\(\\alpha(C,R) \\cdot B(R)\\) estimates the cost of scanning the tuples of \\(R\\) . Since the index is clustered, tuples that are stored in the order of the index attributes, we can fetch adjacient tuple w.r.t the attribute referred in \\(C\\) in a single page. Case 2: Unclustered Index When the B+ Tree is not clustered, the estimated cost becomes \\[ log_d(|R|/d) + \\alpha(C,R) \\cdot | R | \\] A page read is needed for every tuple (in the worst case). As we can see that when \\(\\alpha(C,R) > (1 / pagesize)\\) , it is definitely more viable to abandon the index and go for the sequential scan. One possible improvement of using unclustered index is traverse down the B+ Tree to find the correct starting and ending tuple locations (i.e. page id and slot id), sort all the tuple locations by page ids before reading the sorted pages and filter. This will bring us back to the same estimated cost as the clustered index (modular the cost of sorting). Sort Operation Sorting is essential. From Data Driven World and Algorithms, we learned various sorting algorthms. All of these algorithms assumes that the data can fit in the RAM. In the settings of database operations, this assumption is no longer valid. External Sort The external sorting algorithm is an instance of the merge sort. Recap Merge sort The merge sort algorithm is as follows def merge_sort(input) if input size < 2 return input, it is already sorted otherwise divide input into halves, say, i1 and i2 call merge_sort(i1) to get s1 call merge_sort(i2) to get s2 combine(s1,s2) to get sorted return sorted def combine(s1, s2) create an empty list out while s1 is not empty and s2 is not empty let e1 be the first element of s1 let e2 be the first element of s2 if e1 < e2 add e1 to out remove e1 from s1 otherwise add e2 to out remove e2 from s2 if s1 is not empty concatenate s1 to out if s2 is not empty concatentate s2 to out return out External sort The external sort algorithm is a generalized version of merge sort, which operates without loading the full set of data into RAM. External sort by example The algorithm makes use of the buffer pool. Let's consider an example. Suppose we have a buffer pool of 3 frames and would like to sort the following 8 pages of data. Each page contains two tuples. Phase 0 In this phase, we divide the input pages into \\(\\lceil8/3 \\rceil = 3\\) runs and sort each run. From the next phase onwards, we merge each the sorted runs into larger runs, until all are merged into a single one run. Phase 1 We merge 3 runs into two runs. Firstly we merge the two runs on the left into one. When merging two runs into one, we divide the buffer pool's frames into the input frames and output frame. There is only one output frame, the rest are inputs. In this running example, we have two input frames and one output frame. We merge the run [(2,3), (4,6), (9,9)] with [(3,5), (6,7), (8,11)] . We use list notation to denote a run, and a tuple to denote a page. We load the page (2,3) into an input frame and (3,5) into another input frame. We find the smallest leading tuple from both frames, i.e. 2 and move it in the output frame We repeat the same operation, and find 3 , which is moved to the output frame The output frame is full and write it to disk. The first input frame is not empty (because, we assume both 2 and 3 are moved to the output frame). We load the second page (4,6) into this frame. The process is repeated until both runs are processed. The 3rd run [(0,1), (2,)] is alone, hence there is no merging required. At the end of phase 1, we have a new set of runs [ [(2,3), (3,4), (5,6), (6,7), (8,9), (9,11)], [(0,1), (2,1)] ] Phase 2 In this phase we merge the output from the phase into a single run External sort algorithm We consider the pseudo-code of the exernal sort algorithm. The algorithm is defined by a pseudo function ext_sort(in_pages, bpool) , which has the following input and output. input bpool - the buffer pool (in RAM) in_pages - the pages to be sorted (on disk) output sorted results (on disk) We find the pseudo code as follows def ext_sort(in_pages, bpool) let runs = divide_n_sort(in_pages, bpool) while size_of(runs) > 1 let runs = merge(runs, bpool) return runs At step 1.1, we call a helper function divide_n_sort(in_pages, bpool) to generate the initial runs, (phase 0). Steps 1.2 to 1.3 define the merging phases (phase 1, phase 2, .., etc). We repeatly call the helper function merge(runs, run_size, bpool) to merge the current runs set until all runs are merged into a single run. Helper function divide_n_sort Next we consider the helper function divide_n_sort(in_pages, bpool) , which has the following input and output input bpool - the buffer pool (in RAM) in_pages - the pages to be sorted (on disk) output runs - list of lists (on disk). Each inner list (a run) consists of a set of sorted data. (on disk) def divide_n_sort(in_pages, bpool) let count = 0 let m = size_of(bpool) initialize runs to be an empty list (on disk) while (m * count) < size_of(in_pages) load the m pages from in_pages at offset (m * count) sort data in bpool group these m sorted pages into one run (on disk) append run to runs (on disk) increase count by 1 return runs Helper function merge We consider the pseudo code of the function merge(runs, bpool) input bpool - the buffer pool (in RAM) runs - the list of lists (on disk), each inner list (a run) consists of a set of sorted data. (on disk) output next_runs - the runs in next phase (on disk) def merge(runs, bpool) initialize next_runs to be an empty list (on disk) let m = size_of(bpool) let l = size_of(runs) divide bpool into m-1 and 1 frames let in_frames = m-1 frames of bpool for input let out_frame = 1 frame of bpool for output let count = 0 while (m-1)*count < l let batch = extract m-1 runs from runs at offset (m-1) * count let out_run = merge_one(batch, in_frames, out_frame) append out_run to next_runs increment count by 1 return next_runs The merge function processes the runs by merging every batch (consist of b-1 runs) into a larger run for the next phase, where b is the size of the buffer pool. It utilizes another helper function merge_one(runs, bpool) to merge a batch, which has the following signature and definition input batch - a segment from global runs. In variant, size_of(batch) <= size_of(in_frames) in_frame - frames set aside for input pages out_frame - output frame set aside for output. output output_run (on disk) def merge_one(batch, in_frames, out_frame) initialize output_run to be an empty collection let m = size_of(in_frames) while (batch is not empty) and not (all in_frames isempty) for i in (0 to m) if in_frames[i] is empty and batch[i] is not empty load a page batch[i] to in_frames[i] find the smallest record from the leading tuples from in_frames[0] to in_frames[m], move it to out_frame if out_frame is full, write it to output_run return output_run Cost estimation for External sort Assuming the size of in_pages is \\(n\\) , and the buffer pool size is \\(m\\) , we consider the estimated cost (disk I/Os) of external sort algorithm. The ext_sort function has two main parts. The phase 0, the divide_n_sort function call and the while loop, i.e. phase i where i>0. In divide_n_sort , we read each page to be sorted once and write back to disk once. Hence the cost is \\(2 \\cdot n\\) . In each iteration of the while loop in ext_sort function, we merge every \\(m-1\\) runs into a single run, until all runs are merged. There should be \\(\\lceil log_{(m-1)}(\\lceil n/m \\rceil) \\rceil\\) iterations. For each iteration, we read and write each page once. Hence the cost is \\(2 \\cdot n \\cdot \\lceil log_{(m-1)}(\\lceil n/m \\rceil)\\rceil\\) . Join Operation Lastly, we consider the implementation of the join operation. Let \\(R\\) and \\(S\\) be relations. There are several ways of implementing \\(R \\bowtie_{c} S\\) . The most naive implementation is to compute \\(\\sigma_{c}(R \\times S)\\) . It is costly, because of the computation of cartesian product. Alternatively, we may use a nested loop Nested Loop Join One possible approach is to use nested loop for each tuple \\(t\\) in \\(R\\) for each tuple \\(u\\) in \\(S\\) if \\(t\\) and \\(u\\) satisfy \\(c\\) , output \\(t\\) and \\(u\\) . The cost of this approach is \\(B(R) + |R| \\cdot B(S)\\) . The \\(B(R)\\) is total cost of loading all tuples from \\(R\\) once. For each tuple of \\(R\\) , it will be compared with every tuple in \\(S\\) , hence \\(|R| \\cdot B(S)\\) . If we flip the outer/inner relation roles of \\(R\\) and \\(S\\) , we get the following cost \\(B(S) + |S| \\cdot B(R)\\) . Block Nested Loop Join One obvious issue with the nested loop join is that it only utilizes three frames of the buffer pool, 1 frame for \\(R\\) and 1 frame for \\(S\\) and 1 frame for the output. The frame of \\(S\\) is evicted everytime a tuple in \\(R\\) is checked. We could speed up nested loop join by utilizing all frames of the buffer pool. Assuming the buffer pool is of size \\(m\\) , we divide the buffer pool into \\(m-2\\) frames for loading \\(R\\) and 1 frame for loading \\(S\\) and 1 frame for output for each \\(m-2\\) pages in \\(R\\) , we extract each tuple \\(t\\) for each page in \\(S\\) , we extract each tuple \\(u\\) . if \\(t\\) and \\(u\\) satisfy \\(c\\) , output \\(t\\) and \\(u\\) . The cost of this approach is \\(B(R) + \\lceil B(R) / (m - 2) \\rceil \\cdot B(S)\\) . The cost will be \\(B(S) + \\lceil B(S) / (m - 2) \\rceil \\cdot B(R)\\) if the outer/inner relations are swapped. Index Nested Loop Join If the join predicate \\(c\\) is \\(R.a = S.b\\) and an index attribute exists for \\(S.b\\) , we can optimize the nested loop join by using the indexed relation as the inner relation. for each tuple \\(t\\) in \\(R\\) find the tuple \\(u\\) in \\(S\\) with \\(u.b = t.c\\) using index output \\(t\\) and \\(u\\) . If the index is clustered, the cost of this approach is \\(B(R) + |R| \\cdot K\\) where \\(K\\) is a constant dependent on the structure of the index or the height of the B+ Tree index. E.g. if the height of the B+ Tree is 3, then \\(K = 4\\) assuming each node occupying a page. When the index is not clustered, the cost will become \\(B(R) + |R| \\cdot K \\cdot j\\) where \\(j\\) is the maximum number of tuples of \\(S\\) sharing one key. An extreme case where \\(R\\) is sorted by \\(a\\) and \\(S.b\\) is a clustered index. The total cost is \\(B(R) + B(S)\\) . Sort Merge Join The fourth alternative is sort merge join. Sort \\(R\\) by the attribute used in \\(c\\) Sort \\(S\\) by the attribute used in \\(c\\) Merge the sorted \\(R\\) and sorted \\(S\\) like external sort The cost of step 1 is \\[2\\cdot B(R) \\cdot (1 + \\lceil log_{m-1}(\\lceil B(R) / m \\rceil)\\rceil)\\] The cost of step 2 is \\[2\\cdot B(S) \\cdot (1 + \\lceil log_{m-1}(\\lceil B(S) / m \\rceil)\\rceil)\\] The cost of step 3 is \\[B(R) + B(S)\\] Hash Join The fourth alternative is to make use of an in-memory hash table. Assuming we hash the join attribute used in \\(c\\) of \\(S\\) and stored it in a hash table, and the hash table is fitting in the RAM, (one big assumption), we can compute join by for each tuple \\(u\\) in \\(S\\) add \\(u\\) (projected attributes) to HT for each tuple \\(t\\) in \\(R\\) lookup \\(t\\) 's attribute in HT output \\(t\\) and the value in HT The cost of this approach is \\(B(R) +B(S)\\) because we scan both tables exactly once. Grace Hash Join The Hash Join is perhaps impractical, as in most of the cases, the hash table can't fit in the RAM. The assumption - there exists a hash function \\(h\\) such that \\(R\\) can be partitioned into \\(n\\) partitions by \\(h\\) and \\(n \\leq m - 1\\) , and none of the partitions have more than \\(m-1\\) pages, or \\(S\\) can be partitioned into \\(n\\) partitions by \\(h\\) and \\(n \\leq m - 1\\) , and none of the partitions have more than \\(m-1\\) pages. The algorithm works as follows partition \\(R\\) and \\(S\\) by \\(h\\) If \\(S\\) is the relation that satisfies that assumption for each partition \\(p_i\\) in \\(S\\) , load pages in \\(p_i\\) into the buffer pool, and build a in-memory hashtable using a different hash function \\(h_2\\) load data (page by page) from partition \\(q_i\\) in \\(R\\) (values in \\(q_i\\) and \\(p_i\\) should share the same hash values of \\(h\\) ) look up the data in the hash table with \\(h_2\\) , output if a match is found. The total cost of the Grace Hash Join is \\[ 3 \\cdot (B(R) + B(S)) \\]","title":"50.043 Query Operations"},{"location":"notes/l6_query_operation/#50043-query-operations","text":"","title":"50.043 Query Operations"},{"location":"notes/l6_query_operation/#learning-outcomes","text":"By the end of this unit, you should be able to assess estimated cost of database operation, namely, select, sort and join describe the external sort algorithm describe nested loop join, block nested loop join, index nested join, sort merge join and hash join","title":"Learning Outcomes"},{"location":"notes/l6_query_operation/#recap-relational-algebra","text":"Recall that in week 2, we investigated into relation model and relational alegbra. We argue that relational algebra is a good abstraction to describe data processing operation over a relational model. Given that we know a bit more of the internal implementation of a physical database, (the data structure aspect) we start look into possible ways of implemeting the relational algebra operations in a physical datase (the algorithm aspect).","title":"Recap Relational Algebra"},{"location":"notes/l6_query_operation/#selection-operation","text":"Let \\(R\\) denote a relation (or in other words, a table), we consider the selection operation \\(\\sigma_{C}(R)\\) where \\(C\\) is a logical predicate. The result of this operation is a new relation whose tuples are those coming from \\(R\\) and satisfying the predicate \\(C\\) . There are several ways to implement this operation.","title":"Selection Operation"},{"location":"notes/l6_query_operation/#sequential-scanning","text":"Knowing that the data in \\(R\\) are stored in a Heap file as a set of Heap pages, we could scan \\(R\\) by reading the data from \\(R\\) page by page. For each tuple in the page, we remove it when it does not satisfy \\(C\\) and retain it otherwise. The cost (in terms of number of Disk I/Os) of using the sequential scanning approach is \\(B(R)\\) , where \\(B(R)\\) denotes the number of pages storing \\(R\\) .","title":"Sequential Scanning"},{"location":"notes/l6_query_operation/#scanning-with-b-tree-index","text":"A smarter (but not always) way is to make use to the B+ tree index created over the attributes mentioned in \\(C\\) .","title":"Scanning with B+ Tree index"},{"location":"notes/l6_query_operation/#case-1-clustered-index","text":"In this case the B+ Tree index is clustered, i.e. the data in the pages are stored according to the order of the index attribute. We have to traverse down the B+ Tree to find the boundaries of \\(C\\) , and for each leaf node value (reference) we retrieve the page storing that tuple and check whether that tuple satisfies \\(C\\) . Assuming each node in the B+ Tree occupies a page, the cost of this approach is \\[ log_d(|R|/d) + \\alpha(C,R) \\cdot B(R) \\] Where \\(|R|\\) denotes the number of tuples in \\(R\\) , \\(d\\) denotes the order of the B+Tree. \\((|R|/d)\\) is number of leaf nodes in the B+ Tree, which is the worst case, every node is just half-full. For simplicity, we assume root node is having at least \\(d\\) entries. \\(\\alpha(C,R)\\) denotes the selectivity of the predicate \\(C\\) over \\(R\\) , i.e. the number of tuples \\(R\\) satisfying \\(C\\) divided by \\(|R|\\) . The first term \\(log_d(|R|/d)\\) estimates the cost of traversing the B+ Tree to find the correct starting leaf node. \\(\\alpha(C,R) \\cdot B(R)\\) estimates the cost of scanning the tuples of \\(R\\) . Since the index is clustered, tuples that are stored in the order of the index attributes, we can fetch adjacient tuple w.r.t the attribute referred in \\(C\\) in a single page.","title":"Case 1: Clustered index"},{"location":"notes/l6_query_operation/#case-2-unclustered-index","text":"When the B+ Tree is not clustered, the estimated cost becomes \\[ log_d(|R|/d) + \\alpha(C,R) \\cdot | R | \\] A page read is needed for every tuple (in the worst case). As we can see that when \\(\\alpha(C,R) > (1 / pagesize)\\) , it is definitely more viable to abandon the index and go for the sequential scan. One possible improvement of using unclustered index is traverse down the B+ Tree to find the correct starting and ending tuple locations (i.e. page id and slot id), sort all the tuple locations by page ids before reading the sorted pages and filter. This will bring us back to the same estimated cost as the clustered index (modular the cost of sorting).","title":"Case 2: Unclustered Index"},{"location":"notes/l6_query_operation/#sort-operation","text":"Sorting is essential. From Data Driven World and Algorithms, we learned various sorting algorthms. All of these algorithms assumes that the data can fit in the RAM. In the settings of database operations, this assumption is no longer valid.","title":"Sort Operation"},{"location":"notes/l6_query_operation/#external-sort","text":"The external sorting algorithm is an instance of the merge sort.","title":"External Sort"},{"location":"notes/l6_query_operation/#recap-merge-sort","text":"The merge sort algorithm is as follows def merge_sort(input) if input size < 2 return input, it is already sorted otherwise divide input into halves, say, i1 and i2 call merge_sort(i1) to get s1 call merge_sort(i2) to get s2 combine(s1,s2) to get sorted return sorted def combine(s1, s2) create an empty list out while s1 is not empty and s2 is not empty let e1 be the first element of s1 let e2 be the first element of s2 if e1 < e2 add e1 to out remove e1 from s1 otherwise add e2 to out remove e2 from s2 if s1 is not empty concatenate s1 to out if s2 is not empty concatentate s2 to out return out","title":"Recap Merge sort"},{"location":"notes/l6_query_operation/#external-sort_1","text":"The external sort algorithm is a generalized version of merge sort, which operates without loading the full set of data into RAM.","title":"External sort"},{"location":"notes/l6_query_operation/#external-sort-by-example","text":"The algorithm makes use of the buffer pool. Let's consider an example. Suppose we have a buffer pool of 3 frames and would like to sort the following 8 pages of data. Each page contains two tuples.","title":"External sort by example"},{"location":"notes/l6_query_operation/#phase-0","text":"In this phase, we divide the input pages into \\(\\lceil8/3 \\rceil = 3\\) runs and sort each run. From the next phase onwards, we merge each the sorted runs into larger runs, until all are merged into a single one run.","title":"Phase 0"},{"location":"notes/l6_query_operation/#phase-1","text":"We merge 3 runs into two runs. Firstly we merge the two runs on the left into one. When merging two runs into one, we divide the buffer pool's frames into the input frames and output frame. There is only one output frame, the rest are inputs. In this running example, we have two input frames and one output frame. We merge the run [(2,3), (4,6), (9,9)] with [(3,5), (6,7), (8,11)] . We use list notation to denote a run, and a tuple to denote a page. We load the page (2,3) into an input frame and (3,5) into another input frame. We find the smallest leading tuple from both frames, i.e. 2 and move it in the output frame We repeat the same operation, and find 3 , which is moved to the output frame The output frame is full and write it to disk. The first input frame is not empty (because, we assume both 2 and 3 are moved to the output frame). We load the second page (4,6) into this frame. The process is repeated until both runs are processed. The 3rd run [(0,1), (2,)] is alone, hence there is no merging required. At the end of phase 1, we have a new set of runs [ [(2,3), (3,4), (5,6), (6,7), (8,9), (9,11)], [(0,1), (2,1)] ]","title":"Phase 1"},{"location":"notes/l6_query_operation/#phase-2","text":"In this phase we merge the output from the phase into a single run","title":"Phase 2"},{"location":"notes/l6_query_operation/#external-sort-algorithm","text":"We consider the pseudo-code of the exernal sort algorithm. The algorithm is defined by a pseudo function ext_sort(in_pages, bpool) , which has the following input and output. input bpool - the buffer pool (in RAM) in_pages - the pages to be sorted (on disk) output sorted results (on disk) We find the pseudo code as follows def ext_sort(in_pages, bpool) let runs = divide_n_sort(in_pages, bpool) while size_of(runs) > 1 let runs = merge(runs, bpool) return runs At step 1.1, we call a helper function divide_n_sort(in_pages, bpool) to generate the initial runs, (phase 0). Steps 1.2 to 1.3 define the merging phases (phase 1, phase 2, .., etc). We repeatly call the helper function merge(runs, run_size, bpool) to merge the current runs set until all runs are merged into a single run.","title":"External sort algorithm"},{"location":"notes/l6_query_operation/#helper-function-divide_n_sort","text":"Next we consider the helper function divide_n_sort(in_pages, bpool) , which has the following input and output input bpool - the buffer pool (in RAM) in_pages - the pages to be sorted (on disk) output runs - list of lists (on disk). Each inner list (a run) consists of a set of sorted data. (on disk) def divide_n_sort(in_pages, bpool) let count = 0 let m = size_of(bpool) initialize runs to be an empty list (on disk) while (m * count) < size_of(in_pages) load the m pages from in_pages at offset (m * count) sort data in bpool group these m sorted pages into one run (on disk) append run to runs (on disk) increase count by 1 return runs","title":"Helper function divide_n_sort"},{"location":"notes/l6_query_operation/#helper-function-merge","text":"We consider the pseudo code of the function merge(runs, bpool) input bpool - the buffer pool (in RAM) runs - the list of lists (on disk), each inner list (a run) consists of a set of sorted data. (on disk) output next_runs - the runs in next phase (on disk) def merge(runs, bpool) initialize next_runs to be an empty list (on disk) let m = size_of(bpool) let l = size_of(runs) divide bpool into m-1 and 1 frames let in_frames = m-1 frames of bpool for input let out_frame = 1 frame of bpool for output let count = 0 while (m-1)*count < l let batch = extract m-1 runs from runs at offset (m-1) * count let out_run = merge_one(batch, in_frames, out_frame) append out_run to next_runs increment count by 1 return next_runs The merge function processes the runs by merging every batch (consist of b-1 runs) into a larger run for the next phase, where b is the size of the buffer pool. It utilizes another helper function merge_one(runs, bpool) to merge a batch, which has the following signature and definition input batch - a segment from global runs. In variant, size_of(batch) <= size_of(in_frames) in_frame - frames set aside for input pages out_frame - output frame set aside for output. output output_run (on disk) def merge_one(batch, in_frames, out_frame) initialize output_run to be an empty collection let m = size_of(in_frames) while (batch is not empty) and not (all in_frames isempty) for i in (0 to m) if in_frames[i] is empty and batch[i] is not empty load a page batch[i] to in_frames[i] find the smallest record from the leading tuples from in_frames[0] to in_frames[m], move it to out_frame if out_frame is full, write it to output_run return output_run","title":"Helper function merge"},{"location":"notes/l6_query_operation/#cost-estimation-for-external-sort","text":"Assuming the size of in_pages is \\(n\\) , and the buffer pool size is \\(m\\) , we consider the estimated cost (disk I/Os) of external sort algorithm. The ext_sort function has two main parts. The phase 0, the divide_n_sort function call and the while loop, i.e. phase i where i>0. In divide_n_sort , we read each page to be sorted once and write back to disk once. Hence the cost is \\(2 \\cdot n\\) . In each iteration of the while loop in ext_sort function, we merge every \\(m-1\\) runs into a single run, until all runs are merged. There should be \\(\\lceil log_{(m-1)}(\\lceil n/m \\rceil) \\rceil\\) iterations. For each iteration, we read and write each page once. Hence the cost is \\(2 \\cdot n \\cdot \\lceil log_{(m-1)}(\\lceil n/m \\rceil)\\rceil\\) .","title":"Cost estimation for External sort"},{"location":"notes/l6_query_operation/#join-operation","text":"Lastly, we consider the implementation of the join operation. Let \\(R\\) and \\(S\\) be relations. There are several ways of implementing \\(R \\bowtie_{c} S\\) . The most naive implementation is to compute \\(\\sigma_{c}(R \\times S)\\) . It is costly, because of the computation of cartesian product. Alternatively, we may use a nested loop","title":"Join Operation"},{"location":"notes/l6_query_operation/#nested-loop-join","text":"One possible approach is to use nested loop for each tuple \\(t\\) in \\(R\\) for each tuple \\(u\\) in \\(S\\) if \\(t\\) and \\(u\\) satisfy \\(c\\) , output \\(t\\) and \\(u\\) . The cost of this approach is \\(B(R) + |R| \\cdot B(S)\\) . The \\(B(R)\\) is total cost of loading all tuples from \\(R\\) once. For each tuple of \\(R\\) , it will be compared with every tuple in \\(S\\) , hence \\(|R| \\cdot B(S)\\) . If we flip the outer/inner relation roles of \\(R\\) and \\(S\\) , we get the following cost \\(B(S) + |S| \\cdot B(R)\\) .","title":"Nested Loop Join"},{"location":"notes/l6_query_operation/#block-nested-loop-join","text":"One obvious issue with the nested loop join is that it only utilizes three frames of the buffer pool, 1 frame for \\(R\\) and 1 frame for \\(S\\) and 1 frame for the output. The frame of \\(S\\) is evicted everytime a tuple in \\(R\\) is checked. We could speed up nested loop join by utilizing all frames of the buffer pool. Assuming the buffer pool is of size \\(m\\) , we divide the buffer pool into \\(m-2\\) frames for loading \\(R\\) and 1 frame for loading \\(S\\) and 1 frame for output for each \\(m-2\\) pages in \\(R\\) , we extract each tuple \\(t\\) for each page in \\(S\\) , we extract each tuple \\(u\\) . if \\(t\\) and \\(u\\) satisfy \\(c\\) , output \\(t\\) and \\(u\\) . The cost of this approach is \\(B(R) + \\lceil B(R) / (m - 2) \\rceil \\cdot B(S)\\) . The cost will be \\(B(S) + \\lceil B(S) / (m - 2) \\rceil \\cdot B(R)\\) if the outer/inner relations are swapped.","title":"Block Nested Loop Join"},{"location":"notes/l6_query_operation/#index-nested-loop-join","text":"If the join predicate \\(c\\) is \\(R.a = S.b\\) and an index attribute exists for \\(S.b\\) , we can optimize the nested loop join by using the indexed relation as the inner relation. for each tuple \\(t\\) in \\(R\\) find the tuple \\(u\\) in \\(S\\) with \\(u.b = t.c\\) using index output \\(t\\) and \\(u\\) . If the index is clustered, the cost of this approach is \\(B(R) + |R| \\cdot K\\) where \\(K\\) is a constant dependent on the structure of the index or the height of the B+ Tree index. E.g. if the height of the B+ Tree is 3, then \\(K = 4\\) assuming each node occupying a page. When the index is not clustered, the cost will become \\(B(R) + |R| \\cdot K \\cdot j\\) where \\(j\\) is the maximum number of tuples of \\(S\\) sharing one key. An extreme case where \\(R\\) is sorted by \\(a\\) and \\(S.b\\) is a clustered index. The total cost is \\(B(R) + B(S)\\) .","title":"Index Nested Loop Join"},{"location":"notes/l6_query_operation/#sort-merge-join","text":"The fourth alternative is sort merge join. Sort \\(R\\) by the attribute used in \\(c\\) Sort \\(S\\) by the attribute used in \\(c\\) Merge the sorted \\(R\\) and sorted \\(S\\) like external sort The cost of step 1 is \\[2\\cdot B(R) \\cdot (1 + \\lceil log_{m-1}(\\lceil B(R) / m \\rceil)\\rceil)\\] The cost of step 2 is \\[2\\cdot B(S) \\cdot (1 + \\lceil log_{m-1}(\\lceil B(S) / m \\rceil)\\rceil)\\] The cost of step 3 is \\[B(R) + B(S)\\]","title":"Sort Merge Join"},{"location":"notes/l6_query_operation/#hash-join","text":"The fourth alternative is to make use of an in-memory hash table. Assuming we hash the join attribute used in \\(c\\) of \\(S\\) and stored it in a hash table, and the hash table is fitting in the RAM, (one big assumption), we can compute join by for each tuple \\(u\\) in \\(S\\) add \\(u\\) (projected attributes) to HT for each tuple \\(t\\) in \\(R\\) lookup \\(t\\) 's attribute in HT output \\(t\\) and the value in HT The cost of this approach is \\(B(R) +B(S)\\) because we scan both tables exactly once.","title":"Hash Join"},{"location":"notes/l6_query_operation/#grace-hash-join","text":"The Hash Join is perhaps impractical, as in most of the cases, the hash table can't fit in the RAM. The assumption - there exists a hash function \\(h\\) such that \\(R\\) can be partitioned into \\(n\\) partitions by \\(h\\) and \\(n \\leq m - 1\\) , and none of the partitions have more than \\(m-1\\) pages, or \\(S\\) can be partitioned into \\(n\\) partitions by \\(h\\) and \\(n \\leq m - 1\\) , and none of the partitions have more than \\(m-1\\) pages. The algorithm works as follows partition \\(R\\) and \\(S\\) by \\(h\\) If \\(S\\) is the relation that satisfies that assumption for each partition \\(p_i\\) in \\(S\\) , load pages in \\(p_i\\) into the buffer pool, and build a in-memory hashtable using a different hash function \\(h_2\\) load data (page by page) from partition \\(q_i\\) in \\(R\\) (values in \\(q_i\\) and \\(p_i\\) should share the same hash values of \\(h\\) ) look up the data in the hash table with \\(h_2\\) , output if a match is found. The total cost of the Grace Hash Join is \\[ 3 \\cdot (B(R) + B(S)) \\]","title":"Grace Hash Join"},{"location":"notes/l8_query_optimization/","text":"50.043 - Query Optimization Learning Outcomes By the end of this unit, you should be able to name the stages in a query life cycle identify the process involved in different stages of a query life cycle identify the different execution model explain the process of query optimization estimate the cost of complex query operations Query Life Cycle From the previous lessons, we learn how to implement the primitive relational algebra operation in a physical database system, namely, select, join and sort. However in real word applications, we often encounter queries far more complex. Recall the article/book/publisher example article( id , name) book( id , name, date) publisher( id , name) publish( article_id, book_id, publisher_id ) We would like to find out article names that are published in year 2010 \\[ \\Pi_{article.name} (\\sigma_{book.date = 2010}(article \\bowtie_{article.id=publish.article\\_id} (publish \\bowtie_{publish.book\\_id = book.id} book))) ~~~~ {\\tt (E1)} \\] Note that the above relation algebra expression is equivalent to the following thanks to the associativity of the \\(\\bowtie\\) operator. \\[ \\Pi_{article.name} (\\sigma_{book.date = 2010}((article \\bowtie_{article.id=publish.article\\_id} publish) \\bowtie_{publish.book\\_id = book.id} book)) ~~~~ {\\tt (E2)} \\] Besides these two alternatives, we also find the following expression producing the same result by \"pushing\" the selection operation down to the book relation, since it is only constrainted by the book 's attribute. \\[ \\Pi_{article.name} (article \\bowtie_{article.id=publish.article\\_id} (publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book))) ~~~~ {\\tt (E3)} \\] We call them different logical query plans of the same query. We might argue that (E3) might perform better as we push down the selection operator and reduce the size of the joined relation. Sometimes for ease of reasoning we might consider representing the relational algbra expression in a tree structure, for instance, (E3) can be represented as graph a(\"&Pi;<sub>article.name</sub>\") --- b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- c(\"article\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- e(\"publish\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- f(\"&sigma;<sub>book.date = 2020</sub>\") f(\"&sigma;<sub>book.date = 2020</sub>\") --- g(\"book\") In reality, we also need to consider the selectivity of selection condition, the size of the relations, the different join algorithms, availability of indices, as well as the sortedness of the relation. Each intermediate operation can be computed \"on-the-fly\" or \"materalized.\" Each possible combination performs differently. We refer to these combinations as physical query plans . For instance, graph a(\"&Pi;<sub>article.name</sub><br/>[iterator]\") --- b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- c(\"article <br/> [heap scan]\") b(\"&bowtie;<sub>article.id=publish.article_id</sub><br/>[block nested loop]\") --- d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- e(\"publish <br/> [heap scan]\") d(\"&bowtie;<sub>publish.book_id = book.id</sub> <br/> [block nested loop]\") --- f(\"&sigma;<sub>book.date = 2020</sub>\") f(\"&sigma;<sub>book.date = 2020</sub>\") --- g(\"book<br/>[heap scan]\") The physical plan above scan the heap file to filter books with date = 2020 , then a block nested join is performed on the publish relation and the immediate results coming from the filter. Finally, we apply another block nested loop to join the article relation with the immediate results coming from the nested join. The top level project is performed \"on-the-fly\". Alternatively, we might have a slightly different phyiscal query plan, graph a(\"&Pi;<sub>article.name</sub><br/>[iterator]\") --- b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- c(\"article <br/> [heap scan]\") b(\"&bowtie;<sub>article.id=publish.article_id</sub><br/>[index nested loop]\") --- d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- e(\"publish <br/> [heap scan]\") d(\"&bowtie;<sub>publish.book_id = book.id</sub> <br/> [block nested loop]\") --- f(\"&sigma;<sub>book.date = 2020</sub>\") f(\"&sigma;<sub>book.date = 2020</sub>\") --- g(\"book<br/>[heap scan]\") In the above physical query plan, we assume following indices are preesent article.id (it's a primary key anyway) The physical plan applies the index nested loop to the outer join by using article as the inner relation. Though we preform a heap scan on the article relation, the operation is also a clustered index scan, since article is having primary key index on id . Note that there could be many physical query plans generated from a logical plan. Given a set of physical query plans, we need to find the \"best\" one (with the lowest I/O cost). It turns out that it is very challenging problem, and we will discuss about this shortly. This step is called query optimization . Suppose we identify a good physical query plan, we translate it into actual code to be executed in the database host machine. This final step is called query execution . stateDiagram-v2 s1 : query s2 : logical plans s1 --> s2 : query rewriter s3 : physical plans s2 --> s3 : plan generation / cost estimation s4 : result s3 --> s4 : execution Execution Model The input of an execution model is the logical query plan. Every operation in the query plan can be executed differently. There are several options Iterator Model (seen in our project!) Materialization Model Vector Model Iterator Model The main idea of the iterator model is \"on the fly\" or \"lazy\", i.e. tuples are not retrieved/computed until they are needed. The idea is actualized by implementing the physical access operation, filter operation, join operation and etc has iterators. One possible implementation in Java is to use the Iterator interface, in which classDiagram class Iterator~T~ { bool hasNext() T next() } <<interface>> Iterator The hasNext() method checks whether the iterator is currently not empty. The next() method advances the internal pointer and produces the next item. For instance \\(\\sigma_{book.date = 2020}(book)\\) , the table access of book is an instance of Iterator<Tuple> , The filter operator \\(\\sigma\\) takes the book iterator and returns an iterator instance Iterator<Tuple> , too. When the top most iterator is executed, it calls its children's iterators to produce the next value. One strong advantage of the iterator model is that it is memory efficient. For instance, in the running example, we don't need to load all the tuples from the book table into the buffer pool before the filtering takes place. There might be worst case scenarios in which the intermediate results are too huge for the RAM and needed to be written back to the disk and be reloaded when required. The downside of the iterator model is that sometimes we need to re-compute some common sub-queries, e.g. \\((\\sigma_{book.date = 2020}(book)) \\bowtie (\\sigma_{book.date = 2020}(book))\\) , we need to compute \\(\\sigma_{book.date = 2020}(book)\\) twice. We would be in the worst situation if an immediate result is needed as a whole, e.g. need to build a hash table, need to sort the immediate data first. In these situation, we have to block and consume all the children's data first. Materialization Model In materialization model, immediate results are fully computed and emit before the outer operation starts its computation. We can think of each operator having its own local storage holding the immediate results. The advantage of this model is that we can re-use immediate results if they needed more than once and we ca use the immediate results to build immediate index, hash table, etc. It performs better when the immediate results are eventually part of the final results. Vectorized Model The vectorized model, (AKA the batch model) is a hybrid of iterator and materialization model. Every operator in the vectorized model implements the next() method. The next() method returns a batch of tuples instead of a single one. The advantage of this model is to strike a balance between iterator and materialization. It is memory efficient, as we don't compute and store the entire immediate result, yet we can compute Hash table (e.g. grace hash join) when needed. Query Planning The process of query planning involves enumerating a large enough subset of all possible equivalent query given the input expression. Two relaional algebra expressions are consider equivalent if they produce the same set of results. Equivalence rules Let \\(R\\) , \\(R'\\) and \\(R''\\) be relations, the subset of requivalence rules is as follows, \\(\\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_1 \\wedge c_2}(R)\\) \\(\\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_2}(\\sigma_{c_1}(R))\\) \\(\\Pi_{a_1}(\\Pi_{a_2} ... (\\Pi_{a_n}(R))) = \\Pi_{a_1}(R)\\) if \\(a_1 \\subseteq a_2 \\subseteq ... \\subseteq a_n\\) \\(\\sigma_{c}(R \\times R') \\equiv R \\bowtie_{c} R'\\) . \\(R \\bowtie_{c} R' \\equiv R' \\bowtie_{c} R\\) . \\(R \\bowtie_{c_1} (R' \\bowtie_{c_2} R'') \\equiv (R \\bowtie_{c_1} R') \\bowtie_{c_2} R''\\) \\(\\sigma_{c}(R) \\equiv R\\) if \\(c \\not \\subseteq attr(R)\\) \\(\\sigma_{c_1}(R\\bowtie_{c_2} R') \\equiv \\sigma_{c_1}(R) \\bowtie_{c_2} \\sigma_{c_1}(R')\\) \\(R \\cap R' \\equiv R' \\cap R\\) \\(R \\cup R' \\equiv R \\cup R\\) \\(R \\cap (R' \\cap R'') \\equiv (R \\cap R') \\cap R''\\) \\(R \\cup (R' \\cup R'') \\equiv (R \\cup R') \\cup R''\\) \\(\\Pi_{a_1 \\cup a_2} (R \\bowtie_{c} R') \\equiv \\Pi_{a_1}(R) \\bowtie_{c} \\Pi_{a_2}(R')\\) if \\(attr(c) \\subseteq a_1 \\cap a_2\\) . \\(\\Pi_{a_1 \\cup a_2} (R \\bowtie_{c} R') \\equiv \\Pi_{a_1 \\cup a_2}(\\Pi_{a_1\\cup a_3}(R) \\bowtie_{c} \\Pi_{a_2\\cup a_4}(R') )\\) if \\(attr(c) \\subseteq a_3 \\cap a_4\\) \\(\\Pi_{a}(R \\cup R') \\equiv \\Pi_{a}(R) \\cup \\Pi_{a}(R')\\) \\(\\Pi_{a}(\\sigma_{c}(R)) \\equiv \\sigma_{c}(\\Pi_{a}(R))\\) if \\(attr(c) \\subseteq a\\) \\(\\sigma_{c}(R - R') \\equiv \\sigma_{c}(R) - R'\\) \\(\\sigma_{c}(R \\cap R') \\equiv \\sigma_{c}(R) \\cap \\sigma_{c}(R')\\) \\(\\sigma_{c}(R \\cup R') \\equiv \\sigma_{c}(R) \\cup \\sigma_{c}(R')\\) With this subset of rules, we can already enumerate a substantially large set of alternative logical query plans from the original plan. Cost Estimation Recall that given a logical query plan, there are still multiple alternatives of physical query plans depending on the choice of access method, execution model, index and etc. The ultimate goal is to find the physical plan that has the lowest I/O cost. Without running the plans, we could only estimate the cost. From the previous lesson, we learned how to estimate the I/O cost for different access methods, selection, join and etc. The cost is subject to two extra information. the sizes of the relations the selectivity of the predicate. In DBMS, the sizes of the relations are recorded periodically in the catalog. The catalog sub-system keeps track of the following the statistics which can be used to approximate the selectivity. \\(N(R)\\) the number of tuples in \\(R\\) \\(V(a,R)\\) the number of unique values of attribute \\(a\\) in \\(R\\) . \\(min(a,R)\\) the minumum value of attribute \\(a\\) in \\(R\\) \\(max(a,R)\\) the maximum value of attribtue \\(a\\) in \\(R\\) . Cardinality Estimation Given the above catalog statistics, we can estimate the selectivity ratio, \\(\\alpha(c, R)\\) . Equality Predicate In case \\(c\\) is \\(a = v\\) where \\(a\\) is an attribute of \\(R\\) and \\(v\\) is a value, \\(\\alpha(a = v, R) = 1 / V(a,R)\\) . Range Equality Predicate In case \\(c\\) is \\(a > v\\) where \\(a\\) is an attribute of \\(R\\) and \\(v\\) is a value, \\(\\alpha(a > v, R) = (max(a,R) - v) / (max(a,R) - min(a,R) + 1)\\) assuming values of \\(a\\) in \\(R\\) follow a uniform distribution. In case the distribution is not uniform, we may use a histogram with binning to obtain a better estimation. However building historgram for a large table could be expenive. In case of large data set, DBMS often collect a small sample to build the histogram. Conjunction Predicate In case \\(c\\) is \\(c_1 \\wedge c_2\\) , assuming values constrained by \\(c_1\\) are independent of those constrained by \\(c_2\\) , \\(\\alpha(c_1 \\wedge c_2, R) = \\alpha(c_1, R) \\cdot \\alpha(c_2, R)\\) . Search Algorithm The last piece of the query optimization is to search for the best physical plan given a method to enumerate alternative a way to estimate the I/O cost given a plan The challenge A naive approach would generate all possible plans, apply estimation to all of them and find the best one. However enumerating all possible plans could be expensive. For instance, enumerating all plan of a \\(n\\) -way join will yield \\(4^{n}\\) possibilities. Recall from our algorithm class that the number of possible Binary Search Trees with \\(n\\) different keys is catalan number \\[C_n = (2\\cdot n)! / ((n + 1)! \\cdot n!)\\] As \\(n\\) grows, catalan numbers grow as \\[4^n / (n^{3/2} \\cdot \\sqrt{\\pi}) \\] A set of possible query plans with \\(n\\) -way join shares the same size! Selinger Algorithm The selinger algorithm is one of the classic algorthm for searching for optimal plan. The idea behind is to apply the following heuristics Only consider the left-skew trees. This is because in many different physical plans, we need to scan the right sub-tree multiple times in join. It is better to keep the right sub-tree as simple as possible. If we apply this heuristic, we cut down the search base to \\[ \\left (\\begin{array}{c} n \\\\ n - 1 \\end{array} \\right ) \\cdot \\left (\\begin{array}{c} n - 1 \\\\ n - 2 \\end{array} \\right ) \\cdot ... \\cdot \\left (\\begin{array}{c} 2\\\\ 1 \\end{array} \\right ) = n! \\] Considering the left-skew trees with \\(n\\) relations still computing \\(n!\\) possible plans. To further cut down the search, the algorithm assumes the best overall plan consists of best sub-plans. It proceeds by finding the best plans for leaf nodes, then \"walk-up\" the trees by finding the best plans to combine intermediate steps. For example, Pass 1. find the best plans for each relations (leaf nodes) Pass 2. find the best plans to join any 2 relations Pass 3. find the best plans to join any 3 relations ... For each pass \\(i\\) where \\(i>1\\) , the best plans are computed based on the result from pass \\(i-1\\) . Overall this cut down the final search space of an \\(n\\) -way join to \\[ \\left (\\begin{array}{c} n \\\\ n-1 \\end{array} \\right ) \\] And the overall time-complexity of \\[ \\left (\\begin{array}{c} n \\\\ n - 1 \\end{array} \\right ) + \\left (\\begin{array}{c} n \\\\ n - 2 \\end{array} \\right ) + ... + \\left (\\begin{array}{c} n\\\\ 1 \\end{array} \\right ) = O(2^n) \\] For instance, recall our running example, \\[ \\Pi_{article.name} (article \\bowtie_{article.id=publish.article\\_id} publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book)) \\] For simplicity, we only focus on the join expression \\[ article \\bowtie_{article.id=publish.article\\_id} publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book) \\] Assuming there is no index created on \\(book.date\\) . Pass 1 we find the cheapest way to access \\(\\sigma_{book.date = 2020}(book)\\) , (because \\(\\sigma\\) is not a join), is an heap scan. the cheapest way to acccess \\(publish\\) is heap scan. the cheapest way to accesss \\(artcle\\) is heap scan. Pass 2 immediate relation 1: we join \\(publish\\) and \\(article\\) by taking the cheapest access methods from Pass 1, hence the cheapest way to join \\(publish\\) and \\(article\\) is an index nested join on \\(article.id\\) , let's say the cost is 1000. immediate relation 2: we join \\(publish\\) and \\(\\sigma_{book.date = 2020}(book)\\) , let's say the cheapest way to join is an block nested join with \\(\\sigma_{book.date = 2020}(book)\\) materialized, the cost is 2750. immediate relation 3: we join \\(article\\) and \\(\\sigma_{book.date = 2020}(book)\\) . There is no common attribute, hence it is a cross product. The cost is 25000. Pass 3 alternative 1: we join \\(\\sigma_{book.date = 2020}(book)\\) with immediate relation 1 from Pass 2. Let's say the cheapest cost is 5000 with a block nested loop. alternative 2: we join \\(article\\) with immediate relation 2 from Pass 2. Let's say the cheapest cost is 6000 with an index nested loop. alternative 3: we join \\(publish\\) with the immediate rlation 3, with an index nested loop, the cost is 50000. Overall, the cheapest plan is alterantive 1. Note that in real world DBMS, the query optimizer would prepare two best plans for each relation for all passes, an unordered plan and a best index plan. With these adjustment, we have double the size of the top-level search space and the overall time complexity. Optional Reading - Translating SQL into relational algebra Translating SQL into the Relational Algebra","title":"50.043 - Query Optimization"},{"location":"notes/l8_query_optimization/#50043-query-optimization","text":"","title":"50.043 - Query Optimization"},{"location":"notes/l8_query_optimization/#learning-outcomes","text":"By the end of this unit, you should be able to name the stages in a query life cycle identify the process involved in different stages of a query life cycle identify the different execution model explain the process of query optimization estimate the cost of complex query operations","title":"Learning Outcomes"},{"location":"notes/l8_query_optimization/#query-life-cycle","text":"From the previous lessons, we learn how to implement the primitive relational algebra operation in a physical database system, namely, select, join and sort. However in real word applications, we often encounter queries far more complex. Recall the article/book/publisher example article( id , name) book( id , name, date) publisher( id , name) publish( article_id, book_id, publisher_id ) We would like to find out article names that are published in year 2010 \\[ \\Pi_{article.name} (\\sigma_{book.date = 2010}(article \\bowtie_{article.id=publish.article\\_id} (publish \\bowtie_{publish.book\\_id = book.id} book))) ~~~~ {\\tt (E1)} \\] Note that the above relation algebra expression is equivalent to the following thanks to the associativity of the \\(\\bowtie\\) operator. \\[ \\Pi_{article.name} (\\sigma_{book.date = 2010}((article \\bowtie_{article.id=publish.article\\_id} publish) \\bowtie_{publish.book\\_id = book.id} book)) ~~~~ {\\tt (E2)} \\] Besides these two alternatives, we also find the following expression producing the same result by \"pushing\" the selection operation down to the book relation, since it is only constrainted by the book 's attribute. \\[ \\Pi_{article.name} (article \\bowtie_{article.id=publish.article\\_id} (publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book))) ~~~~ {\\tt (E3)} \\] We call them different logical query plans of the same query. We might argue that (E3) might perform better as we push down the selection operator and reduce the size of the joined relation. Sometimes for ease of reasoning we might consider representing the relational algbra expression in a tree structure, for instance, (E3) can be represented as graph a(\"&Pi;<sub>article.name</sub>\") --- b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- c(\"article\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- e(\"publish\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- f(\"&sigma;<sub>book.date = 2020</sub>\") f(\"&sigma;<sub>book.date = 2020</sub>\") --- g(\"book\") In reality, we also need to consider the selectivity of selection condition, the size of the relations, the different join algorithms, availability of indices, as well as the sortedness of the relation. Each intermediate operation can be computed \"on-the-fly\" or \"materalized.\" Each possible combination performs differently. We refer to these combinations as physical query plans . For instance, graph a(\"&Pi;<sub>article.name</sub><br/>[iterator]\") --- b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- c(\"article <br/> [heap scan]\") b(\"&bowtie;<sub>article.id=publish.article_id</sub><br/>[block nested loop]\") --- d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- e(\"publish <br/> [heap scan]\") d(\"&bowtie;<sub>publish.book_id = book.id</sub> <br/> [block nested loop]\") --- f(\"&sigma;<sub>book.date = 2020</sub>\") f(\"&sigma;<sub>book.date = 2020</sub>\") --- g(\"book<br/>[heap scan]\") The physical plan above scan the heap file to filter books with date = 2020 , then a block nested join is performed on the publish relation and the immediate results coming from the filter. Finally, we apply another block nested loop to join the article relation with the immediate results coming from the nested join. The top level project is performed \"on-the-fly\". Alternatively, we might have a slightly different phyiscal query plan, graph a(\"&Pi;<sub>article.name</sub><br/>[iterator]\") --- b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") b(\"&bowtie;<sub>article.id=publish.article_id</sub>\") --- c(\"article <br/> [heap scan]\") b(\"&bowtie;<sub>article.id=publish.article_id</sub><br/>[index nested loop]\") --- d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") d(\"&bowtie;<sub>publish.book_id = book.id</sub>\") --- e(\"publish <br/> [heap scan]\") d(\"&bowtie;<sub>publish.book_id = book.id</sub> <br/> [block nested loop]\") --- f(\"&sigma;<sub>book.date = 2020</sub>\") f(\"&sigma;<sub>book.date = 2020</sub>\") --- g(\"book<br/>[heap scan]\") In the above physical query plan, we assume following indices are preesent article.id (it's a primary key anyway) The physical plan applies the index nested loop to the outer join by using article as the inner relation. Though we preform a heap scan on the article relation, the operation is also a clustered index scan, since article is having primary key index on id . Note that there could be many physical query plans generated from a logical plan. Given a set of physical query plans, we need to find the \"best\" one (with the lowest I/O cost). It turns out that it is very challenging problem, and we will discuss about this shortly. This step is called query optimization . Suppose we identify a good physical query plan, we translate it into actual code to be executed in the database host machine. This final step is called query execution . stateDiagram-v2 s1 : query s2 : logical plans s1 --> s2 : query rewriter s3 : physical plans s2 --> s3 : plan generation / cost estimation s4 : result s3 --> s4 : execution","title":"Query Life Cycle"},{"location":"notes/l8_query_optimization/#execution-model","text":"The input of an execution model is the logical query plan. Every operation in the query plan can be executed differently. There are several options Iterator Model (seen in our project!) Materialization Model Vector Model","title":"Execution Model"},{"location":"notes/l8_query_optimization/#iterator-model","text":"The main idea of the iterator model is \"on the fly\" or \"lazy\", i.e. tuples are not retrieved/computed until they are needed. The idea is actualized by implementing the physical access operation, filter operation, join operation and etc has iterators. One possible implementation in Java is to use the Iterator interface, in which classDiagram class Iterator~T~ { bool hasNext() T next() } <<interface>> Iterator The hasNext() method checks whether the iterator is currently not empty. The next() method advances the internal pointer and produces the next item. For instance \\(\\sigma_{book.date = 2020}(book)\\) , the table access of book is an instance of Iterator<Tuple> , The filter operator \\(\\sigma\\) takes the book iterator and returns an iterator instance Iterator<Tuple> , too. When the top most iterator is executed, it calls its children's iterators to produce the next value. One strong advantage of the iterator model is that it is memory efficient. For instance, in the running example, we don't need to load all the tuples from the book table into the buffer pool before the filtering takes place. There might be worst case scenarios in which the intermediate results are too huge for the RAM and needed to be written back to the disk and be reloaded when required. The downside of the iterator model is that sometimes we need to re-compute some common sub-queries, e.g. \\((\\sigma_{book.date = 2020}(book)) \\bowtie (\\sigma_{book.date = 2020}(book))\\) , we need to compute \\(\\sigma_{book.date = 2020}(book)\\) twice. We would be in the worst situation if an immediate result is needed as a whole, e.g. need to build a hash table, need to sort the immediate data first. In these situation, we have to block and consume all the children's data first.","title":"Iterator Model"},{"location":"notes/l8_query_optimization/#materialization-model","text":"In materialization model, immediate results are fully computed and emit before the outer operation starts its computation. We can think of each operator having its own local storage holding the immediate results. The advantage of this model is that we can re-use immediate results if they needed more than once and we ca use the immediate results to build immediate index, hash table, etc. It performs better when the immediate results are eventually part of the final results.","title":"Materialization Model"},{"location":"notes/l8_query_optimization/#vectorized-model","text":"The vectorized model, (AKA the batch model) is a hybrid of iterator and materialization model. Every operator in the vectorized model implements the next() method. The next() method returns a batch of tuples instead of a single one. The advantage of this model is to strike a balance between iterator and materialization. It is memory efficient, as we don't compute and store the entire immediate result, yet we can compute Hash table (e.g. grace hash join) when needed.","title":"Vectorized Model"},{"location":"notes/l8_query_optimization/#query-planning","text":"The process of query planning involves enumerating a large enough subset of all possible equivalent query given the input expression. Two relaional algebra expressions are consider equivalent if they produce the same set of results.","title":"Query Planning"},{"location":"notes/l8_query_optimization/#equivalence-rules","text":"Let \\(R\\) , \\(R'\\) and \\(R''\\) be relations, the subset of requivalence rules is as follows, \\(\\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_1 \\wedge c_2}(R)\\) \\(\\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_2}(\\sigma_{c_1}(R))\\) \\(\\Pi_{a_1}(\\Pi_{a_2} ... (\\Pi_{a_n}(R))) = \\Pi_{a_1}(R)\\) if \\(a_1 \\subseteq a_2 \\subseteq ... \\subseteq a_n\\) \\(\\sigma_{c}(R \\times R') \\equiv R \\bowtie_{c} R'\\) . \\(R \\bowtie_{c} R' \\equiv R' \\bowtie_{c} R\\) . \\(R \\bowtie_{c_1} (R' \\bowtie_{c_2} R'') \\equiv (R \\bowtie_{c_1} R') \\bowtie_{c_2} R''\\) \\(\\sigma_{c}(R) \\equiv R\\) if \\(c \\not \\subseteq attr(R)\\) \\(\\sigma_{c_1}(R\\bowtie_{c_2} R') \\equiv \\sigma_{c_1}(R) \\bowtie_{c_2} \\sigma_{c_1}(R')\\) \\(R \\cap R' \\equiv R' \\cap R\\) \\(R \\cup R' \\equiv R \\cup R\\) \\(R \\cap (R' \\cap R'') \\equiv (R \\cap R') \\cap R''\\) \\(R \\cup (R' \\cup R'') \\equiv (R \\cup R') \\cup R''\\) \\(\\Pi_{a_1 \\cup a_2} (R \\bowtie_{c} R') \\equiv \\Pi_{a_1}(R) \\bowtie_{c} \\Pi_{a_2}(R')\\) if \\(attr(c) \\subseteq a_1 \\cap a_2\\) . \\(\\Pi_{a_1 \\cup a_2} (R \\bowtie_{c} R') \\equiv \\Pi_{a_1 \\cup a_2}(\\Pi_{a_1\\cup a_3}(R) \\bowtie_{c} \\Pi_{a_2\\cup a_4}(R') )\\) if \\(attr(c) \\subseteq a_3 \\cap a_4\\) \\(\\Pi_{a}(R \\cup R') \\equiv \\Pi_{a}(R) \\cup \\Pi_{a}(R')\\) \\(\\Pi_{a}(\\sigma_{c}(R)) \\equiv \\sigma_{c}(\\Pi_{a}(R))\\) if \\(attr(c) \\subseteq a\\) \\(\\sigma_{c}(R - R') \\equiv \\sigma_{c}(R) - R'\\) \\(\\sigma_{c}(R \\cap R') \\equiv \\sigma_{c}(R) \\cap \\sigma_{c}(R')\\) \\(\\sigma_{c}(R \\cup R') \\equiv \\sigma_{c}(R) \\cup \\sigma_{c}(R')\\) With this subset of rules, we can already enumerate a substantially large set of alternative logical query plans from the original plan.","title":"Equivalence rules"},{"location":"notes/l8_query_optimization/#cost-estimation","text":"Recall that given a logical query plan, there are still multiple alternatives of physical query plans depending on the choice of access method, execution model, index and etc. The ultimate goal is to find the physical plan that has the lowest I/O cost. Without running the plans, we could only estimate the cost. From the previous lesson, we learned how to estimate the I/O cost for different access methods, selection, join and etc. The cost is subject to two extra information. the sizes of the relations the selectivity of the predicate. In DBMS, the sizes of the relations are recorded periodically in the catalog. The catalog sub-system keeps track of the following the statistics which can be used to approximate the selectivity. \\(N(R)\\) the number of tuples in \\(R\\) \\(V(a,R)\\) the number of unique values of attribute \\(a\\) in \\(R\\) . \\(min(a,R)\\) the minumum value of attribute \\(a\\) in \\(R\\) \\(max(a,R)\\) the maximum value of attribtue \\(a\\) in \\(R\\) .","title":"Cost Estimation"},{"location":"notes/l8_query_optimization/#cardinality-estimation","text":"Given the above catalog statistics, we can estimate the selectivity ratio, \\(\\alpha(c, R)\\) .","title":"Cardinality Estimation"},{"location":"notes/l8_query_optimization/#equality-predicate","text":"In case \\(c\\) is \\(a = v\\) where \\(a\\) is an attribute of \\(R\\) and \\(v\\) is a value, \\(\\alpha(a = v, R) = 1 / V(a,R)\\) .","title":"Equality Predicate"},{"location":"notes/l8_query_optimization/#range-equality-predicate","text":"In case \\(c\\) is \\(a > v\\) where \\(a\\) is an attribute of \\(R\\) and \\(v\\) is a value, \\(\\alpha(a > v, R) = (max(a,R) - v) / (max(a,R) - min(a,R) + 1)\\) assuming values of \\(a\\) in \\(R\\) follow a uniform distribution. In case the distribution is not uniform, we may use a histogram with binning to obtain a better estimation. However building historgram for a large table could be expenive. In case of large data set, DBMS often collect a small sample to build the histogram.","title":"Range Equality Predicate"},{"location":"notes/l8_query_optimization/#conjunction-predicate","text":"In case \\(c\\) is \\(c_1 \\wedge c_2\\) , assuming values constrained by \\(c_1\\) are independent of those constrained by \\(c_2\\) , \\(\\alpha(c_1 \\wedge c_2, R) = \\alpha(c_1, R) \\cdot \\alpha(c_2, R)\\) .","title":"Conjunction Predicate"},{"location":"notes/l8_query_optimization/#search-algorithm","text":"The last piece of the query optimization is to search for the best physical plan given a method to enumerate alternative a way to estimate the I/O cost given a plan","title":"Search Algorithm"},{"location":"notes/l8_query_optimization/#the-challenge","text":"A naive approach would generate all possible plans, apply estimation to all of them and find the best one. However enumerating all possible plans could be expensive. For instance, enumerating all plan of a \\(n\\) -way join will yield \\(4^{n}\\) possibilities. Recall from our algorithm class that the number of possible Binary Search Trees with \\(n\\) different keys is catalan number \\[C_n = (2\\cdot n)! / ((n + 1)! \\cdot n!)\\] As \\(n\\) grows, catalan numbers grow as \\[4^n / (n^{3/2} \\cdot \\sqrt{\\pi}) \\] A set of possible query plans with \\(n\\) -way join shares the same size!","title":"The challenge"},{"location":"notes/l8_query_optimization/#selinger-algorithm","text":"The selinger algorithm is one of the classic algorthm for searching for optimal plan. The idea behind is to apply the following heuristics Only consider the left-skew trees. This is because in many different physical plans, we need to scan the right sub-tree multiple times in join. It is better to keep the right sub-tree as simple as possible. If we apply this heuristic, we cut down the search base to \\[ \\left (\\begin{array}{c} n \\\\ n - 1 \\end{array} \\right ) \\cdot \\left (\\begin{array}{c} n - 1 \\\\ n - 2 \\end{array} \\right ) \\cdot ... \\cdot \\left (\\begin{array}{c} 2\\\\ 1 \\end{array} \\right ) = n! \\] Considering the left-skew trees with \\(n\\) relations still computing \\(n!\\) possible plans. To further cut down the search, the algorithm assumes the best overall plan consists of best sub-plans. It proceeds by finding the best plans for leaf nodes, then \"walk-up\" the trees by finding the best plans to combine intermediate steps. For example, Pass 1. find the best plans for each relations (leaf nodes) Pass 2. find the best plans to join any 2 relations Pass 3. find the best plans to join any 3 relations ... For each pass \\(i\\) where \\(i>1\\) , the best plans are computed based on the result from pass \\(i-1\\) . Overall this cut down the final search space of an \\(n\\) -way join to \\[ \\left (\\begin{array}{c} n \\\\ n-1 \\end{array} \\right ) \\] And the overall time-complexity of \\[ \\left (\\begin{array}{c} n \\\\ n - 1 \\end{array} \\right ) + \\left (\\begin{array}{c} n \\\\ n - 2 \\end{array} \\right ) + ... + \\left (\\begin{array}{c} n\\\\ 1 \\end{array} \\right ) = O(2^n) \\] For instance, recall our running example, \\[ \\Pi_{article.name} (article \\bowtie_{article.id=publish.article\\_id} publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book)) \\] For simplicity, we only focus on the join expression \\[ article \\bowtie_{article.id=publish.article\\_id} publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book) \\] Assuming there is no index created on \\(book.date\\) . Pass 1 we find the cheapest way to access \\(\\sigma_{book.date = 2020}(book)\\) , (because \\(\\sigma\\) is not a join), is an heap scan. the cheapest way to acccess \\(publish\\) is heap scan. the cheapest way to accesss \\(artcle\\) is heap scan. Pass 2 immediate relation 1: we join \\(publish\\) and \\(article\\) by taking the cheapest access methods from Pass 1, hence the cheapest way to join \\(publish\\) and \\(article\\) is an index nested join on \\(article.id\\) , let's say the cost is 1000. immediate relation 2: we join \\(publish\\) and \\(\\sigma_{book.date = 2020}(book)\\) , let's say the cheapest way to join is an block nested join with \\(\\sigma_{book.date = 2020}(book)\\) materialized, the cost is 2750. immediate relation 3: we join \\(article\\) and \\(\\sigma_{book.date = 2020}(book)\\) . There is no common attribute, hence it is a cross product. The cost is 25000. Pass 3 alternative 1: we join \\(\\sigma_{book.date = 2020}(book)\\) with immediate relation 1 from Pass 2. Let's say the cheapest cost is 5000 with a block nested loop. alternative 2: we join \\(article\\) with immediate relation 2 from Pass 2. Let's say the cheapest cost is 6000 with an index nested loop. alternative 3: we join \\(publish\\) with the immediate rlation 3, with an index nested loop, the cost is 50000. Overall, the cheapest plan is alterantive 1. Note that in real world DBMS, the query optimizer would prepare two best plans for each relation for all passes, an unordered plan and a best index plan. With these adjustment, we have double the size of the top-level search space and the overall time complexity.","title":"Selinger Algorithm"},{"location":"notes/l8_query_optimization/#optional-reading-translating-sql-into-relational-algebra","text":"Translating SQL into the Relational Algebra","title":"Optional Reading - Translating SQL into relational algebra"},{"location":"notes/l9_transaction_concurrency/","text":"50.043 - Transactions and Concurrency Control Learning Outcomes By the end of this unit, you should be able to Explain serializability, confict serializability. Apply conflict serializability check to schedule. Explain Strict 2PL and 2PL Apply Strict 2PL and 2PL to generate conflict serialiable schedules. Recall Isolation Recall from the previous unit, we learn that the isoliation property ensures that multiple concurrent transactions must not be dependent on one another. In other words, if the DBMS executing these concurrent transactions by interleaving, the result be the same as executing them in a sequential order. Serializability Suppose we have a set of tasks \\(t_1,...,t_n\\) , each task \\(t_i\\) consists of a sequence of instructions \\(c^i_1,..., c^i_{m_i}\\) . Schedule We call a sequence of distinct instructions \\(c_1,...,c_k\\) a schedule of tasks \\(t_1,...,t_n\\) , iff for all \\(i\\in\\{1,n\\}\\) , for all \\(p \\in \\{1, m_i\\}\\) , we find \\(c^i_p \\in \\{c_1,...,c_k\\}\\) . for all \\(q \\in \\{1,k\\}\\) , there exist \\(i\\in\\{1,n\\}\\) and \\(p \\in \\{1, m_i\\}\\) such that \\(c_q = c^i_p\\) . for all \\(q \\in \\{1,k\\}\\) and \\(q' \\in \\{q,k\\}\\) , there not exist \\(i\\in\\{1,n\\}\\) and \\(p \\in \\{1, m_i\\}\\) and \\(p' \\in \\{p, m_i\\}\\) such that \\(c_q = c^i_{p'}\\) and \\(c_{q'} = c^i_p\\) . The first rule enforces all instructions in the tasks can be found in the schedule. The second rule enforces all instructions in the schedule can be found in the tasks. The third rule enforce that the relative order between instructions from the same task are maintained in the schedule. Serial Schedule Given tasks \\(t_1,...,t_n\\) and a schedule \\(c_1,...,c_k\\) , we say \\(c_1,...,c_k\\) is a serial schedule of \\(t_1,...,t_n\\) iff there not exists a sub-sequence \\(c_{q-1}, c_{q}, c_{q+1}\\) of \\(c_1,...,c_k\\) such that \\(c_{q-1}\\) and \\(c_{q+1}\\) are instructions from task \\(t_j\\) and \\(c_q\\) is an instruction from task \\(t_i\\) , for some \\(i\\) and \\(j\\) in \\(\\{1,n\\}\\) . In other words, a schedule is serial iff there is no interleaving. Note that given a set of tasks \\(t_1,...,t_n\\) , there are in total \\(n!\\) serial schedules. State and Instruction Given two database states \\(s_1\\) and \\(s_2\\) , and an instruction \\(c\\) , we say \\(s_1\\vdash c \\rightarrow s_2\\) to denote execution \\(c\\) in the database with in state \\(s_1\\) results in state \\(s_2\\) . Inductively, we can define \\(s_1 \\vdash c_1,c_2,...,c_n \\rightarrow s_n\\) iff \\(s_1 \\vdash c_1 \\rightarrow s_2\\) and \\(s_2 \\vdash c_2,...,c_n \\rightarrow s_n\\) . Serializable Schedule Given tasks \\(t_1,...,t_n\\) and a schedule \\(c_1,...,c_k\\) , we say \\(c_1,...,c_k\\) is serializable iff there exists a serial schedule of \\(t_1,...,t_n\\) , i.e. \\(c_1',...,c_k'\\) such that \\(s_1 \\vdash c_1,...,c_k \\rightarrow s_2\\) and \\(s_1 \\vdash c_1',...,c_k' \\rightarrow s_2'\\) and \\(s_2 = s_2'\\) . In other words, a schedule is serializable if its effect is the same as a serial schedule after execution. For example, consider the following transaction -- transaction 1 begin; select @bal1 := bal from saving where id=1001; -- c1 update saving set bal = @bal1-100 where id=1001; -- c2 commit; -- transaction 2 begin; select @bal2 := bal from saving where id=1001; -- c1' update saving set bal = @bal2*1.05 where id=1001; -- c2' commit; The schedules c1,c2,c1',c2' and c1',c2',c1,c2 are serializable schedules, but c1,c1',c2,c2' is not a serializable schedule, we illustrate the state and instruction execution as follows state before instruction state after 100 c1 100, bal1 = 100 100,bal1 = 100 c1' 100, bal1 = 100, bal2 = 100 100, bal1 = 100, bal2 = 100 c2 0, bal1 = 100, bal2 = 100 0, bal1 = 100, bal2 = 100 c2' 105, bal1 = 100, bal2 = 100 At the end of execution, we found the balance of account 1001 is 105 though we withdraw 100 from this account in transaction1. Serializability Check Our dream approach is to be able to check whether the given schedule is serializable without executing it. However such a check is very expensive because there are \\(n!\\) possible serial schedules we need to verify the given one against. we need to model the set of possible operations and the value domains (without executing them). Our second best approach is to use a conservative approximation check, say \\(C(s)\\) where \\(C\\) is the approximation check and \\(s\\) is a schedule, such that \\(C(s)\\) yields true implies \\(s\\) is serializable. Note that we don't need to guarantee the inverse direction still holds (since \\(C\\) is a conservative approximation). Conflict Serializability Check One possible of consevative approximation is Conflict Serializability Check . Update of terminologies From this point onwards, we sometimes refer an instruction \\(c\\) as an operator, which could be \\(R(A)\\) or \\(W(A)\\) where \\(R(\\cdot)\\) stands for the read operator, and \\(A\\) is the object that being read, \\(W(\\cdot)\\) denotes a write operator. a task \\(t\\) as an transaction \\(T\\) . a state \\(s\\) is a mapping from objects, such as \\(A\\) , \\(B\\) , etc to values. Operator Conflict Two instructions \\(c_1\\) and \\(c_2\\) are conflict iff \\(c_1\\) and \\(c_2\\) belong to different transactions and \\((c_1 = R(A) \\wedge c_2 = W(A))\\) or \\((c_1 = W(A) \\wedge c_2 = R(A))\\) or \\((c_1 = W(A) \\wedge c_2 = W(A))\\) for some common object \\(A\\) in the state. Execution Order Given a schedule \\(X = c_1,...,c_n\\) , we say \\(c_i \\prec c_j\\) iff \\(1 \\leq i \\wedge i < j \\wedge j \\leq n\\) Intuitively, \\(c_i \\prec c_j\\) means \\(c_i\\) is executed before \\(c_j\\) in a schedule \\(X\\) . Conflict Equivalence Given two schedules \\(X = c_1,...,c_n\\) and \\(X' = c_1',...,c_n'\\) are conflict equivalent iff both schedules belong to the same set of transactions. for any pair of conflicting operators \\((c_i, c_j)\\) in \\(X\\) , such that \\(c_i \\prec c_j\\) , we find \\((c_k', c_l')\\) in \\(X'\\) such that \\(c_k' \\prec c_l'\\) and \\(c_i = c_k'\\) and \\(c_j = c_l'\\) . In other words, the second condition in the above definition says that all conflicting operators in \\(X\\) have the same relative execution order (within the conflicting pair) in \\(X'\\) . Conflict Serializable Given a set of tasks \\(t_1,...,t_n\\) , a schedule \\(X\\) is conflict serializable iff it is conflict equivalent to some schedule \\(X'\\) of tasks \\(t_1,...,t_n\\) such that \\(X'\\) is serial. An example Consider the following example with two transactions T1: R(A), W(A), R(B), W(B) T2: R(A), W(A), R(B), W(B) Let's say T1:R(A), W(A) is to increment A 's value by 10. T1:R(B), W(B) is to increment B 's value by 10. T2:R(A), W(A) is to double A 's value. T2:R(B), W(B) is to double B 's value. Note that there are 2! = 2 possible serial schedules. Serial schedule 1 T1 T2 R(A) W(A) R(B) W(B) R(A) W(A) R(B) W(B) assuming the initial state is {A:10, B:10} , the final state of the above is {A:40, B:40} . Serial schedule 2 T1 T2 R(A) W(A) R(B) W(B) R(A) W(A) R(B) W(B) assuming the initial state is {A:10, B:10} , the final state of the above is {A:30, B:30} . Now let's consider the following schedule (let's call it schedule 3) T1 T2 R(A) W(A) R(A) W(A) R(B) W(B) R(B) W(B) Note that the above schedule is not conflict serializable, because conflicting operator pair \\(T2:W(B) \\prec T1:R(B)\\) cannot be reordered to obtain serial schedule 1, and conflicting pair \\(T1:W(A) \\prec T2:R(A)\\) cannot be reordered to obtain serial schedule 2. If we were to execute the above non-conflict serializable schedule with the init state {A:10, B:10} , we get {A:40, B:30} . Recall that conflict serialiability is a conservative approximation of serialzability. Suppose we change the operation of T2 T2:R(A), W(A) is to increase A 's value by 20. T2:R(B), W(B) is to increase B 's value by 20. Given the init state {A:10, B:10} , serial schedule 1, serial schedule 2 and schedule 3 generate {A:30, B:30} , which means that after the change of the operation of T2 , schedule 3 becomes serializable, but still not conflict serializable. Conflict Serializable Check Algorithm In the above example, we illustrate the Conflict Serializable check via a table and manual checking. To implement the idea as an algorithm we could leverage on a directed graph, called precedence graph. The precedence graph is generated from a schedule \\(X\\) as follow. For each transaction \\(t\\) , create a node \\(n_t\\) . For each pair of transactions \\(t\\) and \\(t'\\) , if there exists a conflicting pair of operators \\(t:c \\prec t':c'\\) in \\(X\\) , we draw an directed arc from \\(n_t\\) to \\(n_{t'}\\) . A schedule \\(X\\) is conflict serialzable if its precendence graph is acyclic. For instance, the precendence graph of the schedule 3 in the previous example is as follows graph T1 --> T2 T1 --> T2 T2 --> T1 T2 --> T1 Two Phase Locking Generate schedule in a bruteforce way then check for conflict serializablity is still impractical. DBMSes use Two Phase Locking (2PL) to generate conflict serializable schedule. Strict 2PL Let's consider a strict version of two phase locking is called Strict 2PL. It is governed by the following two rules. When a transaction need to read an object \\(A\\) , it needs to acquire a shared lock \\(S(A)\\) ; when it need to write an object \\(A\\) , it needs to aquire an exclusive lock \\(X(A)\\) . A shared lock can be shared by multiple reader transactions. An exclusive lock cannot be shared. All locks are only released \\(U(A)\\) , when the transaction commit. The requests for lock \\(S(A)\\) , \\(X(A)\\) and release of lock \\(U(A)\\) are inserted by the DBMSes as schedule being generated. Let's consider the earlier example with with two transactions T1: R(A), W(A), R(B), W(B) T2: R(A), W(A), R(B), W(B) Suppose T1 starts slightly earlier than T2 . A schedule is generated with strict 2PL as follows Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 R(B) 6 W(B) 7 U(A), U(B), Commit 8 X(A) 9 R(A) 10 W(A) 11 X(B) 12 R(B) 13 W(B) 14 U(A), U(B), Commit At timestep 1, T1 acquires the exclusive lock on A since it needs to read and write to it. T2 is blocked as it would also need to acquire an exclusive lock on A to start. At timestep 4, T1 acquires the exclusive lock on B . At timestep 7, T1 is commited, hence both locks are released. At timestep 8, T2 acquires lock on A and starts. 2PL Once issue with Strict 2PL is that a transaction holds on to the locks it acquires until it commits, this blocks off other possible concurrent execution opportunity, hence throughput might be affected. A variant of Strict 2PL, known as 2PL, is governed by the following two rules. (First rule is the same as Strict 2PL, the second rule is different). When a transaction needs to read an object \\(A\\) , it needs to acquire a shared lock \\(S(A)\\) ; when it needs to write an object \\(A\\) , it needs to aquire an exclusive lock \\(X(A)\\) . A shared lock can be shared by multiple reader transactions. An exclusive lock cannot be shared. A transaction releases the lock right after usage. A transaction cannot acquire new lock once it releases some lock. The reason why this is called a two phase locking is that there is a phase of lock acquisition, the number of locks being acquired by a transaction grows until it starts to release some lock, then it will release all the locks gradually. Given the same example, a schedule is generated with 2PL as follows Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 U(A) 6 X(A) 7 R(A) 8 W(A) 9 R(B) 10 W(B) 11 U(B), Commit 12 X(B) 13 U(A) 14 R(B) 15 W(B) 16 U(B), Commit At timestep 1, T1 acquires an exclusive lock on A and starts, T2 is blocked and waits. At timestep 4, T1 is done with A , but needs to acquire an exclusive lock on B before releasing A . At timestep 6, T2 acuiqres an exclusive lock on A and starts and runs until timestep 8, it needs to acquire an exclusive lock on B but it is blocked. At timesteps 9-10, T1 operates on B . At timestep 11, T1 releases lock on B and is commited. At timestep 12, T2 is unblocked and acquires the exclusive lock on B , it then releases lock on A and continues with the rest of operations. Note that schedules generated by both Strict 2PL and 2PL are conflict serializable . (Proof omitted.) Strict 2PL vs 2PL 2PL in general yields a better throughput compared to Strict 2PL by allowing some transaction to start early. Such gain comes with a price. 2PL might produce schedules that are harder to recover due to cascading abort. Consider the following schedule generated by 2PL, we keep the begin and commit explicit. Timestep T1 T2 1 begin 2 X(A) 3 A=10 4 X(B) 5 U(A) 6 begin 7 X(A) 8 A=A+1 9 U(A) 10 commit Assume the initial state is A=0 , executing the above schedule yields the following Undo/Redo Log 1. <T1 begin> 2. <T1 update A 0 10> 3. <T2 begin> 4. <T2 update A 10 11> 5. <T2 commit> After timestep 11, the database crashes. During the recovery phase, we start with undo phase, we find that T1 need to be undone and aborted. However, T2's effect of A=A+1 is based on the assumption that A=10 in T1 was successful. As a result, T2 has be to aborted and undone too. This is known as cascading abort. Conservative 2PL Another variant of Strict 2PL is called Conservative 2PL . It is same as Strick 2PL except that a transaction acquires all the locks it needs at the very begining. For instance, given the previous example, a schedule is generated with conservative 2PL as follows Timestep T1 T2 1 X(A),X(B) 2 R(A) 3 W(A) 4 R(B) 5 W(B) 6 U(A), U(B), Commit 7 X(A),X(B) 8 R(A) 9 W(A) 10 X(B) 11 R(B) 12 W(B) 13 U(A), U(B), Commit Note that all the locks are acquired by a transaction in an atomic operation, i.e. all or none. This is a way to prevent deadlock. Deadlock Let's consider how deadlock can be caused by 2PL (and Strict 2PL). Consider the following transactions T1: R(A), W(A), R(B), W(B) T2: R(B), W(B), R(A), W(A) and a schedule Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 R(B) 6 W(B) 7 try X(B) but blocked try X(A) but blocked At timestep 1, T1 acquires an exclusive lock on A and starts At timestep 4, T2 acquires an exclusive lock on B and starts AT timestep 7, T1 tries to get a lock on B but it gets blocked. T2 tries to get a lock on A but it is blocked too. A deadlock arises. Deadlock detection A way to detect deadlock is that when transactions waiting for locks, the lock manager (part of DBMS) generate a wait-for graph. In a wait for graph, each active transaction \\(t\\) is a node \\(n_t\\) . an arc going from \\(n_t\\) to \\(n_t'\\) if transaction \\(t\\) is waiting for a lock which is being held by \\(t'\\) . For insance, the wait-for graph from the previous example is graph T1 --> T2 T2 --> T1 Deadlock prevention To prevent deadlock, there are several approaches Use conservative 2PL, but throughput will be affected. Set priorities. Transactions with higher priorty get the lock. Transaction wth lower prioerty get aborted and retry.","title":"50.043 - Transactions and Concurrency Control"},{"location":"notes/l9_transaction_concurrency/#50043-transactions-and-concurrency-control","text":"","title":"50.043 - Transactions and Concurrency Control"},{"location":"notes/l9_transaction_concurrency/#learning-outcomes","text":"By the end of this unit, you should be able to Explain serializability, confict serializability. Apply conflict serializability check to schedule. Explain Strict 2PL and 2PL Apply Strict 2PL and 2PL to generate conflict serialiable schedules.","title":"Learning Outcomes"},{"location":"notes/l9_transaction_concurrency/#recall-isolation","text":"Recall from the previous unit, we learn that the isoliation property ensures that multiple concurrent transactions must not be dependent on one another. In other words, if the DBMS executing these concurrent transactions by interleaving, the result be the same as executing them in a sequential order.","title":"Recall Isolation"},{"location":"notes/l9_transaction_concurrency/#serializability","text":"Suppose we have a set of tasks \\(t_1,...,t_n\\) , each task \\(t_i\\) consists of a sequence of instructions \\(c^i_1,..., c^i_{m_i}\\) .","title":"Serializability"},{"location":"notes/l9_transaction_concurrency/#schedule","text":"We call a sequence of distinct instructions \\(c_1,...,c_k\\) a schedule of tasks \\(t_1,...,t_n\\) , iff for all \\(i\\in\\{1,n\\}\\) , for all \\(p \\in \\{1, m_i\\}\\) , we find \\(c^i_p \\in \\{c_1,...,c_k\\}\\) . for all \\(q \\in \\{1,k\\}\\) , there exist \\(i\\in\\{1,n\\}\\) and \\(p \\in \\{1, m_i\\}\\) such that \\(c_q = c^i_p\\) . for all \\(q \\in \\{1,k\\}\\) and \\(q' \\in \\{q,k\\}\\) , there not exist \\(i\\in\\{1,n\\}\\) and \\(p \\in \\{1, m_i\\}\\) and \\(p' \\in \\{p, m_i\\}\\) such that \\(c_q = c^i_{p'}\\) and \\(c_{q'} = c^i_p\\) . The first rule enforces all instructions in the tasks can be found in the schedule. The second rule enforces all instructions in the schedule can be found in the tasks. The third rule enforce that the relative order between instructions from the same task are maintained in the schedule.","title":"Schedule"},{"location":"notes/l9_transaction_concurrency/#serial-schedule","text":"Given tasks \\(t_1,...,t_n\\) and a schedule \\(c_1,...,c_k\\) , we say \\(c_1,...,c_k\\) is a serial schedule of \\(t_1,...,t_n\\) iff there not exists a sub-sequence \\(c_{q-1}, c_{q}, c_{q+1}\\) of \\(c_1,...,c_k\\) such that \\(c_{q-1}\\) and \\(c_{q+1}\\) are instructions from task \\(t_j\\) and \\(c_q\\) is an instruction from task \\(t_i\\) , for some \\(i\\) and \\(j\\) in \\(\\{1,n\\}\\) . In other words, a schedule is serial iff there is no interleaving. Note that given a set of tasks \\(t_1,...,t_n\\) , there are in total \\(n!\\) serial schedules.","title":"Serial Schedule"},{"location":"notes/l9_transaction_concurrency/#state-and-instruction","text":"Given two database states \\(s_1\\) and \\(s_2\\) , and an instruction \\(c\\) , we say \\(s_1\\vdash c \\rightarrow s_2\\) to denote execution \\(c\\) in the database with in state \\(s_1\\) results in state \\(s_2\\) . Inductively, we can define \\(s_1 \\vdash c_1,c_2,...,c_n \\rightarrow s_n\\) iff \\(s_1 \\vdash c_1 \\rightarrow s_2\\) and \\(s_2 \\vdash c_2,...,c_n \\rightarrow s_n\\) .","title":"State and Instruction"},{"location":"notes/l9_transaction_concurrency/#serializable-schedule","text":"Given tasks \\(t_1,...,t_n\\) and a schedule \\(c_1,...,c_k\\) , we say \\(c_1,...,c_k\\) is serializable iff there exists a serial schedule of \\(t_1,...,t_n\\) , i.e. \\(c_1',...,c_k'\\) such that \\(s_1 \\vdash c_1,...,c_k \\rightarrow s_2\\) and \\(s_1 \\vdash c_1',...,c_k' \\rightarrow s_2'\\) and \\(s_2 = s_2'\\) . In other words, a schedule is serializable if its effect is the same as a serial schedule after execution. For example, consider the following transaction -- transaction 1 begin; select @bal1 := bal from saving where id=1001; -- c1 update saving set bal = @bal1-100 where id=1001; -- c2 commit; -- transaction 2 begin; select @bal2 := bal from saving where id=1001; -- c1' update saving set bal = @bal2*1.05 where id=1001; -- c2' commit; The schedules c1,c2,c1',c2' and c1',c2',c1,c2 are serializable schedules, but c1,c1',c2,c2' is not a serializable schedule, we illustrate the state and instruction execution as follows state before instruction state after 100 c1 100, bal1 = 100 100,bal1 = 100 c1' 100, bal1 = 100, bal2 = 100 100, bal1 = 100, bal2 = 100 c2 0, bal1 = 100, bal2 = 100 0, bal1 = 100, bal2 = 100 c2' 105, bal1 = 100, bal2 = 100 At the end of execution, we found the balance of account 1001 is 105 though we withdraw 100 from this account in transaction1.","title":"Serializable Schedule"},{"location":"notes/l9_transaction_concurrency/#serializability-check","text":"Our dream approach is to be able to check whether the given schedule is serializable without executing it. However such a check is very expensive because there are \\(n!\\) possible serial schedules we need to verify the given one against. we need to model the set of possible operations and the value domains (without executing them). Our second best approach is to use a conservative approximation check, say \\(C(s)\\) where \\(C\\) is the approximation check and \\(s\\) is a schedule, such that \\(C(s)\\) yields true implies \\(s\\) is serializable. Note that we don't need to guarantee the inverse direction still holds (since \\(C\\) is a conservative approximation).","title":"Serializability Check"},{"location":"notes/l9_transaction_concurrency/#conflict-serializability-check","text":"One possible of consevative approximation is Conflict Serializability Check .","title":"Conflict Serializability Check"},{"location":"notes/l9_transaction_concurrency/#update-of-terminologies","text":"From this point onwards, we sometimes refer an instruction \\(c\\) as an operator, which could be \\(R(A)\\) or \\(W(A)\\) where \\(R(\\cdot)\\) stands for the read operator, and \\(A\\) is the object that being read, \\(W(\\cdot)\\) denotes a write operator. a task \\(t\\) as an transaction \\(T\\) . a state \\(s\\) is a mapping from objects, such as \\(A\\) , \\(B\\) , etc to values.","title":"Update of terminologies"},{"location":"notes/l9_transaction_concurrency/#operator-conflict","text":"Two instructions \\(c_1\\) and \\(c_2\\) are conflict iff \\(c_1\\) and \\(c_2\\) belong to different transactions and \\((c_1 = R(A) \\wedge c_2 = W(A))\\) or \\((c_1 = W(A) \\wedge c_2 = R(A))\\) or \\((c_1 = W(A) \\wedge c_2 = W(A))\\) for some common object \\(A\\) in the state.","title":"Operator Conflict"},{"location":"notes/l9_transaction_concurrency/#execution-order","text":"Given a schedule \\(X = c_1,...,c_n\\) , we say \\(c_i \\prec c_j\\) iff \\(1 \\leq i \\wedge i < j \\wedge j \\leq n\\) Intuitively, \\(c_i \\prec c_j\\) means \\(c_i\\) is executed before \\(c_j\\) in a schedule \\(X\\) .","title":"Execution Order"},{"location":"notes/l9_transaction_concurrency/#conflict-equivalence","text":"Given two schedules \\(X = c_1,...,c_n\\) and \\(X' = c_1',...,c_n'\\) are conflict equivalent iff both schedules belong to the same set of transactions. for any pair of conflicting operators \\((c_i, c_j)\\) in \\(X\\) , such that \\(c_i \\prec c_j\\) , we find \\((c_k', c_l')\\) in \\(X'\\) such that \\(c_k' \\prec c_l'\\) and \\(c_i = c_k'\\) and \\(c_j = c_l'\\) . In other words, the second condition in the above definition says that all conflicting operators in \\(X\\) have the same relative execution order (within the conflicting pair) in \\(X'\\) .","title":"Conflict Equivalence"},{"location":"notes/l9_transaction_concurrency/#conflict-serializable","text":"Given a set of tasks \\(t_1,...,t_n\\) , a schedule \\(X\\) is conflict serializable iff it is conflict equivalent to some schedule \\(X'\\) of tasks \\(t_1,...,t_n\\) such that \\(X'\\) is serial.","title":"Conflict Serializable"},{"location":"notes/l9_transaction_concurrency/#an-example","text":"Consider the following example with two transactions T1: R(A), W(A), R(B), W(B) T2: R(A), W(A), R(B), W(B) Let's say T1:R(A), W(A) is to increment A 's value by 10. T1:R(B), W(B) is to increment B 's value by 10. T2:R(A), W(A) is to double A 's value. T2:R(B), W(B) is to double B 's value. Note that there are 2! = 2 possible serial schedules. Serial schedule 1 T1 T2 R(A) W(A) R(B) W(B) R(A) W(A) R(B) W(B) assuming the initial state is {A:10, B:10} , the final state of the above is {A:40, B:40} . Serial schedule 2 T1 T2 R(A) W(A) R(B) W(B) R(A) W(A) R(B) W(B) assuming the initial state is {A:10, B:10} , the final state of the above is {A:30, B:30} . Now let's consider the following schedule (let's call it schedule 3) T1 T2 R(A) W(A) R(A) W(A) R(B) W(B) R(B) W(B) Note that the above schedule is not conflict serializable, because conflicting operator pair \\(T2:W(B) \\prec T1:R(B)\\) cannot be reordered to obtain serial schedule 1, and conflicting pair \\(T1:W(A) \\prec T2:R(A)\\) cannot be reordered to obtain serial schedule 2. If we were to execute the above non-conflict serializable schedule with the init state {A:10, B:10} , we get {A:40, B:30} . Recall that conflict serialiability is a conservative approximation of serialzability. Suppose we change the operation of T2 T2:R(A), W(A) is to increase A 's value by 20. T2:R(B), W(B) is to increase B 's value by 20. Given the init state {A:10, B:10} , serial schedule 1, serial schedule 2 and schedule 3 generate {A:30, B:30} , which means that after the change of the operation of T2 , schedule 3 becomes serializable, but still not conflict serializable.","title":"An example"},{"location":"notes/l9_transaction_concurrency/#conflict-serializable-check-algorithm","text":"In the above example, we illustrate the Conflict Serializable check via a table and manual checking. To implement the idea as an algorithm we could leverage on a directed graph, called precedence graph. The precedence graph is generated from a schedule \\(X\\) as follow. For each transaction \\(t\\) , create a node \\(n_t\\) . For each pair of transactions \\(t\\) and \\(t'\\) , if there exists a conflicting pair of operators \\(t:c \\prec t':c'\\) in \\(X\\) , we draw an directed arc from \\(n_t\\) to \\(n_{t'}\\) . A schedule \\(X\\) is conflict serialzable if its precendence graph is acyclic. For instance, the precendence graph of the schedule 3 in the previous example is as follows graph T1 --> T2 T1 --> T2 T2 --> T1 T2 --> T1","title":"Conflict Serializable Check Algorithm"},{"location":"notes/l9_transaction_concurrency/#two-phase-locking","text":"Generate schedule in a bruteforce way then check for conflict serializablity is still impractical. DBMSes use Two Phase Locking (2PL) to generate conflict serializable schedule.","title":"Two Phase Locking"},{"location":"notes/l9_transaction_concurrency/#strict-2pl","text":"Let's consider a strict version of two phase locking is called Strict 2PL. It is governed by the following two rules. When a transaction need to read an object \\(A\\) , it needs to acquire a shared lock \\(S(A)\\) ; when it need to write an object \\(A\\) , it needs to aquire an exclusive lock \\(X(A)\\) . A shared lock can be shared by multiple reader transactions. An exclusive lock cannot be shared. All locks are only released \\(U(A)\\) , when the transaction commit. The requests for lock \\(S(A)\\) , \\(X(A)\\) and release of lock \\(U(A)\\) are inserted by the DBMSes as schedule being generated. Let's consider the earlier example with with two transactions T1: R(A), W(A), R(B), W(B) T2: R(A), W(A), R(B), W(B) Suppose T1 starts slightly earlier than T2 . A schedule is generated with strict 2PL as follows Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 R(B) 6 W(B) 7 U(A), U(B), Commit 8 X(A) 9 R(A) 10 W(A) 11 X(B) 12 R(B) 13 W(B) 14 U(A), U(B), Commit At timestep 1, T1 acquires the exclusive lock on A since it needs to read and write to it. T2 is blocked as it would also need to acquire an exclusive lock on A to start. At timestep 4, T1 acquires the exclusive lock on B . At timestep 7, T1 is commited, hence both locks are released. At timestep 8, T2 acquires lock on A and starts.","title":"Strict 2PL"},{"location":"notes/l9_transaction_concurrency/#2pl","text":"Once issue with Strict 2PL is that a transaction holds on to the locks it acquires until it commits, this blocks off other possible concurrent execution opportunity, hence throughput might be affected. A variant of Strict 2PL, known as 2PL, is governed by the following two rules. (First rule is the same as Strict 2PL, the second rule is different). When a transaction needs to read an object \\(A\\) , it needs to acquire a shared lock \\(S(A)\\) ; when it needs to write an object \\(A\\) , it needs to aquire an exclusive lock \\(X(A)\\) . A shared lock can be shared by multiple reader transactions. An exclusive lock cannot be shared. A transaction releases the lock right after usage. A transaction cannot acquire new lock once it releases some lock. The reason why this is called a two phase locking is that there is a phase of lock acquisition, the number of locks being acquired by a transaction grows until it starts to release some lock, then it will release all the locks gradually. Given the same example, a schedule is generated with 2PL as follows Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 U(A) 6 X(A) 7 R(A) 8 W(A) 9 R(B) 10 W(B) 11 U(B), Commit 12 X(B) 13 U(A) 14 R(B) 15 W(B) 16 U(B), Commit At timestep 1, T1 acquires an exclusive lock on A and starts, T2 is blocked and waits. At timestep 4, T1 is done with A , but needs to acquire an exclusive lock on B before releasing A . At timestep 6, T2 acuiqres an exclusive lock on A and starts and runs until timestep 8, it needs to acquire an exclusive lock on B but it is blocked. At timesteps 9-10, T1 operates on B . At timestep 11, T1 releases lock on B and is commited. At timestep 12, T2 is unblocked and acquires the exclusive lock on B , it then releases lock on A and continues with the rest of operations. Note that schedules generated by both Strict 2PL and 2PL are conflict serializable . (Proof omitted.)","title":"2PL"},{"location":"notes/l9_transaction_concurrency/#strict-2pl-vs-2pl","text":"2PL in general yields a better throughput compared to Strict 2PL by allowing some transaction to start early. Such gain comes with a price. 2PL might produce schedules that are harder to recover due to cascading abort. Consider the following schedule generated by 2PL, we keep the begin and commit explicit. Timestep T1 T2 1 begin 2 X(A) 3 A=10 4 X(B) 5 U(A) 6 begin 7 X(A) 8 A=A+1 9 U(A) 10 commit Assume the initial state is A=0 , executing the above schedule yields the following Undo/Redo Log 1. <T1 begin> 2. <T1 update A 0 10> 3. <T2 begin> 4. <T2 update A 10 11> 5. <T2 commit> After timestep 11, the database crashes. During the recovery phase, we start with undo phase, we find that T1 need to be undone and aborted. However, T2's effect of A=A+1 is based on the assumption that A=10 in T1 was successful. As a result, T2 has be to aborted and undone too. This is known as cascading abort.","title":"Strict 2PL vs 2PL"},{"location":"notes/l9_transaction_concurrency/#conservative-2pl","text":"Another variant of Strict 2PL is called Conservative 2PL . It is same as Strick 2PL except that a transaction acquires all the locks it needs at the very begining. For instance, given the previous example, a schedule is generated with conservative 2PL as follows Timestep T1 T2 1 X(A),X(B) 2 R(A) 3 W(A) 4 R(B) 5 W(B) 6 U(A), U(B), Commit 7 X(A),X(B) 8 R(A) 9 W(A) 10 X(B) 11 R(B) 12 W(B) 13 U(A), U(B), Commit Note that all the locks are acquired by a transaction in an atomic operation, i.e. all or none. This is a way to prevent deadlock.","title":"Conservative 2PL"},{"location":"notes/l9_transaction_concurrency/#deadlock","text":"Let's consider how deadlock can be caused by 2PL (and Strict 2PL). Consider the following transactions T1: R(A), W(A), R(B), W(B) T2: R(B), W(B), R(A), W(A) and a schedule Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 R(B) 6 W(B) 7 try X(B) but blocked try X(A) but blocked At timestep 1, T1 acquires an exclusive lock on A and starts At timestep 4, T2 acquires an exclusive lock on B and starts AT timestep 7, T1 tries to get a lock on B but it gets blocked. T2 tries to get a lock on A but it is blocked too. A deadlock arises.","title":"Deadlock"},{"location":"notes/l9_transaction_concurrency/#deadlock-detection","text":"A way to detect deadlock is that when transactions waiting for locks, the lock manager (part of DBMS) generate a wait-for graph. In a wait for graph, each active transaction \\(t\\) is a node \\(n_t\\) . an arc going from \\(n_t\\) to \\(n_t'\\) if transaction \\(t\\) is waiting for a lock which is being held by \\(t'\\) . For insance, the wait-for graph from the previous example is graph T1 --> T2 T2 --> T1","title":"Deadlock detection"},{"location":"notes/l9_transaction_concurrency/#deadlock-prevention","text":"To prevent deadlock, there are several approaches Use conservative 2PL, but throughput will be affected. Set priorities. Transactions with higher priorty get the lock. Transaction wth lower prioerty get aborted and retry.","title":"Deadlock prevention"},{"location":"notes/l9_transaction_crash_recover/","text":"50.043 Transactions and Crash Recovery Learning Outcomes By the end of this unit, you should be able to describe the ACID properties of database system. describe how atomicity is guaranteed by using transaction and log. explain write ahead log with different policies. explain recovery process with undo, redo, undo/redo logging. explain quiescent and nonquiescent checkpoints. ACID properties To ensure data correctness, modern DBMSes often ensure certain properties. Atomicity The atomicity property allows users of DBMSes to define a sequence of operations to be performed as none or all, there is no in-between. For example, consider the following sequence of database operations represent a balance transfer of $100 from account A to account B. debit $100 from account A credit $100 into account B These two operations must be performed altogether but not partially. Consistency The consistency property ensures the data integrity constraints are not violated. This property has two levels. Data consistency The data consistency refers to the overall constraint (aka business logic) needs to be maintained by the database. DBMS ensures data consistency if the user's operations (in terms of transactions) are consistent. Operation consistency This refers to database operations issued by the users to the database as a single transaction must not violate the business constraint, e.g. the total sum of balance of all accounts in a bank must remain unchanged, and the user operations given in earlier example is of consistent. Esnuring operation consistency has to be user's responsibility. Isolation The isoliation property ensures that multiple concurrent transactions must not be dependent on one another. In other words, if the DBMS executing these concurrent transactions by interleaving, the result be the same as executing them in a sequential order. Note that isolation does not entail determinism. Durability The durability property ensures the effect of transaction operations will stay permanently upon commit. Transaction and ACID Transaction is one of the popular techniques in ensuring ACID properties. In SQL, one can enclose the operations between BEGIN and COMMIT begin; select @bal := bal from saving where id=1001; update saving set bal = @bal-100 where id=1001; select @bal := bal from saving where id=2001; update saving set bal = @bal+100 where id=2001; commit; In the above transaction, we transfer $100 from account with id 1001 to another account with id 2001 . The debit from account 1001 and the credit to account 2001 will take effect altogether or none. In the following section, we first study how to apply transaction with logs to ensure atomicity. Then we consider how to apply transaction to ensure isolation. Durability follows from atomicty and crash recovery process if page flush operation is atomic (as an assumption we take). Consistency is ensured if transactions operations are consistent. Transaction and Crash recovery Let's consider the running example of of debitting account 1001 and crediting account 2001 . There are a few possible program points where a crash could occur before the first update statement after the first update statement and before the second update statement after the second update statement before the commit after the commit Force and no stealing One way to simplify the crash recovery is to assume that A dirty page in the buffer pool is written to disk when the transaction is committed. This is known as the Force policy A page in the buffer pool belonging to an uncommitted transaction will not be evicted. This is known is the No-stealing policy With force and no stealing policy in-place, there is little work to do for recovery, as a committed transaction must have been written to disk. Uncommit transactions have not be written to disk. Note we assume that the commit and page flushing are atomically performed altogether. However such a policy is not very practical. Forcing increases unncessary page I/Os, e.g. when multiple records from the same page get updated in consecutive transactions. No-stealing policy disallows concurrent transaction activities in the event the transaction involves larger set of pages. Stealing and undo logging Let's relax our policy to allow page stealing (but forcing is still enforced). Now the issue is that some of the pages in the buffer pool belonging to an uncommitted transaction could have been written to the disk due to stealing before the commit. When the crash occurs before the commit, we need to undo the page write. To enable recovery, the system maintains a write ahead log with undo information. Each entry in this log has the following fields transaction_id entry_type file_id page_id offset old_value where entry_type could be begin , commit , update and abort . For now we assume abort is issued only by the DBMS but not the user for simplicity. In case of begin , abort , and commit , the file id , page id , offset and old_value fields are null. The undo logging is governed by the following two rules. In a transaction T , before writing the updated page X containing changes V to disk, we must append <T, update, X.file_id, X.page_id, V.offset, V.old_value> to the log. ( V.offset denotes where V is located in the page X .) When T is being committed, to make sure all log entries are written to disk, all updated pages are written to disk. Then append <T, commit> to the log on the disk. For instance, from our running example, let's say the transaction id is t1 , the balance of account 1001 was 2000 in page p1 with offset 0 , and the account 2001 balance was 500 in page p2 with offset 40 . Both pages belong to file f0 . When the transaction committing, we should have the following in the log <t1 begin> <t1 update f0 p1 0 2000> <t1 update f0 p2 40 500> <t1 commit> Now let's imagine the crash occurs after the second update statement, but we are unsure whether all the dirty pages have been written to disk. In that scenario, the recovery manager scans the log backwards (from the tail of the log to the start of the log) to look for transactions that have a begin but without commit (and without abort ) and it finds the following entries at the tail. <t1 begin> <t1 update f0 p1 0 2000> <t1 update f0 p2 40 500> With the above information the recovery manager can (conservatively) restore pages p1 and p2 back to the origin state before the transaction t1 started. Finally it marks the transaction has been undone correctly with <t1, abort> log entry, so that the future recovery routine should not bother about this \"chunk of history\". No force and redo logging If we relax the policy by lifting force (but no stealing is enforced), we need another kind of logging. Now the issue we face is that all the dirty pages in the buffer pool might have not been flushed off the disk even after the transaction has been committed. When the crash occurs after the transaction commit, we need to find a way to check and \"redo\" these page writes during the crash recovery phase. To enable recovery, the system maintains a write ahead log with redo information. Each entry in this log has the following fields transaction_id entry_type file_id page_id offset new_value the fields are almost the same as the one we saw in undo logging except for new_value , in which it captures the new value that the update operation is trying to write. The redo logging is governed by the following rules In a transaction T , before writing the updated page X containing changes V to disk, we must have appended <T, update, X.file_id, X.page_id, V.offset, V.new_value> to the log and appended <T, commit> to the log. (i.e. update and commit to log before flushing). When T is being committed, to make sure all log entries are written to disk. Then append <T, commit> to the log on the disk. Next, let's reuse the same running example and imagine the crash occurs after the commit statement, but we are unsure whether all the dirty pages have been flushed to disk. In that scenario, the recovery manager scans the log from the very begining of the log to the end, for every transaction with begin and commit , apply the redo operation. For instance, ... <t1 begin> <t1 update f0 p1 0 2100> <t1 update f0 p2 40 600> <t1 commit> given the above entry, the recovery algorithm re-applies the updates though they could have been written to disk. It needs start from the begining, i.e. it has to look at all log entries of committed transaction including those older than t1 . A Quick Summary so far - Undo vs Redo Let's contrast Undo logging with Redo logging. Undo logging assumes force and stealing. Dirty pages must be flushed before committing the transaction, hence the commit is slower (more work to do). On the other hand, the recovery process is easier, as we only need to look for uncommitted transaction logs by scanning backward from the tail of the log till the start of the log. Very often we only need to small fraction of the log entries (by heuristics we often find very small numbers of uncommited transaction near the tail). Redo logging assumes no-force and no-stealing. Dirty pages live in the buffer pool beyond transaction committing point, however, pages belonging to an active transaction must not be evicted (written to disk). Hence the commit is faster (no need to make sure dirty pages are flushed). On the other hand, the recovery process overhaul all the transaction entries in the log. Undo/Redo Logging Let's further relax the policy to allow no-force and stealing. To support this liberal policy, we combine both Undo and Redo into a single logging mechannism. Each entry of the Undo/Redo logging has the following fields transaction_id entry_type file_id page_id offset old_value new_value The Undo/Redo logging is governed by the following rules. In a transaction T , before writing the updated page X containing changes V to disk, we must have appended <T, update, X.file_id, X.page_id, V.offset, V.old_value, V.new_value> to the log. When T is being committed, to make sure all log entries are written to disk. Then append <T, commit> to the log on the disk. The recovery is carried out in two phases. Phase 1 - backward pass Since stealing is allowed, in the first pass, like undo logging, we scan the log backwards from the tail, to search for open transactions (i.e. transactions without commit or abort), and perform the undo steps. Phase 2 - forward pass Since no force is in placed, during the second pass, like redo logging, we scan the log forward from the begining. For each committed transaction, we redo the update. Checkpoint To reduce the recovery overhead (especially the redo step), we could insert checkpoints into the log. Quiescent checkpoint One naive approach is to pause the system by preventing new transactions to be initiated wait for the active transactions to be committed and flushed. insert a checkpoint entry to the log, ( checkpoint is a special log entry). unpause the system. During the undo recovery phase, we scan the log backward upto the checkpoint. During the redo recovery phase, we scan the log forward starting from the checkpoint. For instance, consider the tail fragment of some log (simplified). For the ease of referencing, we added line number to each entry. 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 10 20> 5. <t2 begin> 6. <t2 update f2 p3 0 200 300> 7. <start_checkpoint t1,t2> # sys pauses, no more new transaction 8. <t1 commit> 9. <t2 commit> 10. <end_checkpoint t1,t2> # sys resumes, new transaction can be added 11. <t3 begin> 12. <t3 update f3 p2 0 30 40> For undo, we scan the log backwards until line 10, we undo the update at line 12. For redo, we start from line 10 and scan forward, but we find nothing to re-do because there is nothing to re-do. Nonquiescent checkpoint A major disadvantage of quiescent checkpoing is during the pause, no new transactions is initiated, it hurts the overall performance. A nonquiescent checkpoint overcomes this drawback by capturing the uncommitted active transactions when the checkpoint is started. Nonquiescent checkpoint with undo-logging (with stealing and force) The main idea is to flush all uncommitted tranactions' dirty pages during the checkpoint and commit them. find all the active (and uncommitted) transactions ids, T1 , ..., Tn . insert a <start_checkpoint T1,...,Tn> entry to the log. when all the dirty pages belonging to T1,...,Tn are flushed and committed, insert <end_checkpoint T1,...,Tn> entry to the log. During the undo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and scan for uncommitted transactions in the log and undo the page write if there is any. This is because during the checkpoint, there might be new transaction initiated. Note that that undo operations are applied backwards starting from the tail. For instance, consider the tail fragment of some log (simplified). 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 10> 5. <t2 begin> 6. <t2 update f2 p3 0 200> 7. <start_checkpoint t1,t2> 8. <t3 begin> 9. <t1 commit> 10. <t2 commit> 11. <t3 update f3 p2 0 30> 12. <end_checkpoint t1,t2> During the recovery, we start from line 7 <start_checkpoint t1,t2> and look for uncommitted transaction, which is t3 in this case, and undo the update at line 11. Nonquiescent checkpoint with redo-logging (with no force and no stealing) The main idea is to flush all committed transactions (the dirty pages) during the check point. find all the active (and uncommitted) transactions ids, T1 , ..., Tn . insert a <start_checkpoint T1,...,Tn> entry to the log. flush any dirty pages belonging to some committed transactions (committed before the start of the check point.) insert a <end_checkpoint T1,...,Tn> . During the redo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and search for transactions being committed after this point, and redo these transactions. Note that some of these transactions (to be redone) could have been started before the starting of the check point (but still active during the check point). For instance, consider the tail fragment of some log (simplified). 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 20> 5. <t2 begin> 6. <t2 update f2 p3 0 300> 7. <start_checkpoint t1,t2> 8. <t3 begin> 9. <t1 commit> 10. <t3 update f3 p2 0 40> 11. <end_checkpoint t1,t2> # dirty pages belong to t0 have been flushed 12. <t2 commit> We start from line 7 <start_checkpoint t1,t2> and search for committed transactions, i.e. t1 and t2 , we need to re-do the updates at lines 4 and 6. Nonquiescent checkpoint with undo/redo-logging (with no force and stealing) The most complicated checkpoint so far. The main idea is to flush all dirty pages being updated before the check point start. find all the active (and uncommitted) transactions ids, T1 , ..., Tn . insert a <start_checkpoint T1,...,Tn> entry to the log. flush any dirty pages belonging to some update entries made before the start of the check point. insert a <end_checkpoint T1,...,Tn> . During the undo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and search for uncommitted transactions and undo the update, some of these transaction could have been started before the start_checkpoint . During the redo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and search for transactions being committed after this point, and redo these transactions (the same as the redo-logging case). For instance, consider the tail fragment of some log (simplified). 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 10 20> 5. <t2 begin> 6. <t2 update f2 p3 0 200 300> 7. <start_checkpoint t1,t2> 8. <t3 begin> 9. <t1 update f1 p10 0 5 7> 10. <t1 commit> 11. <t3 update f3 p2 0 30 40> 12. <end_checkpoint t1,t2> # p1, p3 and other dirty pages belong to t0 have been flushed 13. <t2 commit> For undo-phase, we start from line 7 <start_checkpoint t1,t2> and search for uncommitted transactions, i.e. t3 , we undo the update at line 11. For redo-phase, we start from line 7 <start_checkpoint t1,t2> and search for committed transactions, i.e. t1 and t2 , we need to re-do the updates at lines 4, 6 and 9.","title":"50.043 Transactions and Crash Recovery"},{"location":"notes/l9_transaction_crash_recover/#50043-transactions-and-crash-recovery","text":"","title":"50.043 Transactions and Crash Recovery"},{"location":"notes/l9_transaction_crash_recover/#learning-outcomes","text":"By the end of this unit, you should be able to describe the ACID properties of database system. describe how atomicity is guaranteed by using transaction and log. explain write ahead log with different policies. explain recovery process with undo, redo, undo/redo logging. explain quiescent and nonquiescent checkpoints.","title":"Learning Outcomes"},{"location":"notes/l9_transaction_crash_recover/#acid-properties","text":"To ensure data correctness, modern DBMSes often ensure certain properties.","title":"ACID properties"},{"location":"notes/l9_transaction_crash_recover/#atomicity","text":"The atomicity property allows users of DBMSes to define a sequence of operations to be performed as none or all, there is no in-between. For example, consider the following sequence of database operations represent a balance transfer of $100 from account A to account B. debit $100 from account A credit $100 into account B These two operations must be performed altogether but not partially.","title":"Atomicity"},{"location":"notes/l9_transaction_crash_recover/#consistency","text":"The consistency property ensures the data integrity constraints are not violated. This property has two levels.","title":"Consistency"},{"location":"notes/l9_transaction_crash_recover/#data-consistency","text":"The data consistency refers to the overall constraint (aka business logic) needs to be maintained by the database. DBMS ensures data consistency if the user's operations (in terms of transactions) are consistent.","title":"Data consistency"},{"location":"notes/l9_transaction_crash_recover/#operation-consistency","text":"This refers to database operations issued by the users to the database as a single transaction must not violate the business constraint, e.g. the total sum of balance of all accounts in a bank must remain unchanged, and the user operations given in earlier example is of consistent. Esnuring operation consistency has to be user's responsibility.","title":"Operation consistency"},{"location":"notes/l9_transaction_crash_recover/#isolation","text":"The isoliation property ensures that multiple concurrent transactions must not be dependent on one another. In other words, if the DBMS executing these concurrent transactions by interleaving, the result be the same as executing them in a sequential order. Note that isolation does not entail determinism.","title":"Isolation"},{"location":"notes/l9_transaction_crash_recover/#durability","text":"The durability property ensures the effect of transaction operations will stay permanently upon commit.","title":"Durability"},{"location":"notes/l9_transaction_crash_recover/#transaction-and-acid","text":"Transaction is one of the popular techniques in ensuring ACID properties. In SQL, one can enclose the operations between BEGIN and COMMIT begin; select @bal := bal from saving where id=1001; update saving set bal = @bal-100 where id=1001; select @bal := bal from saving where id=2001; update saving set bal = @bal+100 where id=2001; commit; In the above transaction, we transfer $100 from account with id 1001 to another account with id 2001 . The debit from account 1001 and the credit to account 2001 will take effect altogether or none. In the following section, we first study how to apply transaction with logs to ensure atomicity. Then we consider how to apply transaction to ensure isolation. Durability follows from atomicty and crash recovery process if page flush operation is atomic (as an assumption we take). Consistency is ensured if transactions operations are consistent.","title":"Transaction and ACID"},{"location":"notes/l9_transaction_crash_recover/#transaction-and-crash-recovery","text":"Let's consider the running example of of debitting account 1001 and crediting account 2001 . There are a few possible program points where a crash could occur before the first update statement after the first update statement and before the second update statement after the second update statement before the commit after the commit","title":"Transaction and Crash recovery"},{"location":"notes/l9_transaction_crash_recover/#force-and-no-stealing","text":"One way to simplify the crash recovery is to assume that A dirty page in the buffer pool is written to disk when the transaction is committed. This is known as the Force policy A page in the buffer pool belonging to an uncommitted transaction will not be evicted. This is known is the No-stealing policy With force and no stealing policy in-place, there is little work to do for recovery, as a committed transaction must have been written to disk. Uncommit transactions have not be written to disk. Note we assume that the commit and page flushing are atomically performed altogether. However such a policy is not very practical. Forcing increases unncessary page I/Os, e.g. when multiple records from the same page get updated in consecutive transactions. No-stealing policy disallows concurrent transaction activities in the event the transaction involves larger set of pages.","title":"Force and no stealing"},{"location":"notes/l9_transaction_crash_recover/#stealing-and-undo-logging","text":"Let's relax our policy to allow page stealing (but forcing is still enforced). Now the issue is that some of the pages in the buffer pool belonging to an uncommitted transaction could have been written to the disk due to stealing before the commit. When the crash occurs before the commit, we need to undo the page write. To enable recovery, the system maintains a write ahead log with undo information. Each entry in this log has the following fields transaction_id entry_type file_id page_id offset old_value where entry_type could be begin , commit , update and abort . For now we assume abort is issued only by the DBMS but not the user for simplicity. In case of begin , abort , and commit , the file id , page id , offset and old_value fields are null. The undo logging is governed by the following two rules. In a transaction T , before writing the updated page X containing changes V to disk, we must append <T, update, X.file_id, X.page_id, V.offset, V.old_value> to the log. ( V.offset denotes where V is located in the page X .) When T is being committed, to make sure all log entries are written to disk, all updated pages are written to disk. Then append <T, commit> to the log on the disk. For instance, from our running example, let's say the transaction id is t1 , the balance of account 1001 was 2000 in page p1 with offset 0 , and the account 2001 balance was 500 in page p2 with offset 40 . Both pages belong to file f0 . When the transaction committing, we should have the following in the log <t1 begin> <t1 update f0 p1 0 2000> <t1 update f0 p2 40 500> <t1 commit> Now let's imagine the crash occurs after the second update statement, but we are unsure whether all the dirty pages have been written to disk. In that scenario, the recovery manager scans the log backwards (from the tail of the log to the start of the log) to look for transactions that have a begin but without commit (and without abort ) and it finds the following entries at the tail. <t1 begin> <t1 update f0 p1 0 2000> <t1 update f0 p2 40 500> With the above information the recovery manager can (conservatively) restore pages p1 and p2 back to the origin state before the transaction t1 started. Finally it marks the transaction has been undone correctly with <t1, abort> log entry, so that the future recovery routine should not bother about this \"chunk of history\".","title":"Stealing and undo logging"},{"location":"notes/l9_transaction_crash_recover/#no-force-and-redo-logging","text":"If we relax the policy by lifting force (but no stealing is enforced), we need another kind of logging. Now the issue we face is that all the dirty pages in the buffer pool might have not been flushed off the disk even after the transaction has been committed. When the crash occurs after the transaction commit, we need to find a way to check and \"redo\" these page writes during the crash recovery phase. To enable recovery, the system maintains a write ahead log with redo information. Each entry in this log has the following fields transaction_id entry_type file_id page_id offset new_value the fields are almost the same as the one we saw in undo logging except for new_value , in which it captures the new value that the update operation is trying to write. The redo logging is governed by the following rules In a transaction T , before writing the updated page X containing changes V to disk, we must have appended <T, update, X.file_id, X.page_id, V.offset, V.new_value> to the log and appended <T, commit> to the log. (i.e. update and commit to log before flushing). When T is being committed, to make sure all log entries are written to disk. Then append <T, commit> to the log on the disk. Next, let's reuse the same running example and imagine the crash occurs after the commit statement, but we are unsure whether all the dirty pages have been flushed to disk. In that scenario, the recovery manager scans the log from the very begining of the log to the end, for every transaction with begin and commit , apply the redo operation. For instance, ... <t1 begin> <t1 update f0 p1 0 2100> <t1 update f0 p2 40 600> <t1 commit> given the above entry, the recovery algorithm re-applies the updates though they could have been written to disk. It needs start from the begining, i.e. it has to look at all log entries of committed transaction including those older than t1 .","title":"No force and redo logging"},{"location":"notes/l9_transaction_crash_recover/#a-quick-summary-so-far-undo-vs-redo","text":"Let's contrast Undo logging with Redo logging. Undo logging assumes force and stealing. Dirty pages must be flushed before committing the transaction, hence the commit is slower (more work to do). On the other hand, the recovery process is easier, as we only need to look for uncommitted transaction logs by scanning backward from the tail of the log till the start of the log. Very often we only need to small fraction of the log entries (by heuristics we often find very small numbers of uncommited transaction near the tail). Redo logging assumes no-force and no-stealing. Dirty pages live in the buffer pool beyond transaction committing point, however, pages belonging to an active transaction must not be evicted (written to disk). Hence the commit is faster (no need to make sure dirty pages are flushed). On the other hand, the recovery process overhaul all the transaction entries in the log.","title":"A Quick Summary so far - Undo vs Redo"},{"location":"notes/l9_transaction_crash_recover/#undoredo-logging","text":"Let's further relax the policy to allow no-force and stealing. To support this liberal policy, we combine both Undo and Redo into a single logging mechannism. Each entry of the Undo/Redo logging has the following fields transaction_id entry_type file_id page_id offset old_value new_value The Undo/Redo logging is governed by the following rules. In a transaction T , before writing the updated page X containing changes V to disk, we must have appended <T, update, X.file_id, X.page_id, V.offset, V.old_value, V.new_value> to the log. When T is being committed, to make sure all log entries are written to disk. Then append <T, commit> to the log on the disk. The recovery is carried out in two phases.","title":"Undo/Redo Logging"},{"location":"notes/l9_transaction_crash_recover/#phase-1-backward-pass","text":"Since stealing is allowed, in the first pass, like undo logging, we scan the log backwards from the tail, to search for open transactions (i.e. transactions without commit or abort), and perform the undo steps.","title":"Phase 1 - backward pass"},{"location":"notes/l9_transaction_crash_recover/#phase-2-forward-pass","text":"Since no force is in placed, during the second pass, like redo logging, we scan the log forward from the begining. For each committed transaction, we redo the update.","title":"Phase 2 - forward pass"},{"location":"notes/l9_transaction_crash_recover/#checkpoint","text":"To reduce the recovery overhead (especially the redo step), we could insert checkpoints into the log.","title":"Checkpoint"},{"location":"notes/l9_transaction_crash_recover/#quiescent-checkpoint","text":"One naive approach is to pause the system by preventing new transactions to be initiated wait for the active transactions to be committed and flushed. insert a checkpoint entry to the log, ( checkpoint is a special log entry). unpause the system. During the undo recovery phase, we scan the log backward upto the checkpoint. During the redo recovery phase, we scan the log forward starting from the checkpoint. For instance, consider the tail fragment of some log (simplified). For the ease of referencing, we added line number to each entry. 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 10 20> 5. <t2 begin> 6. <t2 update f2 p3 0 200 300> 7. <start_checkpoint t1,t2> # sys pauses, no more new transaction 8. <t1 commit> 9. <t2 commit> 10. <end_checkpoint t1,t2> # sys resumes, new transaction can be added 11. <t3 begin> 12. <t3 update f3 p2 0 30 40> For undo, we scan the log backwards until line 10, we undo the update at line 12. For redo, we start from line 10 and scan forward, but we find nothing to re-do because there is nothing to re-do.","title":"Quiescent checkpoint"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint","text":"A major disadvantage of quiescent checkpoing is during the pause, no new transactions is initiated, it hurts the overall performance. A nonquiescent checkpoint overcomes this drawback by capturing the uncommitted active transactions when the checkpoint is started.","title":"Nonquiescent checkpoint"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint-with-undo-logging-with-stealing-and-force","text":"The main idea is to flush all uncommitted tranactions' dirty pages during the checkpoint and commit them. find all the active (and uncommitted) transactions ids, T1 , ..., Tn . insert a <start_checkpoint T1,...,Tn> entry to the log. when all the dirty pages belonging to T1,...,Tn are flushed and committed, insert <end_checkpoint T1,...,Tn> entry to the log. During the undo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and scan for uncommitted transactions in the log and undo the page write if there is any. This is because during the checkpoint, there might be new transaction initiated. Note that that undo operations are applied backwards starting from the tail. For instance, consider the tail fragment of some log (simplified). 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 10> 5. <t2 begin> 6. <t2 update f2 p3 0 200> 7. <start_checkpoint t1,t2> 8. <t3 begin> 9. <t1 commit> 10. <t2 commit> 11. <t3 update f3 p2 0 30> 12. <end_checkpoint t1,t2> During the recovery, we start from line 7 <start_checkpoint t1,t2> and look for uncommitted transaction, which is t3 in this case, and undo the update at line 11.","title":"Nonquiescent checkpoint with undo-logging (with stealing and force)"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint-with-redo-logging-with-no-force-and-no-stealing","text":"The main idea is to flush all committed transactions (the dirty pages) during the check point. find all the active (and uncommitted) transactions ids, T1 , ..., Tn . insert a <start_checkpoint T1,...,Tn> entry to the log. flush any dirty pages belonging to some committed transactions (committed before the start of the check point.) insert a <end_checkpoint T1,...,Tn> . During the redo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and search for transactions being committed after this point, and redo these transactions. Note that some of these transactions (to be redone) could have been started before the starting of the check point (but still active during the check point). For instance, consider the tail fragment of some log (simplified). 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 20> 5. <t2 begin> 6. <t2 update f2 p3 0 300> 7. <start_checkpoint t1,t2> 8. <t3 begin> 9. <t1 commit> 10. <t3 update f3 p2 0 40> 11. <end_checkpoint t1,t2> # dirty pages belong to t0 have been flushed 12. <t2 commit> We start from line 7 <start_checkpoint t1,t2> and search for committed transactions, i.e. t1 and t2 , we need to re-do the updates at lines 4 and 6.","title":"Nonquiescent checkpoint with redo-logging (with no force and no stealing)"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint-with-undoredo-logging-with-no-force-and-stealing","text":"The most complicated checkpoint so far. The main idea is to flush all dirty pages being updated before the check point start. find all the active (and uncommitted) transactions ids, T1 , ..., Tn . insert a <start_checkpoint T1,...,Tn> entry to the log. flush any dirty pages belonging to some update entries made before the start of the check point. insert a <end_checkpoint T1,...,Tn> . During the undo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and search for uncommitted transactions and undo the update, some of these transaction could have been started before the start_checkpoint . During the redo recovery phase, we start from the last completed checkpoint's start_checkpoint entry and search for transactions being committed after this point, and redo these transactions (the same as the redo-logging case). For instance, consider the tail fragment of some log (simplified). 1. ... 2. <t0 commit> 3. <t1 begin> 4. <t1 update f1 p1 0 10 20> 5. <t2 begin> 6. <t2 update f2 p3 0 200 300> 7. <start_checkpoint t1,t2> 8. <t3 begin> 9. <t1 update f1 p10 0 5 7> 10. <t1 commit> 11. <t3 update f3 p2 0 30 40> 12. <end_checkpoint t1,t2> # p1, p3 and other dirty pages belong to t0 have been flushed 13. <t2 commit> For undo-phase, we start from line 7 <start_checkpoint t1,t2> and search for uncommitted transactions, i.e. t3 , we undo the update at line 11. For redo-phase, we start from line 7 <start_checkpoint t1,t2> and search for committed transactions, i.e. t1 and t2 , we need to re-do the updates at lines 4, 6 and 9.","title":"Nonquiescent checkpoint with undo/redo-logging (with no force and stealing)"}]}